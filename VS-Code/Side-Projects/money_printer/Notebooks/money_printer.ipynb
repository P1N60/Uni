{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Library Installation (if needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install libraries the first time\n",
    "#! pip install -U yfinance pandas pathlib numpy "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pathlib as Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['MATAS.CO', 'TRIFOR.CO', 'QQ.L', 'RNMBY', 'SAABF', 'BCKIY',\n",
       "       'BAESY', 'IVSO.ST', 'NSKFF', 'GMAB', 'GN.CO', 'NVDA', 'LLY',\n",
       "       'DANSKE.CO', 'CARL-B.CO', 'MAERSK-B.CO', 'RBREW.CO', 'ISS.CO',\n",
       "       'DSV.CO', 'SCHO.CO', 'NETC.CO', 'JYSK.CO', 'ABBN.SW', 'TER',\n",
       "       'PARKEN.CO', 'NFLX', 'TRMD-A.CO', 'STG.CO', 'NOVO-B.CO', 'EQNR',\n",
       "       'NKT.CO', 'NSIS-B.CO', 'KCC.OL'], dtype=object)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "symbols = pd.read_csv('../data/simple_screener_results.csv')['Ticker'].tolist()\n",
    "symbols = pd.Series(symbols).unique()\n",
    "symbols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download and Process Historical Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#symbols = ['AAPL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MATAS.CO...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "$MATAS.CO: possibly delisted; no price data found  (1d 2024-03-30 -> 2024-04-01)\n",
      "C:\\Users\\Gamer\\AppData\\Local\\Temp\\ipykernel_3208\\3458907841.py:44: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, pd.DataFrame([row_data])], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRIFOR.CO...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "$TRIFOR.CO: possibly delisted; no price data found  (1d 2023-12-30 -> 2024-01-01)\n",
      "C:\\Users\\Gamer\\AppData\\Local\\Temp\\ipykernel_3208\\3458907841.py:44: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, pd.DataFrame([row_data])], ignore_index=True)\n",
      "$TRIFOR.CO: possibly delisted; no price data found  (1d 2020-12-30 -> 2021-01-01) (Yahoo error = \"Data doesn't exist for startDate = 1609282800, endDate = 1609455600\")\n",
      "C:\\Users\\Gamer\\AppData\\Local\\Temp\\ipykernel_3208\\3458907841.py:44: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, pd.DataFrame([row_data])], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QQ.L...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "$QQ.L: possibly delisted; no price data found  (1d 2024-03-30 -> 2024-04-01)\n",
      "C:\\Users\\Gamer\\AppData\\Local\\Temp\\ipykernel_3208\\3458907841.py:44: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, pd.DataFrame([row_data])], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNMBY...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "$RNMBY: possibly delisted; no price data found  (1d 2023-12-30 -> 2024-01-01)\n",
      "C:\\Users\\Gamer\\AppData\\Local\\Temp\\ipykernel_3208\\3458907841.py:44: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, pd.DataFrame([row_data])], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAABF...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "$SAABF: possibly delisted; no price data found  (1d 2023-12-30 -> 2024-01-01)\n",
      "C:\\Users\\Gamer\\AppData\\Local\\Temp\\ipykernel_3208\\3458907841.py:44: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, pd.DataFrame([row_data])], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BCKIY...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "$BCKIY: possibly delisted; no price data found  (1d 2024-03-30 -> 2024-04-01)\n",
      "C:\\Users\\Gamer\\AppData\\Local\\Temp\\ipykernel_3208\\3458907841.py:44: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, pd.DataFrame([row_data])], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BAESY...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "$BAESY: possibly delisted; no price data found  (1d 2023-12-30 -> 2024-01-01)\n",
      "C:\\Users\\Gamer\\AppData\\Local\\Temp\\ipykernel_3208\\3458907841.py:44: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, pd.DataFrame([row_data])], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IVSO.ST...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "$IVSO.ST: possibly delisted; no price data found  (1d 2023-12-30 -> 2024-01-01)\n",
      "C:\\Users\\Gamer\\AppData\\Local\\Temp\\ipykernel_3208\\3458907841.py:44: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, pd.DataFrame([row_data])], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NSKFF...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "$NSKFF: possibly delisted; no price data found  (1d 2023-12-30 -> 2024-01-01)\n",
      "C:\\Users\\Gamer\\AppData\\Local\\Temp\\ipykernel_3208\\3458907841.py:44: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, pd.DataFrame([row_data])], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error for NSKFF on 2020-12-31 00:00:00: Timestamp('2020-12-31 00:00:00')\n",
      "GMAB...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "$GMAB: possibly delisted; no price data found  (1d 2023-12-30 -> 2024-01-01)\n",
      "C:\\Users\\Gamer\\AppData\\Local\\Temp\\ipykernel_3208\\3458907841.py:44: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, pd.DataFrame([row_data])], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GN.CO...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "$GN.CO: possibly delisted; no price data found  (1d 2023-12-30 -> 2024-01-01)\n",
      "C:\\Users\\Gamer\\AppData\\Local\\Temp\\ipykernel_3208\\3458907841.py:44: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, pd.DataFrame([row_data])], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error for GN.CO on 2020-12-31 00:00:00: Timestamp('2020-12-31 00:00:00')\n",
      "NVDA...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "$NVDA: possibly delisted; no price data found  (1d 2021-01-30 -> 2021-02-01)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error for NVDA on 2021-01-31 00:00:00: Timestamp('2021-01-31 00:00:00')\n",
      "LLY...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gamer\\AppData\\Local\\Temp\\ipykernel_3208\\3458907841.py:44: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, pd.DataFrame([row_data])], ignore_index=True)\n",
      "$LLY: possibly delisted; no price data found  (1d 2023-12-30 -> 2024-01-01)\n",
      "C:\\Users\\Gamer\\AppData\\Local\\Temp\\ipykernel_3208\\3458907841.py:44: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, pd.DataFrame([row_data])], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DANSKE.CO...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "$DANSKE.CO: possibly delisted; no price data found  (1d 2023-12-30 -> 2024-01-01)\n",
      "C:\\Users\\Gamer\\AppData\\Local\\Temp\\ipykernel_3208\\3458907841.py:44: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, pd.DataFrame([row_data])], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CARL-B.CO...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "$CARL-B.CO: possibly delisted; no price data found  (1d 2023-12-30 -> 2024-01-01)\n",
      "C:\\Users\\Gamer\\AppData\\Local\\Temp\\ipykernel_3208\\3458907841.py:44: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, pd.DataFrame([row_data])], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAERSK-B.CO...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "$MAERSK-B.CO: possibly delisted; no price data found  (1d 2023-12-30 -> 2024-01-01)\n",
      "C:\\Users\\Gamer\\AppData\\Local\\Temp\\ipykernel_3208\\3458907841.py:44: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, pd.DataFrame([row_data])], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RBREW.CO...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "$RBREW.CO: possibly delisted; no price data found  (1d 2023-12-30 -> 2024-01-01)\n",
      "C:\\Users\\Gamer\\AppData\\Local\\Temp\\ipykernel_3208\\3458907841.py:44: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, pd.DataFrame([row_data])], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ISS.CO...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "$ISS.CO: possibly delisted; no price data found  (1d 2023-12-30 -> 2024-01-01)\n",
      "C:\\Users\\Gamer\\AppData\\Local\\Temp\\ipykernel_3208\\3458907841.py:44: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, pd.DataFrame([row_data])], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error for ISS.CO on 2020-12-31 00:00:00: Timestamp('2020-12-31 00:00:00')\n",
      "DSV.CO...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "$DSV.CO: possibly delisted; no price data found  (1d 2023-12-30 -> 2024-01-01)\n",
      "C:\\Users\\Gamer\\AppData\\Local\\Temp\\ipykernel_3208\\3458907841.py:44: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, pd.DataFrame([row_data])], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCHO.CO...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "$SCHO.CO: possibly delisted; no price data found  (1d 2023-12-30 -> 2024-01-01)\n",
      "C:\\Users\\Gamer\\AppData\\Local\\Temp\\ipykernel_3208\\3458907841.py:44: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, pd.DataFrame([row_data])], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NETC.CO...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "$NETC.CO: possibly delisted; no price data found  (1d 2023-12-30 -> 2024-01-01)\n",
      "C:\\Users\\Gamer\\AppData\\Local\\Temp\\ipykernel_3208\\3458907841.py:44: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, pd.DataFrame([row_data])], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error for NETC.CO on 2020-12-31 00:00:00: Timestamp('2020-12-31 00:00:00')\n",
      "JYSK.CO...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "$JYSK.CO: possibly delisted; no price data found  (1d 2023-12-30 -> 2024-01-01)\n",
      "C:\\Users\\Gamer\\AppData\\Local\\Temp\\ipykernel_3208\\3458907841.py:44: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, pd.DataFrame([row_data])], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ABBN.SW...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "$ABBN.SW: possibly delisted; no price data found  (1d 2023-12-30 -> 2024-01-01)\n",
      "C:\\Users\\Gamer\\AppData\\Local\\Temp\\ipykernel_3208\\3458907841.py:44: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, pd.DataFrame([row_data])], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TER...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "$TER: possibly delisted; no price data found  (1d 2023-12-30 -> 2024-01-01)\n",
      "C:\\Users\\Gamer\\AppData\\Local\\Temp\\ipykernel_3208\\3458907841.py:44: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, pd.DataFrame([row_data])], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PARKEN.CO...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "$PARKEN.CO: possibly delisted; no price data found  (1d 2023-12-30 -> 2024-01-01)\n",
      "C:\\Users\\Gamer\\AppData\\Local\\Temp\\ipykernel_3208\\3458907841.py:44: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, pd.DataFrame([row_data])], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NFLX...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "$NFLX: possibly delisted; no price data found  (1d 2023-12-30 -> 2024-01-01)\n",
      "C:\\Users\\Gamer\\AppData\\Local\\Temp\\ipykernel_3208\\3458907841.py:44: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, pd.DataFrame([row_data])], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error for NFLX on 2020-12-31 00:00:00: Timestamp('2020-12-31 00:00:00')\n",
      "TRMD-A.CO...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "$TRMD-A.CO: possibly delisted; no price data found  (1d 2023-12-30 -> 2024-01-01)\n",
      "C:\\Users\\Gamer\\AppData\\Local\\Temp\\ipykernel_3208\\3458907841.py:44: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, pd.DataFrame([row_data])], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STG.CO...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "$STG.CO: possibly delisted; no price data found  (1d 2023-12-30 -> 2024-01-01)\n",
      "C:\\Users\\Gamer\\AppData\\Local\\Temp\\ipykernel_3208\\3458907841.py:44: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, pd.DataFrame([row_data])], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOVO-B.CO...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "$NOVO-B.CO: possibly delisted; no price data found  (1d 2023-12-30 -> 2024-01-01)\n",
      "C:\\Users\\Gamer\\AppData\\Local\\Temp\\ipykernel_3208\\3458907841.py:44: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, pd.DataFrame([row_data])], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error for NOVO-B.CO on 2020-12-31 00:00:00: Timestamp('2020-12-31 00:00:00')\n",
      "EQNR...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "$EQNR: possibly delisted; no price data found  (1d 2023-12-30 -> 2024-01-01)\n",
      "C:\\Users\\Gamer\\AppData\\Local\\Temp\\ipykernel_3208\\3458907841.py:44: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, pd.DataFrame([row_data])], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NKT.CO...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "$NKT.CO: possibly delisted; no price data found  (1d 2023-12-30 -> 2024-01-01)\n",
      "C:\\Users\\Gamer\\AppData\\Local\\Temp\\ipykernel_3208\\3458907841.py:44: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, pd.DataFrame([row_data])], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error for NKT.CO on 2020-12-31 00:00:00: Timestamp('2020-12-31 00:00:00')\n",
      "NSIS-B.CO...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "$NSIS-B.CO: possibly delisted; no price data found  (1d 2023-12-30 -> 2024-01-01)\n",
      "C:\\Users\\Gamer\\AppData\\Local\\Temp\\ipykernel_3208\\3458907841.py:44: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, pd.DataFrame([row_data])], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KCC.OL...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "$KCC.OL: possibly delisted; no price data found  (1d 2023-12-30 -> 2024-01-01)\n",
      "C:\\Users\\Gamer\\AppData\\Local\\Temp\\ipykernel_3208\\3458907841.py:44: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, pd.DataFrame([row_data])], ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame()\n",
    "\n",
    "for symbol in symbols:\n",
    "    print(f'{symbol}...')\n",
    "    ticker = yf.Ticker(symbol)\n",
    "\n",
    "    earning_dates = ticker.cashflow.columns.tolist()\n",
    "    cashflow = ticker.cashflow\n",
    "    cashflow_columns = cashflow[earning_dates[0]].keys().tolist()\n",
    "    balancesheet = ticker.balancesheet\n",
    "    balancesheet_columns = balancesheet[earning_dates[0]].keys().tolist()\n",
    "    incomestatement = ticker.incomestmt\n",
    "    incomestatement_columns = incomestatement[earning_dates[0]].keys().tolist()\n",
    "\n",
    "    for earning_date in earning_dates:\n",
    "        row_data = {'Ticker': symbol, 'Date': earning_date}\n",
    "\n",
    "        try:\n",
    "            for column in cashflow_columns:\n",
    "                row_data[column] = cashflow[earning_date][column]\n",
    "\n",
    "            for column in balancesheet_columns:\n",
    "                row_data[column] = balancesheet[earning_date][column]\n",
    "\n",
    "            for column in incomestatement_columns:\n",
    "                row_data[column] = incomestatement[earning_date][column]\n",
    "        except Exception as e:\n",
    "            print(f\"Error for {symbol} on {earning_date}: {e}\")\n",
    "\n",
    "        try:\n",
    "            start_date = (earning_date - pd.Timedelta(days=1)).strftime(\"%Y-%m-%d\")\n",
    "            end_date = (earning_date + pd.Timedelta(days=1)).strftime(\"%Y-%m-%d\")\n",
    "\n",
    "            price_data = ticker.history(start=start_date, end=end_date)\n",
    "            if not price_data.empty:\n",
    "                # Use the closest available price\n",
    "                row_data['Close Price'] = price_data['Close'].iloc[-1]\n",
    "            else:\n",
    "                row_data['Close Price'] = None\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching price for {symbol} on {earning_date}: {e}\")\n",
    "            row_data['Close Price'] = None\n",
    "\n",
    "        df = pd.concat([df, pd.DataFrame([row_data])], ignore_index=True)\n",
    "\n",
    "df.to_csv('../data/earnings_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'testFolder' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m tickers \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[43mtestFolder\u001b[49m \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfilteredTickers.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTicker\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      2\u001b[0m trainingTickers \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mchoice(tickers, size\u001b[38;5;241m=\u001b[39mtrainingSize, replace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m      3\u001b[0m trainingRowAmount \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(pd\u001b[38;5;241m.\u001b[39mread_csv(testFolder \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfilteredTickers.csv\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'testFolder' is not defined"
     ]
    }
   ],
   "source": [
    "tickers = pd.read_csv(testFolder / 'filteredTickers.csv')['Ticker']\n",
    "trainingTickers = np.random.choice(tickers, size=trainingSize, replace=False)\n",
    "trainingRowAmount = len(pd.read_csv(testFolder / 'filteredTickers.csv'))\n",
    "\n",
    "if getNewData:\n",
    "    histData = pd.DataFrame()\n",
    "    valid_tickers = []\n",
    "    \n",
    "    for ticker in trainingTickers:\n",
    "        print(f\"Processing {ticker}...\")\n",
    "        try:\n",
    "            data = calculateFutureYearChange(ticker, timeFrame)\n",
    "            if not data.empty:\n",
    "                data['Ticker'] = ticker\n",
    "                data['Industry'] = yf.Ticker(ticker).info.get('industry', 'Unknown')\n",
    "                data['Date'] = pd.to_datetime(data['Date']).dt.tz_localize(None)\n",
    "                \n",
    "                # Enrich individual ticker data first\n",
    "                ticker_data = enrichDataWithMetrics(data)\n",
    "                histData = pd.concat([histData, ticker_data])\n",
    "                \n",
    "                # Check if metrics were added\n",
    "                if 'ROIC' not in ticker_data.columns:\n",
    "                    print(f\"WARNING: Failed to add metrics for {ticker}\")\n",
    "                \n",
    "                valid_tickers.append(ticker)\n",
    "            else:\n",
    "                print(f\"Skipped {ticker} - insufficient data\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {ticker}: {e}\")\n",
    "\n",
    "    print(f\"\\nColumns in final dataset: {histData.columns.tolist()}\")\n",
    "    \n",
    "    if not histData.empty:\n",
    "        histData = enrichDataWithMetrics(histData)\n",
    "        histData.to_csv(dataFolder / trainingData, index=True)\n",
    "        # Verify no future targets leaked to past dates\n",
    "        latest_date = pd.to_datetime(histData['Date']).max()\n",
    "        if 'Future Year Change' in histData.columns:\n",
    "            target_dates = histData[histData['Future Year Change'].notnull()]['Date']\n",
    "            if any(pd.to_datetime(target_dates) > latest_date):\n",
    "                raise ValueError(\"CRITICAL: Analyst targets contain future dates!\")\n",
    "        trainingRowAmount = len(histData)\n",
    "        print(f\"Saved training data with {trainingRowAmount} rows\")\n",
    "    else:\n",
    "        print(\"Warning: No data collected - check your tickers list\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Short visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if getNewData:\n",
    "    histData.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if trainNewModel:\n",
    "    TRAINING_CUTOFF = pd.to_datetime('2023-01-01')\n",
    "\n",
    "    df = pd.read_csv(dataFolder / trainingData)\n",
    "    dfCleaned = df.dropna(subset=['EV/EBIT', 'ROIC']).copy()\n",
    "\n",
    "    # Convert 'Date' to datetime, parse UTC-aware dates, then make naive\n",
    "    dfCleaned['Date'] = pd.to_datetime(dfCleaned['Date'], errors='coerce', utc=True).dt.tz_convert(None)\n",
    "    dfCleaned = dfCleaned.dropna(subset=['Date'])\n",
    "\n",
    "    # Clean 'EV/EBIT' and reset index\n",
    "    dfCleaned['EV/EBIT'] = dfCleaned['EV/EBIT'].replace([np.inf, -np.inf], np.nan)\n",
    "    dfCleaned = dfCleaned.dropna(subset=['EV/EBIT', 'ROIC'])\n",
    "    dfCleaned = dfCleaned.reset_index(drop=True)\n",
    "    dfCleaned.to_csv(dataFolder / trainingData, index=False)\n",
    "\n",
    "    # Check for empty data\n",
    "    if dfCleaned.empty:\n",
    "        raise ValueError(\"The cleaned DataFrame is empty.\")\n",
    "\n",
    "    # Create splits with valid indices\n",
    "    train_mask = dfCleaned['Date'] < TRAINING_CUTOFF\n",
    "    valid_mask = ~train_mask\n",
    "    splits = (list(dfCleaned[train_mask].index), list(dfCleaned[valid_mask].index))\n",
    "\n",
    "    if not splits[0] or not splits[1]:\n",
    "        raise ValueError(\"Empty training or validation split.\")\n",
    "\n",
    "    # Proceed with TabularPandas\n",
    "    to = TabularPandas(\n",
    "        dfCleaned, \n",
    "        procs=[Categorify, FillMissing, Normalize],\n",
    "        y_names=yNames,\n",
    "        cat_names=catNames, \n",
    "        cont_names=contNames,\n",
    "        splits=splits\n",
    "    )\n",
    "\n",
    "    dls = to.dataloaders(bs=batchSize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if trainNewModel:\n",
    "    learn = tabular_learner(dls, metrics=[rmse, mae])\n",
    "\n",
    "    # Learning rate finder\n",
    "    lr_find_results = learn.lr_find(suggest_funcs=(minimum, steep))\n",
    "\n",
    "    # Debugging information\n",
    "    print(f\"Learning rate finder results: {lr_find_results}\")\n",
    "\n",
    "    # Extract learning rates\n",
    "    lr_min, lr_steep = lr_find_results\n",
    "\n",
    "    # Check if learning rates are valid\n",
    "    if lr_min is None or lr_steep is None or lr_min == 0 or lr_steep == 0:\n",
    "        raise ValueError(\"Learning rate finder did not return valid learning rates.\")\n",
    "\n",
    "    # Train\n",
    "    print(f\"Training for {epochs} epochs...\")\n",
    "    learn.fit_one_cycle(epochs, lr_max=lr_steep)\n",
    "    print(\"Model training complete\")\n",
    "\n",
    "    learn.export(modelFolder / f'{modelName}.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing and benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logEvaluation(model_name, mae, rmse, r2, model_folder, test_tickers):\n",
    "    \"\"\"Log evaluation metrics to CSV file\"\"\"\n",
    "    log_file = model_folder / \"modelEvaluations.csv\"\n",
    "    \n",
    "    new_entry_df = pd.DataFrame([{\n",
    "        \"Model Name\": model_name,\n",
    "        \"Timestamp\": datetime.now().strftime('%Y-%m-%d %H:%M'),\n",
    "        \"MAE\": f'{mae:.3f}',\n",
    "        \"RMSE\": f'{rmse:.3f}',\n",
    "        \"R2\": f'{r2:.3f}',\n",
    "        \"Epochs\": epochs,\n",
    "        \"Training Size\": trainingSize,\n",
    "        \"Training Rows\": trainingRowAmount,\n",
    "        \"Test Size\": len(test_tickers),\n",
    "        \"Cat Names\": catNames,\n",
    "        \"Cont Names\": contNames,\n",
    "    }])\n",
    "    \n",
    "    try:\n",
    "        log_df = pd.read_csv(log_file)\n",
    "        log_df = pd.concat([log_df, new_entry_df], ignore_index=True)\n",
    "    except FileNotFoundError:\n",
    "        log_df = new_entry_df\n",
    "        \n",
    "    log_df.to_csv(log_file, index=False)\n",
    "    print(f\"Logged evaluation results to {log_file}\")\n",
    "\n",
    "def plotResults(results_df, model_name, model_folder):\n",
    "    \"\"\"Create and save visualization plots using all data points.\"\"\"\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.scatter(results_df['Actual'], results_df['Predicted'], alpha=0.7, label='Predictions')\n",
    "    min_val = min(results_df['Actual'].min(), results_df['Predicted'].min())\n",
    "    max_val = max(results_df['Actual'].max(), results_df['Predicted'].max())\n",
    "    plt.plot([min_val, max_val], [min_val, max_val], color='red', linestyle='--', label='Perfect Prediction')\n",
    "    plt.title(f'Predicted vs. Actual Returns - {model_name}', fontsize=14)\n",
    "    plt.xlabel('Actual Returns')\n",
    "    plt.ylabel('Predicted Returns')\n",
    "    plt.legend()\n",
    "    plt.grid(alpha=0.5)\n",
    "    \n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.scatter(results_df['Predicted'], results_df['Residual'], alpha=0.7)\n",
    "    plt.axhline(y=0, color='r', linestyle='--')\n",
    "    plt.title('Residual Plot')\n",
    "    plt.xlabel('Predicted Returns')\n",
    "    plt.ylabel('Residual')\n",
    "    plt.grid(alpha=0.5)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "if trainNewModel:\n",
    "    nonTrainingTickers = list(set(tickers) - set(trainingTickers))\n",
    "    validTestData = []\n",
    "    attempted_tickers = set()\n",
    "    attempts = 0\n",
    "\n",
    "    if testSize * 4 <= len(tickers):  \n",
    "        max_attempts = testSize * 4 # Prevent infinite loops\n",
    "    else:\n",
    "        max_attempts = len(tickers)\n",
    "\n",
    "    # Keep trying until we reach testSize or exhaust attempts\n",
    "    while len(validTestData) < testSize and attempts < max_attempts:\n",
    "        # Get a new ticker we haven't tried yet\n",
    "        remaining_tickers = [t for t in nonTrainingTickers if t not in attempted_tickers]\n",
    "        if not remaining_tickers:  # If all tried, reset attempted list\n",
    "            attempted_tickers = set()\n",
    "            remaining_tickers = nonTrainingTickers\n",
    "            \n",
    "        ticker = np.random.choice(remaining_tickers)\n",
    "        attempted_tickers.add(ticker)\n",
    "        attempts += 1\n",
    "\n",
    "        # Fetch and validate data\n",
    "        data = getTickerDataFrom1YrAgo(ticker)\n",
    "        if not data.empty and not data[['EV/EBIT', 'ROIC']].isna().any().any():\n",
    "            validTestData.append(data)\n",
    "\n",
    "    if not validTestData:\n",
    "        raise ValueError(\"No valid test data collected after multiple attempts\")\n",
    "        \n",
    "    # Trim to exact testSize if we collected more\n",
    "    validTestData = validTestData[:testSize]  \n",
    "    combinedTestData = pd.concat(validTestData, ignore_index=True)\n",
    "\n",
    "    # Clean data\n",
    "    test_data_clean = combinedTestData.dropna(subset=['EV/EBIT', 'ROIC', 'Future Year Change'])\n",
    "    \n",
    "    if test_data_clean.empty:\n",
    "        raise ValueError(\"No valid test data after cleaning NaN values\")\n",
    "\n",
    "    # Create test dataloader\n",
    "    test_dl = learn.dls.test_dl(test_data_clean)\n",
    "    preds, targs = learn.get_preds(dl=test_dl)\n",
    "    \n",
    "    # Create results DataFrame\n",
    "    results_df = pd.DataFrame({\n",
    "        'Predicted': preds.numpy().flatten(),\n",
    "        'Actual': targs.numpy().flatten()\n",
    "    })\n",
    "    results_df['Residual'] = results_df['Actual'] - results_df['Predicted']\n",
    "\n",
    "    # Calculate metrics\n",
    "    mae = np.mean(np.abs(results_df['Residual']))\n",
    "    rmse = np.sqrt(np.mean(results_df['Residual']**2))\n",
    "    r2 = 1 - (np.sum(results_df['Residual']**2) / np.sum((results_df['Actual'] - results_df['Actual'].mean())**2))\n",
    "\n",
    "    # Log and plot\n",
    "    logEvaluation(modelName, mae, rmse, r2, modelFolder, test_data_clean['Ticker'].unique())\n",
    "    plotResults(results_df, modelName, modelFolder)\n",
    "\n",
    "    # Show collection stats\n",
    "    print(f\"Collected {len(validTestData)} valid test tickers (target: {testSize})\")\n",
    "    if attempts >= max_attempts:\n",
    "        print(f\"Warning: Reached max attempts ({max_attempts}). Some invalid tickers may remain.\")  \n",
    "    print(f\"\\nEvaluation Results:\")\n",
    "    print(f\"MAE: {mae:.3f}\")\n",
    "    print(f\"RMSE: {rmse:.3f}\")\n",
    "    print(f\"R²: {r2:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model files in modelFolder:\n",
      "stockScreenerV1.0.pkl\n",
      "stockScreenerV1.1.pkl\n",
      "stockScreenerV1.10.pkl\n",
      "stockScreenerV1.2.pkl\n",
      "stockScreenerV1.3.pkl\n",
      "stockScreenerV1.4.pkl\n",
      "stockScreenerV1.5.pkl\n",
      "stockScreenerV1.6.pkl\n",
      "stockScreenerV1.7.pkl\n",
      "stockScreenerV1.8.pkl\n",
      "stockScreenerV1.9.pkl\n",
      "stockScreenerV2.0.pkl\n",
      "stockScreenerV2.1.pkl\n",
      "stockScreenerV2.2.pkl\n"
     ]
    }
   ],
   "source": [
    "print('Model files in modelFolder:')\n",
    "for file in modelFolder.glob('*.pkl'):\n",
    "    print(file.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model Name    stockScreenerV1.7\n",
       "Timestamp      2025-01-27 08:45\n",
       "MAE                       0.328\n",
       "RMSE                      0.739\n",
       "R2                        0.077\n",
       "Name: 7, dtype: object"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluations = pd.read_csv(modelFolder / 'modelEvaluations.csv')\n",
    "bestModel = evaluations.sort_values('MAE', ascending=True).iloc[0]\n",
    "bestModel.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.name == 'nt':\n",
    "    temp = pathlib.PosixPath\n",
    "    pathlib.PosixPath = pathlib.WindowsPath\n",
    "else:\n",
    "    pathlib.WindowsPath = pathlib.PosixPath\n",
    "\n",
    "importedModel = Path(f\"{bestModel['Model Name']}.pkl\") # Change this if you want to try other models\n",
    "learn = load_learner(modelFolder / importedModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predictionTarget = '95%'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'getTickerData' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[64], line 12\u001b[0m\n\u001b[0;32m      9\u001b[0m     predictionTickers \u001b[38;5;241m=\u001b[39m [predictionTarget]\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Fetch data for prediction tickers\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m dfPrediction \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([\u001b[43mgetTickerData\u001b[49m(ticker) \u001b[38;5;28;01mfor\u001b[39;00m ticker \u001b[38;5;129;01min\u001b[39;00m predictionTickers], ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Ensure dfPrediction is a DataFrame\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dfPrediction, \u001b[38;5;28mdict\u001b[39m):\n",
      "\u001b[1;31mNameError\u001b[0m: name 'getTickerData' is not defined"
     ]
    }
   ],
   "source": [
    "if predictionTarget != 'None':\n",
    "    if predictionTarget == 'ALL':\n",
    "        predictionTickers = tickers\n",
    "    elif predictionTarget.endswith('%'):\n",
    "        percentage = float(predictionTarget.strip('%')) / 100\n",
    "        num_tickers = int(len(tickers) * percentage)\n",
    "        predictionTickers = np.random.choice(tickers, size=num_tickers, replace=False).tolist()\n",
    "    else:\n",
    "        predictionTickers = [predictionTarget]\n",
    "\n",
    "    # Fetch data for prediction tickers\n",
    "    dfPrediction = pd.concat([getTickerData(ticker) for ticker in predictionTickers], ignore_index=True)\n",
    "\n",
    "    # Ensure dfPrediction is a DataFrame\n",
    "    if isinstance(dfPrediction, dict):\n",
    "        dfPrediction = pd.DataFrame([dfPrediction])\n",
    "\n",
    "    # Drop rows with NaN values in 'EV/EBIT' or 'ROIC'\n",
    "    dfPrediction = dfPrediction.dropna(subset=['EV/EBIT', 'ROIC'])\n",
    "\n",
    "    # Create test dataloader\n",
    "    dl = learn.dls.test_dl(dfPrediction)\n",
    "    dfPrediction.head()\n",
    "\n",
    "    # Make predictions\n",
    "    prediction = learn.get_preds(dl=dl)\n",
    "    adr_df = pd.read_csv(testFolder / 'filteredTickers.csv')\n",
    "    company_dict = dict(zip(adr_df['Ticker'], adr_df['Company']))\n",
    "\n",
    "    if predictionTarget == 'ALL' or predictionTarget.endswith('%'):\n",
    "        sorted_predictions = sorted(zip(predictionTickers, prediction[0]), key=lambda x: x[1], reverse=True)\n",
    "        print(f\"Got predictions for {len(sorted_predictions)} tickers, expected: {len(predictionTickers)}\")\n",
    "        print(f\"Prediction for best performing tickers:\")\n",
    "        for symbol, pred in sorted_predictions:\n",
    "            company_name = company_dict.get(symbol, 'Unknown')\n",
    "            print(f\"{symbol} ({company_name}): {pred[0].item() * 100:.2f}%\")\n",
    "    else:\n",
    "        company_name = company_dict.get(predictionTarget, 'Unknown')\n",
    "        print(f\"Prediction for {predictionTarget} ({company_name}):\")\n",
    "        print(f\"{prediction[0][0][0].item() * 100:.2f}%\")\n",
    "    print(\"Free money?!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
