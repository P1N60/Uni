{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stock Prediction Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Library Installation (if needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install libraries the first time\n",
    "#! pip install -U yfinance pandas pathlib numpy "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pathlib as Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['MATAS.CO', 'TRIFOR.CO', 'QQ.L', 'RNMBY', 'SAABF', 'BCKIY',\n",
       "       'BAESY', 'IVSO.ST', 'NSKFF', 'GMAB', 'GN.CO', 'NVDA', 'LLY',\n",
       "       'DANSKE.CO', 'CARL-B.CO', 'MAERSK-B.CO', 'RBREW.CO', 'ISS.CO',\n",
       "       'DSV.CO', 'SCHO.CO', 'NETC.CO', 'JYSK.CO', 'ABBN.SW', 'TER',\n",
       "       'PARKEN.CO', 'NFLX', 'TRMD-A.CO', 'STG.CO', 'NOVO-B.CO', 'EQNR',\n",
       "       'NKT.CO', 'NSIS-B.CO', 'KCC.OL'], dtype=object)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "symbols = pd.read_csv('../data/simple_screener_results.csv')['Ticker'].tolist()\n",
    "symbols = pd.Series(symbols).unique()\n",
    "symbols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download and Process Historical Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#symbols = ['AAPL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "Date",
         "rawType": "datetime64[ns, America/New_York]",
         "type": "unknown"
        },
        {
         "name": "Open",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "High",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Low",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Close",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Volume",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Dividends",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Stock Splits",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "3e0242a1-0eef-4e79-8e8d-b6f44d9153dd",
       "rows": [
        [
         "2015-04-06 00:00:00-04:00",
         "27.797597513549732",
         "28.47651388747033",
         "27.766331773633098",
         "28.440780639648438",
         "148776000",
         "0.0",
         "0.0"
        ],
        [
         "2015-04-07 00:00:00-04:00",
         "28.50554882317018",
         "28.61274517001839",
         "28.134825709858575",
         "28.141525268554688",
         "140049200",
         "0.0",
         "0.0"
        ],
        [
         "2015-04-08 00:00:00-04:00",
         "28.1057853751071",
         "28.228616268359257",
         "27.90925764975846",
         "28.04995346069336",
         "149316800",
         "0.0",
         "0.0"
        ],
        [
         "2015-04-09 00:00:00-04:00",
         "28.105784648045375",
         "28.26881458361224",
         "27.84002590093208",
         "28.264347076416016",
         "129936000",
         "0.0",
         "0.0"
        ],
        [
         "2015-04-10 00:00:00-04:00",
         "28.128121549163314",
         "28.40951491239349",
         "27.974026603464083",
         "28.38494873046875",
         "160752000",
         "0.0",
         "0.0"
        ],
        [
         "2015-04-13 00:00:00-04:00",
         "28.668576086828708",
         "28.713244353508433",
         "28.275520557103356",
         "28.329118728637695",
         "145460400",
         "0.0",
         "0.0"
        ],
        [
         "2015-04-14 00:00:00-04:00",
         "28.36261608247333",
         "28.427381315785674",
         "28.119189722922986",
         "28.206287384033203",
         "102098400",
         "0.0",
         "0.0"
        ],
        [
         "2015-04-15 00:00:00-04:00",
         "28.230859742623483",
         "28.3916542802627",
         "28.141528308031432",
         "28.31348991394043",
         "115881600",
         "0.0",
         "0.0"
        ],
        [
         "2015-04-16 00:00:00-04:00",
         "28.201817995892192",
         "28.384946617793247",
         "28.163852700781522",
         "28.1772518157959",
         "113476000",
         "0.0",
         "0.0"
        ],
        [
         "2015-04-17 00:00:00-04:00",
         "28.038787391161097",
         "28.1705498879956",
         "27.79535936446777",
         "27.860124588012695",
         "207828000",
         "0.0",
         "0.0"
        ],
        [
         "2015-04-20 00:00:00-04:00",
         "28.04325386237419",
         "28.612738366244688",
         "27.9539224586725",
         "28.49660873413086",
         "188217200",
         "0.0",
         "0.0"
        ],
        [
         "2015-04-21 00:00:00-04:00",
         "28.608274497684626",
         "28.630605218971876",
         "28.288914173119178",
         "28.342514038085938",
         "129740400",
         "0.0",
         "0.0"
        ],
        [
         "2015-04-22 00:00:00-04:00",
         "28.360381781433826",
         "28.78023720904142",
         "28.21075264383407",
         "28.72440528869629",
         "150618000",
         "0.0",
         "0.0"
        ],
        [
         "2015-04-23 00:00:00-04:00",
         "28.65294638863524",
         "29.126400051862642",
         "28.617213136543608",
         "28.958904266357422",
         "183083600",
         "0.0",
         "0.0"
        ],
        [
         "2015-04-24 00:00:00-04:00",
         "29.142027413436796",
         "29.173293149349522",
         "28.860632382512645",
         "29.09512710571289",
         "178103600",
         "0.0",
         "0.0"
        ],
        [
         "2015-04-27 00:00:00-04:00",
         "29.548483853042118",
         "29.731614180501488",
         "29.289422934655136",
         "29.624414443969727",
         "387816800",
         "0.0",
         "0.0"
        ],
        [
         "2015-04-28 00:00:00-04:00",
         "30.02864340140011",
         "30.04650661823436",
         "28.936571107983564",
         "29.157663345336914",
         "475696000",
         "0.0",
         "0.0"
        ],
        [
         "2015-04-29 00:00:00-04:00",
         "29.068329393289822",
         "29.38768631523267",
         "28.652939804382193",
         "28.728870391845703",
         "253544400",
         "0.0",
         "0.0"
        ],
        [
         "2015-04-30 00:00:00-04:00",
         "28.728872190424035",
         "28.728872190424035",
         "27.82216236838046",
         "27.949459075927734",
         "332781600",
         "0.0",
         "0.0"
        ],
        [
         "2015-05-01 00:00:00-04:00",
         "28.161619792405876",
         "29.06163176398269",
         "27.982958672716148",
         "28.79810333251953",
         "234050400",
         "0.0",
         "0.0"
        ],
        [
         "2015-05-04 00:00:00-04:00",
         "28.920938085357456",
         "29.15990036778687",
         "28.6440105016254",
         "28.74227523803711",
         "203953200",
         "0.0",
         "0.0"
        ],
        [
         "2015-05-05 00:00:00-04:00",
         "28.619443525852404",
         "28.68644251770801",
         "28.090157964842682",
         "28.09462547302246",
         "197085600",
         "0.0",
         "0.0"
        ],
        [
         "2015-05-06 00:00:00-04:00",
         "28.26435069961003",
         "28.306783504139773",
         "27.549702803532718",
         "27.918193817138672",
         "288564000",
         "0.0",
         "0.0"
        ],
        [
         "2015-05-07 00:00:00-04:00",
         "27.980982929703067",
         "28.274765359687283",
         "27.812787548105742",
         "28.090871810913086",
         "175763600",
         "0.13",
         "0.0"
        ],
        [
         "2015-05-08 00:00:00-04:00",
         "28.40932129864255",
         "28.620126724243164",
         "28.281492877176024",
         "28.620126724243164",
         "222201600",
         "0.0",
         "0.0"
        ],
        [
         "2015-05-11 00:00:00-04:00",
         "28.56855294137938",
         "28.606676826399557",
         "28.17385387201172",
         "28.328594207763672",
         "168143200",
         "0.0",
         "0.0"
        ],
        [
         "2015-05-12 00:00:00-04:00",
         "28.167121277698104",
         "28.454174472680343",
         "27.992198343941556",
         "28.227672576904297",
         "192640000",
         "0.0",
         "0.0"
        ],
        [
         "2015-05-13 00:00:00-04:00",
         "28.29046959853829",
         "28.523700783059066",
         "28.227676916354287",
         "28.25907325744629",
         "138776800",
         "0.0",
         "0.0"
        ],
        [
         "2015-05-14 00:00:00-04:00",
         "28.573030615872153",
         "28.91839027404785",
         "28.516965491873634",
         "28.91839027404785",
         "180814000",
         "0.0",
         "0.0"
        ],
        [
         "2015-05-15 00:00:00-04:00",
         "28.945309623559844",
         "29.03949863999735",
         "28.75244542172753",
         "28.87803077697754",
         "152832000",
         "0.0",
         "0.0"
        ],
        [
         "2015-05-18 00:00:00-04:00",
         "28.790570654600064",
         "29.315339523078",
         "28.78608448543216",
         "29.196481704711914",
         "203531600",
         "0.0",
         "0.0"
        ],
        [
         "2015-05-19 00:00:00-04:00",
         "29.308615199525867",
         "29.351225256440074",
         "29.073140910541177",
         "29.169574737548828",
         "178532800",
         "0.0",
         "0.0"
        ],
        [
         "2015-05-20 00:00:00-04:00",
         "29.153868981835398",
         "29.373643343707954",
         "29.005856210353336",
         "29.16732406616211",
         "145819600",
         "0.0",
         "0.0"
        ],
        [
         "2015-05-21 00:00:00-04:00",
         "29.169565528768555",
         "29.519411369068177",
         "29.115741775667175",
         "29.465587615966797",
         "158921600",
         "0.0",
         "0.0"
        ],
        [
         "2015-05-22 00:00:00-04:00",
         "29.512683945539724",
         "29.81991974360408",
         "29.46782910680423",
         "29.723485946655273",
         "182384000",
         "0.0",
         "0.0"
        ],
        [
         "2015-05-26 00:00:00-04:00",
         "29.736946195221503",
         "29.806466408616938",
         "28.956517125116992",
         "29.068647384643555",
         "282790400",
         "0.0",
         "0.0"
        ],
        [
         "2015-05-27 00:00:00-04:00",
         "29.23012019454992",
         "29.660700053598696",
         "29.165086138638944",
         "29.61136245727539",
         "183332800",
         "0.0",
         "0.0"
        ],
        [
         "2015-05-28 00:00:00-04:00",
         "29.570992247071736",
         "29.591174872566278",
         "29.400555484100128",
         "29.553050994873047",
         "122933200",
         "0.0",
         "0.0"
        ],
        [
         "2015-05-29 00:00:00-04:00",
         "29.42971019272997",
         "29.4790477861014",
         "29.131443258964264",
         "29.216663360595703",
         "203538000",
         "0.0",
         "0.0"
        ],
        [
         "2015-06-01 00:00:00-04:00",
         "29.216662593019702",
         "29.465591926874144",
         "29.16508362746622",
         "29.27496910095215",
         "128451200",
         "0.0",
         "0.0"
        ],
        [
         "2015-06-02 00:00:00-04:00",
         "29.122471946982635",
         "29.301881051578466",
         "29.001372768998277",
         "29.144899368286133",
         "134670400",
         "0.0",
         "0.0"
        ],
        [
         "2015-06-03 00:00:00-04:00",
         "29.30188684935407",
         "29.364679535095387",
         "29.131446626389298",
         "29.180784225463867",
         "123934000",
         "0.0",
         "0.0"
        ],
        [
         "2015-06-04 00:00:00-04:00",
         "29.05967732293898",
         "29.283937833143533",
         "28.90942319173519",
         "29.010339736938477",
         "153800400",
         "0.0",
         "0.0"
        ],
        [
         "2015-06-05 00:00:00-04:00",
         "29.041746681910034",
         "29.084356741927525",
         "28.786089743750175",
         "28.851123809814453",
         "142507200",
         "0.0",
         "0.0"
        ],
        [
         "2015-06-08 00:00:00-04:00",
         "28.907182205972028",
         "28.97670584773006",
         "28.442964668084898",
         "28.660497665405273",
         "210699200",
         "0.0",
         "0.0"
        ],
        [
         "2015-06-09 00:00:00-04:00",
         "28.4138094674482",
         "28.723290104771344",
         "28.171609386855806",
         "28.57527732849121",
         "224301600",
         "0.0",
         "0.0"
        ],
        [
         "2015-06-10 00:00:00-04:00",
         "28.687405006899652",
         "29.00585453132655",
         "28.67170683910317",
         "28.90269660949707",
         "156349200",
         "0.0",
         "0.0"
        ],
        [
         "2015-06-11 00:00:00-04:00",
         "28.969974639239496",
         "29.194235177107373",
         "28.812992947120826",
         "28.837661743164062",
         "141563600",
         "0.0",
         "0.0"
        ],
        [
         "2015-06-12 00:00:00-04:00",
         "28.74796135646813",
         "28.779357697578135",
         "28.505759544214744",
         "28.519214630126953",
         "147544800",
         "0.0",
         "0.0"
        ],
        [
         "2015-06-15 00:00:00-04:00",
         "28.2792526425688",
         "28.53490951126304",
         "28.191791172277195",
         "28.463146209716797",
         "175955600",
         "0.0",
         "0.0"
        ]
       ],
       "shape": {
        "columns": 7,
        "rows": 2517
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Dividends</th>\n",
       "      <th>Stock Splits</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2015-04-06 00:00:00-04:00</th>\n",
       "      <td>27.797598</td>\n",
       "      <td>28.476514</td>\n",
       "      <td>27.766332</td>\n",
       "      <td>28.440781</td>\n",
       "      <td>148776000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-04-07 00:00:00-04:00</th>\n",
       "      <td>28.505549</td>\n",
       "      <td>28.612745</td>\n",
       "      <td>28.134826</td>\n",
       "      <td>28.141525</td>\n",
       "      <td>140049200</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-04-08 00:00:00-04:00</th>\n",
       "      <td>28.105785</td>\n",
       "      <td>28.228616</td>\n",
       "      <td>27.909258</td>\n",
       "      <td>28.049953</td>\n",
       "      <td>149316800</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-04-09 00:00:00-04:00</th>\n",
       "      <td>28.105785</td>\n",
       "      <td>28.268815</td>\n",
       "      <td>27.840026</td>\n",
       "      <td>28.264347</td>\n",
       "      <td>129936000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-04-10 00:00:00-04:00</th>\n",
       "      <td>28.128122</td>\n",
       "      <td>28.409515</td>\n",
       "      <td>27.974027</td>\n",
       "      <td>28.384949</td>\n",
       "      <td>160752000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-03-31 00:00:00-04:00</th>\n",
       "      <td>217.009995</td>\n",
       "      <td>225.619995</td>\n",
       "      <td>216.229996</td>\n",
       "      <td>222.130005</td>\n",
       "      <td>65299300</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-04-01 00:00:00-04:00</th>\n",
       "      <td>219.809998</td>\n",
       "      <td>223.679993</td>\n",
       "      <td>218.899994</td>\n",
       "      <td>223.190002</td>\n",
       "      <td>36412700</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-04-02 00:00:00-04:00</th>\n",
       "      <td>221.320007</td>\n",
       "      <td>225.190002</td>\n",
       "      <td>221.020004</td>\n",
       "      <td>223.889999</td>\n",
       "      <td>35905900</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-04-03 00:00:00-04:00</th>\n",
       "      <td>205.539993</td>\n",
       "      <td>207.490005</td>\n",
       "      <td>201.250000</td>\n",
       "      <td>203.190002</td>\n",
       "      <td>103419000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-04-04 00:00:00-04:00</th>\n",
       "      <td>193.889999</td>\n",
       "      <td>199.880005</td>\n",
       "      <td>187.339996</td>\n",
       "      <td>188.380005</td>\n",
       "      <td>125569000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2517 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Open        High         Low       Close  \\\n",
       "Date                                                                        \n",
       "2015-04-06 00:00:00-04:00   27.797598   28.476514   27.766332   28.440781   \n",
       "2015-04-07 00:00:00-04:00   28.505549   28.612745   28.134826   28.141525   \n",
       "2015-04-08 00:00:00-04:00   28.105785   28.228616   27.909258   28.049953   \n",
       "2015-04-09 00:00:00-04:00   28.105785   28.268815   27.840026   28.264347   \n",
       "2015-04-10 00:00:00-04:00   28.128122   28.409515   27.974027   28.384949   \n",
       "...                               ...         ...         ...         ...   \n",
       "2025-03-31 00:00:00-04:00  217.009995  225.619995  216.229996  222.130005   \n",
       "2025-04-01 00:00:00-04:00  219.809998  223.679993  218.899994  223.190002   \n",
       "2025-04-02 00:00:00-04:00  221.320007  225.190002  221.020004  223.889999   \n",
       "2025-04-03 00:00:00-04:00  205.539993  207.490005  201.250000  203.190002   \n",
       "2025-04-04 00:00:00-04:00  193.889999  199.880005  187.339996  188.380005   \n",
       "\n",
       "                              Volume  Dividends  Stock Splits  \n",
       "Date                                                           \n",
       "2015-04-06 00:00:00-04:00  148776000        0.0           0.0  \n",
       "2015-04-07 00:00:00-04:00  140049200        0.0           0.0  \n",
       "2015-04-08 00:00:00-04:00  149316800        0.0           0.0  \n",
       "2015-04-09 00:00:00-04:00  129936000        0.0           0.0  \n",
       "2015-04-10 00:00:00-04:00  160752000        0.0           0.0  \n",
       "...                              ...        ...           ...  \n",
       "2025-03-31 00:00:00-04:00   65299300        0.0           0.0  \n",
       "2025-04-01 00:00:00-04:00   36412700        0.0           0.0  \n",
       "2025-04-02 00:00:00-04:00   35905900        0.0           0.0  \n",
       "2025-04-03 00:00:00-04:00  103419000        0.0           0.0  \n",
       "2025-04-04 00:00:00-04:00  125569000        0.0           0.0  \n",
       "\n",
       "[2517 rows x 7 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = yf.Ticker(\"AAPL\").history(period='10y')\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error for MATAS.CO: Timestamp('2024-03-31 00:00:00')\n",
      "Error for MATAS.CO: Timestamp('2023-03-31 00:00:00')\n",
      "Error for MATAS.CO: Timestamp('2022-03-31 00:00:00')\n",
      "Error for MATAS.CO: Timestamp('2021-03-31 00:00:00')\n",
      "Error for TRIFOR.CO: Timestamp('2024-12-31 00:00:00')\n",
      "Error for TRIFOR.CO: Timestamp('2023-12-31 00:00:00')\n",
      "Error for TRIFOR.CO: Timestamp('2022-12-31 00:00:00')\n",
      "Error for TRIFOR.CO: Timestamp('2021-12-31 00:00:00')\n",
      "Error for TRIFOR.CO: Timestamp('2020-12-31 00:00:00')\n",
      "Error for QQ.L: Timestamp('2024-03-31 00:00:00')\n",
      "Error for QQ.L: Timestamp('2023-03-31 00:00:00')\n",
      "Error for QQ.L: Timestamp('2022-03-31 00:00:00')\n",
      "Error for QQ.L: Timestamp('2021-03-31 00:00:00')\n",
      "Error for RNMBY: Timestamp('2024-12-31 00:00:00')\n",
      "Error for RNMBY: Timestamp('2023-12-31 00:00:00')\n",
      "Error for RNMBY: Timestamp('2022-12-31 00:00:00')\n",
      "Error for RNMBY: Timestamp('2021-12-31 00:00:00')\n",
      "Error for RNMBY: Timestamp('2020-12-31 00:00:00')\n",
      "Error for SAABF: Timestamp('2024-12-31 00:00:00')\n",
      "Error for SAABF: Timestamp('2023-12-31 00:00:00')\n",
      "Error for SAABF: Timestamp('2022-12-31 00:00:00')\n",
      "Error for SAABF: Timestamp('2021-12-31 00:00:00')\n",
      "Error for SAABF: Timestamp('2020-12-31 00:00:00')\n",
      "Error for BCKIY: Timestamp('2024-03-31 00:00:00')\n",
      "Error for BCKIY: Timestamp('2023-03-31 00:00:00')\n",
      "Error for BCKIY: Timestamp('2022-03-31 00:00:00')\n",
      "Error for BCKIY: Timestamp('2021-03-31 00:00:00')\n",
      "Error for BAESY: Timestamp('2024-12-31 00:00:00')\n",
      "Error for BAESY: Timestamp('2023-12-31 00:00:00')\n",
      "Error for BAESY: Timestamp('2022-12-31 00:00:00')\n",
      "Error for BAESY: Timestamp('2021-12-31 00:00:00')\n",
      "Error for BAESY: Timestamp('2020-12-31 00:00:00')\n",
      "Error for IVSO.ST: Timestamp('2024-12-31 00:00:00')\n",
      "Error for IVSO.ST: Timestamp('2023-12-31 00:00:00')\n",
      "Error for IVSO.ST: Timestamp('2022-12-31 00:00:00')\n",
      "Error for IVSO.ST: Timestamp('2021-12-31 00:00:00')\n",
      "Error for IVSO.ST: Timestamp('2020-12-31 00:00:00')\n",
      "Error for NSKFF: Timestamp('2024-12-31 00:00:00')\n",
      "Error for NSKFF: Timestamp('2023-12-31 00:00:00')\n",
      "Error for NSKFF: Timestamp('2022-12-31 00:00:00')\n",
      "Error for NSKFF: Timestamp('2021-12-31 00:00:00')\n",
      "Error for NSKFF: Timestamp('2020-12-31 00:00:00')\n",
      "Error for GMAB: Timestamp('2024-12-31 00:00:00')\n",
      "Error for GMAB: Timestamp('2023-12-31 00:00:00')\n",
      "Error for GMAB: Timestamp('2022-12-31 00:00:00')\n",
      "Error for GMAB: Timestamp('2021-12-31 00:00:00')\n",
      "Error for GMAB: Timestamp('2020-12-31 00:00:00')\n",
      "Error for GN.CO: Timestamp('2024-12-31 00:00:00')\n",
      "Error for GN.CO: Timestamp('2023-12-31 00:00:00')\n",
      "Error for GN.CO: Timestamp('2022-12-31 00:00:00')\n",
      "Error for GN.CO: Timestamp('2021-12-31 00:00:00')\n",
      "Error for GN.CO: Timestamp('2020-12-31 00:00:00')\n",
      "Error for NVDA: Timestamp('2025-01-31 00:00:00')\n",
      "Error for NVDA: Timestamp('2024-01-31 00:00:00')\n",
      "Error for NVDA: Timestamp('2023-01-31 00:00:00')\n",
      "Error for NVDA: Timestamp('2022-01-31 00:00:00')\n",
      "Error for NVDA: Timestamp('2021-01-31 00:00:00')\n",
      "Error for LLY: Timestamp('2024-12-31 00:00:00')\n",
      "Error for LLY: Timestamp('2023-12-31 00:00:00')\n",
      "Error for LLY: Timestamp('2022-12-31 00:00:00')\n",
      "Error for LLY: Timestamp('2021-12-31 00:00:00')\n",
      "Error for LLY: Timestamp('2020-12-31 00:00:00')\n",
      "Error for DANSKE.CO: Timestamp('2024-12-31 00:00:00')\n",
      "Error for DANSKE.CO: Timestamp('2023-12-31 00:00:00')\n",
      "Error for DANSKE.CO: Timestamp('2022-12-31 00:00:00')\n",
      "Error for DANSKE.CO: Timestamp('2021-12-31 00:00:00')\n",
      "Error for DANSKE.CO: Timestamp('2020-12-31 00:00:00')\n",
      "Error for CARL-B.CO: Timestamp('2024-12-31 00:00:00')\n",
      "Error for CARL-B.CO: Timestamp('2023-12-31 00:00:00')\n",
      "Error for CARL-B.CO: Timestamp('2022-12-31 00:00:00')\n",
      "Error for CARL-B.CO: Timestamp('2021-12-31 00:00:00')\n",
      "Error for CARL-B.CO: Timestamp('2020-12-31 00:00:00')\n",
      "Error for MAERSK-B.CO: Timestamp('2024-12-31 00:00:00')\n",
      "Error for MAERSK-B.CO: Timestamp('2023-12-31 00:00:00')\n",
      "Error for MAERSK-B.CO: Timestamp('2022-12-31 00:00:00')\n",
      "Error for MAERSK-B.CO: Timestamp('2021-12-31 00:00:00')\n",
      "Error for MAERSK-B.CO: Timestamp('2020-12-31 00:00:00')\n",
      "Error for RBREW.CO: Timestamp('2024-12-31 00:00:00')\n",
      "Error for RBREW.CO: Timestamp('2023-12-31 00:00:00')\n",
      "Error for RBREW.CO: Timestamp('2022-12-31 00:00:00')\n",
      "Error for RBREW.CO: Timestamp('2021-12-31 00:00:00')\n",
      "Error for RBREW.CO: Timestamp('2020-12-31 00:00:00')\n",
      "Error for ISS.CO: Timestamp('2024-12-31 00:00:00')\n",
      "Error for ISS.CO: Timestamp('2023-12-31 00:00:00')\n",
      "Error for ISS.CO: Timestamp('2022-12-31 00:00:00')\n",
      "Error for ISS.CO: Timestamp('2021-12-31 00:00:00')\n",
      "Error for ISS.CO: Timestamp('2020-12-31 00:00:00')\n",
      "Error for DSV.CO: Timestamp('2024-12-31 00:00:00')\n",
      "Error for DSV.CO: Timestamp('2023-12-31 00:00:00')\n",
      "Error for DSV.CO: Timestamp('2022-12-31 00:00:00')\n",
      "Error for DSV.CO: Timestamp('2021-12-31 00:00:00')\n",
      "Error for DSV.CO: Timestamp('2020-12-31 00:00:00')\n",
      "Error for SCHO.CO: Timestamp('2024-12-31 00:00:00')\n",
      "Error for SCHO.CO: Timestamp('2023-12-31 00:00:00')\n",
      "Error for SCHO.CO: Timestamp('2022-12-31 00:00:00')\n",
      "Error for SCHO.CO: Timestamp('2021-12-31 00:00:00')\n",
      "Error for SCHO.CO: Timestamp('2020-12-31 00:00:00')\n",
      "Error for NETC.CO: Timestamp('2024-12-31 00:00:00')\n",
      "Error for NETC.CO: Timestamp('2023-12-31 00:00:00')\n",
      "Error for NETC.CO: Timestamp('2022-12-31 00:00:00')\n",
      "Error for NETC.CO: Timestamp('2021-12-31 00:00:00')\n",
      "Error for NETC.CO: Timestamp('2020-12-31 00:00:00')\n",
      "Error for JYSK.CO: Timestamp('2024-12-31 00:00:00')\n",
      "Error for JYSK.CO: Timestamp('2023-12-31 00:00:00')\n",
      "Error for JYSK.CO: Timestamp('2022-12-31 00:00:00')\n",
      "Error for JYSK.CO: Timestamp('2021-12-31 00:00:00')\n",
      "Error for ABBN.SW: Timestamp('2024-12-31 00:00:00')\n",
      "Error for ABBN.SW: Timestamp('2023-12-31 00:00:00')\n",
      "Error for ABBN.SW: Timestamp('2022-12-31 00:00:00')\n",
      "Error for ABBN.SW: Timestamp('2021-12-31 00:00:00')\n",
      "Error for ABBN.SW: Timestamp('2020-12-31 00:00:00')\n",
      "Error for TER: Timestamp('2024-12-31 00:00:00')\n",
      "Error for TER: Timestamp('2023-12-31 00:00:00')\n",
      "Error for TER: Timestamp('2022-12-31 00:00:00')\n",
      "Error for TER: Timestamp('2021-12-31 00:00:00')\n",
      "Error for TER: Timestamp('2020-12-31 00:00:00')\n",
      "Error for PARKEN.CO: Timestamp('2024-12-31 00:00:00')\n",
      "Error for PARKEN.CO: Timestamp('2023-12-31 00:00:00')\n",
      "Error for PARKEN.CO: Timestamp('2022-12-31 00:00:00')\n",
      "Error for PARKEN.CO: Timestamp('2021-12-31 00:00:00')\n",
      "Error for PARKEN.CO: Timestamp('2020-12-31 00:00:00')\n",
      "Error for NFLX: Timestamp('2024-12-31 00:00:00')\n",
      "Error for NFLX: Timestamp('2023-12-31 00:00:00')\n",
      "Error for NFLX: Timestamp('2022-12-31 00:00:00')\n",
      "Error for NFLX: Timestamp('2021-12-31 00:00:00')\n",
      "Error for NFLX: Timestamp('2020-12-31 00:00:00')\n",
      "Error for TRMD-A.CO: Timestamp('2024-12-31 00:00:00')\n",
      "Error for TRMD-A.CO: Timestamp('2023-12-31 00:00:00')\n",
      "Error for TRMD-A.CO: Timestamp('2022-12-31 00:00:00')\n",
      "Error for TRMD-A.CO: Timestamp('2021-12-31 00:00:00')\n",
      "Error for TRMD-A.CO: Timestamp('2020-12-31 00:00:00')\n",
      "Error for STG.CO: Timestamp('2024-12-31 00:00:00')\n",
      "Error for STG.CO: Timestamp('2023-12-31 00:00:00')\n",
      "Error for STG.CO: Timestamp('2022-12-31 00:00:00')\n",
      "Error for STG.CO: Timestamp('2021-12-31 00:00:00')\n",
      "Error for STG.CO: Timestamp('2020-12-31 00:00:00')\n",
      "Error for NOVO-B.CO: Timestamp('2024-12-31 00:00:00')\n",
      "Error for NOVO-B.CO: Timestamp('2023-12-31 00:00:00')\n",
      "Error for NOVO-B.CO: Timestamp('2022-12-31 00:00:00')\n",
      "Error for NOVO-B.CO: Timestamp('2021-12-31 00:00:00')\n",
      "Error for NOVO-B.CO: Timestamp('2020-12-31 00:00:00')\n",
      "Error for EQNR: Timestamp('2024-12-31 00:00:00')\n",
      "Error for EQNR: Timestamp('2023-12-31 00:00:00')\n",
      "Error for EQNR: Timestamp('2022-12-31 00:00:00')\n",
      "Error for EQNR: Timestamp('2021-12-31 00:00:00')\n",
      "Error for EQNR: Timestamp('2020-12-31 00:00:00')\n",
      "Error for NKT.CO: Timestamp('2024-12-31 00:00:00')\n",
      "Error for NKT.CO: Timestamp('2023-12-31 00:00:00')\n",
      "Error for NKT.CO: Timestamp('2022-12-31 00:00:00')\n",
      "Error for NKT.CO: Timestamp('2021-12-31 00:00:00')\n",
      "Error for NKT.CO: Timestamp('2020-12-31 00:00:00')\n",
      "Error for NSIS-B.CO: Timestamp('2024-12-31 00:00:00')\n",
      "Error for NSIS-B.CO: Timestamp('2023-12-31 00:00:00')\n",
      "Error for NSIS-B.CO: Timestamp('2022-12-31 00:00:00')\n",
      "Error for NSIS-B.CO: Timestamp('2021-12-31 00:00:00')\n",
      "Error for NSIS-B.CO: Timestamp('2020-12-31 00:00:00')\n",
      "Error for KCC.OL: Timestamp('2024-12-31 00:00:00')\n",
      "Error for KCC.OL: Timestamp('2023-12-31 00:00:00')\n",
      "Error for KCC.OL: Timestamp('2022-12-31 00:00:00')\n",
      "Error for KCC.OL: Timestamp('2021-12-31 00:00:00')\n",
      "Error for KCC.OL: Timestamp('2020-12-31 00:00:00')\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame()\n",
    "for symbol in symbols:\n",
    "    ticker = yf.Ticker(symbol)\n",
    "    earning_dates = ticker.cash_flow.columns.tolist()\n",
    "    cash_flow = ticker.cash_flow\n",
    "    cash_flow_columns = cash_flow[earning_dates[0]].keys().tolist()\n",
    "    balance_sheet = ticker.balance_sheet\n",
    "    balance_sheet_columns = balance_sheet[earning_dates[0]].keys().tolist()\n",
    "    income_statement = ticker.income_stmt\n",
    "    income_statement_columns = income_statement[earning_dates[0]].keys().tolist()\n",
    "\n",
    "    for earning_date in earning_dates:\n",
    "        try:\n",
    "            current_ticker_data = {'Ticker': symbol, 'Date': earning_date}\n",
    "\n",
    "            price_data = ticker.history(period='10y')\n",
    "            current_ticker_data['Close Price'] = price_data.loc[earning_date, 'Close']\n",
    "\n",
    "            for column in cash_flow_columns:\n",
    "                current_ticker_data[column] = cash_flow[earning_date][column]\n",
    "\n",
    "            for column in balance_sheet_columns:\n",
    "                current_ticker_data[column] = balance_sheet[earning_date][column]\n",
    "\n",
    "            for column in income_statement_columns:\n",
    "                current_ticker_data[column] = income_statement[earning_date][column]\n",
    "\n",
    "            df = pd.concat([df, pd.DataFrame([current_ticker_data])], ignore_index=True)\n",
    "        except Exception as e:\n",
    "            print(f'Error for {symbol}: {e}')\n",
    "\n",
    "df.to_csv('../data/earnings_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'testFolder' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[59], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m tickers \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(testFolder \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfilteredTickers.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTicker\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      2\u001b[0m trainingTickers \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mchoice(tickers, size\u001b[38;5;241m=\u001b[39mtrainingSize, replace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m      3\u001b[0m trainingRowAmount \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(pd\u001b[38;5;241m.\u001b[39mread_csv(testFolder \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfilteredTickers.csv\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'testFolder' is not defined"
     ]
    }
   ],
   "source": [
    "tickers = pd.read_csv(testFolder / 'filteredTickers.csv')['Ticker']\n",
    "trainingTickers = np.random.choice(tickers, size=trainingSize, replace=False)\n",
    "trainingRowAmount = len(pd.read_csv(testFolder / 'filteredTickers.csv'))\n",
    "\n",
    "if getNewData:\n",
    "    histData = pd.DataFrame()\n",
    "    valid_tickers = []\n",
    "    \n",
    "    for ticker in trainingTickers:\n",
    "        print(f\"Processing {ticker}...\")\n",
    "        try:\n",
    "            data = calculateFutureYearChange(ticker, timeFrame)\n",
    "            if not data.empty:\n",
    "                data['Ticker'] = ticker\n",
    "                data['Industry'] = yf.Ticker(ticker).info.get('industry', 'Unknown')\n",
    "                data['Date'] = pd.to_datetime(data['Date']).dt.tz_localize(None)\n",
    "                \n",
    "                # Enrich individual ticker data first\n",
    "                ticker_data = enrichDataWithMetrics(data)\n",
    "                histData = pd.concat([histData, ticker_data])\n",
    "                \n",
    "                # Check if metrics were added\n",
    "                if 'ROIC' not in ticker_data.columns:\n",
    "                    print(f\"WARNING: Failed to add metrics for {ticker}\")\n",
    "                \n",
    "                valid_tickers.append(ticker)\n",
    "            else:\n",
    "                print(f\"Skipped {ticker} - insufficient data\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {ticker}: {e}\")\n",
    "\n",
    "    print(f\"\\nColumns in final dataset: {histData.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Short visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if getNewData:\n",
    "    histData.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if trainNewModel:\n",
    "    TRAINING_CUTOFF = pd.to_datetime('2023-01-01')\n",
    "\n",
    "    df = pd.read_csv(dataFolder / trainingData)\n",
    "    dfCleaned = df.dropna(subset=['EV/EBIT', 'ROIC']).copy()\n",
    "\n",
    "    # Convert 'Date' to datetime, parse UTC-aware dates, then make naive\n",
    "    dfCleaned['Date'] = pd.to_datetime(dfCleaned['Date'], errors='coerce', utc=True).dt.tz_convert(None)\n",
    "    dfCleaned = dfCleaned.dropna(subset=['Date'])\n",
    "\n",
    "    # Clean 'EV/EBIT' and reset index\n",
    "    dfCleaned['EV/EBIT'] = dfCleaned['EV/EBIT'].replace([np.inf, -np.inf], np.nan)\n",
    "    dfCleaned = dfCleaned.dropna(subset=['EV/EBIT', 'ROIC'])\n",
    "    dfCleaned = dfCleaned.reset_index(drop=True)\n",
    "    dfCleaned.to_csv(dataFolder / trainingData, index=False)\n",
    "\n",
    "    # Check for empty data\n",
    "    if dfCleaned.empty:\n",
    "        raise ValueError(\"The cleaned DataFrame is empty.\")\n",
    "\n",
    "    # Create splits with valid indices\n",
    "    train_mask = dfCleaned['Date'] < TRAINING_CUTOFF\n",
    "    valid_mask = ~train_mask\n",
    "    splits = (list(dfCleaned[train_mask].index), list(dfCleaned[valid_mask].index))\n",
    "\n",
    "    if not splits[0] or not splits[1]:\n",
    "        raise ValueError(\"Empty training or validation split.\")\n",
    "\n",
    "    # Proceed with TabularPandas\n",
    "    to = TabularPandas(\n",
    "        dfCleaned, \n",
    "        procs=[Categorify, FillMissing, Normalize],\n",
    "        y_names=yNames,\n",
    "        cat_names=catNames, \n",
    "        cont_names=contNames,\n",
    "        splits=splits\n",
    "    )\n",
    "\n",
    "    dls = to.dataloaders(bs=batchSize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if trainNewModel:\n",
    "    learn = tabular_learner(dls, metrics=[rmse, mae])\n",
    "\n",
    "    # Learning rate finder\n",
    "    lr_find_results = learn.lr_find(suggest_funcs=(minimum, steep))\n",
    "\n",
    "    # Debugging information\n",
    "    print(f\"Learning rate finder results: {lr_find_results}\")\n",
    "\n",
    "    # Extract learning rates\n",
    "    lr_min, lr_steep = lr_find_results\n",
    "\n",
    "    # Check if learning rates are valid\n",
    "    if lr_min is None or lr_steep is None or lr_min == 0 or lr_steep == 0:\n",
    "        raise ValueError(\"Learning rate finder did not return valid learning rates.\")\n",
    "\n",
    "    # Train\n",
    "    print(f\"Training for {epochs} epochs...\")\n",
    "    learn.fit_one_cycle(epochs, lr_max=lr_steep)\n",
    "    print(\"Model training complete\")\n",
    "\n",
    "    learn.export(modelFolder / f'{modelName}.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing and benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logEvaluation(model_name, mae, rmse, r2, model_folder, test_tickers):\n",
    "    \"\"\"Log evaluation metrics to CSV file\"\"\"\n",
    "    log_file = model_folder / \"modelEvaluations.csv\"\n",
    "    \n",
    "    new_entry_df = pd.DataFrame([{\n",
    "        \"Model Name\": model_name,\n",
    "        \"Timestamp\": datetime.now().strftime('%Y-%m-%d %H:%M'),\n",
    "        \"MAE\": f'{mae:.3f}',\n",
    "        \"RMSE\": f'{rmse:.3f}',\n",
    "        \"R2\": f'{r2:.3f}',\n",
    "        \"Epochs\": epochs,\n",
    "        \"Training Size\": trainingSize,\n",
    "        \"Training Rows\": trainingRowAmount,\n",
    "        \"Test Size\": len(test_tickers),\n",
    "        \"Cat Names\": catNames,\n",
    "        \"Cont Names\": contNames,\n",
    "    }])\n",
    "    \n",
    "    try:\n",
    "        log_df = pd.read_csv(log_file)\n",
    "        log_df = pd.concat([log_df, new_entry_df], ignore_index=True)\n",
    "    except FileNotFoundError:\n",
    "        log_df = new_entry_df\n",
    "        \n",
    "    log_df.to_csv(log_file, index=False)\n",
    "    print(f\"Logged evaluation results to {log_file}\")\n",
    "\n",
    "def plotResults(results_df, model_name, model_folder):\n",
    "    \"\"\"Create and save visualization plots using all data points.\"\"\"\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.scatter(results_df['Actual'], results_df['Predicted'], alpha=0.7, label='Predictions')\n",
    "    min_val = min(results_df['Actual'].min(), results_df['Predicted'].min())\n",
    "    max_val = max(results_df['Actual'].max(), results_df['Predicted'].max())\n",
    "    plt.plot([min_val, max_val], [min_val, max_val], color='red', linestyle='--', label='Perfect Prediction')\n",
    "    plt.title(f'Predicted vs. Actual Returns - {model_name}', fontsize=14)\n",
    "    plt.xlabel('Actual Returns')\n",
    "    plt.ylabel('Predicted Returns')\n",
    "    plt.legend()\n",
    "    plt.grid(alpha=0.5)\n",
    "    \n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.scatter(results_df['Predicted'], results_df['Residual'], alpha=0.7)\n",
    "    plt.axhline(y=0, color='r', linestyle='--')\n",
    "    plt.title('Residual Plot')\n",
    "    plt.xlabel('Predicted Returns')\n",
    "    plt.ylabel('Residual')\n",
    "    plt.grid(alpha=0.5)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "if trainNewModel:\n",
    "    nonTrainingTickers = list(set(tickers) - set(trainingTickers))\n",
    "    validTestData = []\n",
    "    attempted_tickers = set()\n",
    "    attempts = 0\n",
    "\n",
    "    if testSize * 4 <= len(tickers):  \n",
    "        max_attempts = testSize * 4 # Prevent infinite loops\n",
    "    else:\n",
    "        max_attempts = len(tickers)\n",
    "\n",
    "    # Keep trying until we reach testSize or exhaust attempts\n",
    "    while len(validTestData) < testSize and attempts < max_attempts:\n",
    "        # Get a new ticker we haven't tried yet\n",
    "        remaining_tickers = [t for t in nonTrainingTickers if t not in attempted_tickers]\n",
    "        if not remaining_tickers:  # If all tried, reset attempted list\n",
    "            attempted_tickers = set()\n",
    "            remaining_tickers = nonTrainingTickers\n",
    "            \n",
    "        ticker = np.random.choice(remaining_tickers)\n",
    "        attempted_tickers.add(ticker)\n",
    "        attempts += 1\n",
    "\n",
    "        # Fetch and validate data\n",
    "        data = getTickerDataFrom1YrAgo(ticker)\n",
    "        if not data.empty and not data[['EV/EBIT', 'ROIC']].isna().any().any():\n",
    "            validTestData.append(data)\n",
    "\n",
    "    if not validTestData:\n",
    "        raise ValueError(\"No valid test data collected after multiple attempts\")\n",
    "        \n",
    "    # Trim to exact testSize if we collected more\n",
    "    validTestData = validTestData[:testSize]  \n",
    "    combinedTestData = pd.concat(validTestData, ignore_index=True)\n",
    "\n",
    "    # Clean data\n",
    "    test_data_clean = combinedTestData.dropna(subset=['EV/EBIT', 'ROIC', 'Future Year Change'])\n",
    "    \n",
    "    if test_data_clean.empty:\n",
    "        raise ValueError(\"No valid test data after cleaning NaN values\")\n",
    "\n",
    "    # Create test dataloader\n",
    "    test_dl = learn.dls.test_dl(test_data_clean)\n",
    "    preds, targs = learn.get_preds(dl=test_dl)\n",
    "    \n",
    "    # Create results DataFrame\n",
    "    results_df = pd.DataFrame({\n",
    "        'Predicted': preds.numpy().flatten(),\n",
    "        'Actual': targs.numpy().flatten()\n",
    "    })\n",
    "    results_df['Residual'] = results_df['Actual'] - results_df['Predicted']\n",
    "\n",
    "    # Calculate metrics\n",
    "    mae = np.mean(np.abs(results_df['Residual']))\n",
    "    rmse = np.sqrt(np.mean(results_df['Residual']**2))\n",
    "    r2 = 1 - (np.sum(results_df['Residual']**2) / np.sum((results_df['Actual'] - results_df['Actual'].mean())**2))\n",
    "\n",
    "    # Log and plot\n",
    "    logEvaluation(modelName, mae, rmse, r2, modelFolder, test_data_clean['Ticker'].unique())\n",
    "    plotResults(results_df, modelName, modelFolder)\n",
    "\n",
    "    # Show collection stats\n",
    "    print(f\"Collected {len(validTestData)} valid test tickers (target: {testSize})\")\n",
    "    if attempts >= max_attempts:\n",
    "        print(f\"Warning: Reached max attempts ({max_attempts}). Some invalid tickers may remain.\")  \n",
    "    print(f\"\\nEvaluation Results:\")\n",
    "    print(f\"MAE: {mae:.3f}\")\n",
    "    print(f\"RMSE: {rmse:.3f}\")\n",
    "    print(f\"R²: {r2:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model files in modelFolder:\n",
      "stockScreenerV1.0.pkl\n",
      "stockScreenerV1.1.pkl\n",
      "stockScreenerV1.10.pkl\n",
      "stockScreenerV1.2.pkl\n",
      "stockScreenerV1.3.pkl\n",
      "stockScreenerV1.4.pkl\n",
      "stockScreenerV1.5.pkl\n",
      "stockScreenerV1.6.pkl\n",
      "stockScreenerV1.7.pkl\n",
      "stockScreenerV1.8.pkl\n",
      "stockScreenerV1.9.pkl\n",
      "stockScreenerV2.0.pkl\n",
      "stockScreenerV2.1.pkl\n",
      "stockScreenerV2.2.pkl\n"
     ]
    }
   ],
   "source": [
    "print('Model files in modelFolder:')\n",
    "for file in modelFolder.glob('*.pkl'):\n",
    "    print(file.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model Name    stockScreenerV1.7\n",
       "Timestamp      2025-01-27 08:45\n",
       "MAE                       0.328\n",
       "RMSE                      0.739\n",
       "R2                        0.077\n",
       "Name: 7, dtype: object"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluations = pd.read_csv(modelFolder / 'modelEvaluations.csv')\n",
    "bestModel = evaluations.sort_values('MAE', ascending=True).iloc[0]\n",
    "bestModel.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.name == 'nt':\n",
    "    temp = pathlib.PosixPath\n",
    "    pathlib.PosixPath = pathlib.WindowsPath\n",
    "else:\n",
    "    pathlib.WindowsPath = pathlib.PosixPath\n",
    "\n",
    "importedModel = Path(f\"{bestModel['Model Name']}.pkl\") # Change this if you want to try other models\n",
    "learn = load_learner(modelFolder / importedModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predictionTarget = '95%'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'getTickerData' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[64], line 12\u001b[0m\n\u001b[0;32m      9\u001b[0m     predictionTickers \u001b[38;5;241m=\u001b[39m [predictionTarget]\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Fetch data for prediction tickers\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m dfPrediction \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([\u001b[43mgetTickerData\u001b[49m(ticker) \u001b[38;5;28;01mfor\u001b[39;00m ticker \u001b[38;5;129;01min\u001b[39;00m predictionTickers], ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Ensure dfPrediction is a DataFrame\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dfPrediction, \u001b[38;5;28mdict\u001b[39m):\n",
      "\u001b[1;31mNameError\u001b[0m: name 'getTickerData' is not defined"
     ]
    }
   ],
   "source": [
    "if predictionTarget != 'None':\n",
    "    if predictionTarget == 'ALL':\n",
    "        predictionTickers = tickers\n",
    "    elif predictionTarget.endswith('%'):\n",
    "        percentage = float(predictionTarget.strip('%')) / 100\n",
    "        num_tickers = int(len(tickers) * percentage)\n",
    "        predictionTickers = np.random.choice(tickers, size=num_tickers, replace=False).tolist()\n",
    "    else:\n",
    "        predictionTickers = [predictionTarget]\n",
    "\n",
    "    # Fetch data for prediction tickers\n",
    "    dfPrediction = pd.concat([getTickerData(ticker) for ticker in predictionTickers], ignore_index=True)\n",
    "\n",
    "    # Ensure dfPrediction is a DataFrame\n",
    "    if isinstance(dfPrediction, dict):\n",
    "        dfPrediction = pd.DataFrame([dfPrediction])\n",
    "\n",
    "    # Drop rows with NaN values in 'EV/EBIT' or 'ROIC'\n",
    "    dfPrediction = dfPrediction.dropna(subset=['EV/EBIT', 'ROIC'])\n",
    "\n",
    "    # Create test dataloader\n",
    "    dl = learn.dls.test_dl(dfPrediction)\n",
    "    dfPrediction.head()\n",
    "\n",
    "    # Make predictions\n",
    "    prediction = learn.get_preds(dl=dl)\n",
    "    adr_df = pd.read_csv(testFolder / 'filteredTickers.csv')\n",
    "    company_dict = dict(zip(adr_df['Ticker'], adr_df['Company']))\n",
    "\n",
    "    if predictionTarget == 'ALL' or predictionTarget.endswith('%'):\n",
    "        sorted_predictions = sorted(zip(predictionTickers, prediction[0]), key=lambda x: x[1], reverse=True)\n",
    "        print(f\"Got predictions for {len(sorted_predictions)} tickers, expected: {len(predictionTickers)}\")\n",
    "        print(f\"Prediction for best performing tickers:\")\n",
    "        for symbol, pred in sorted_predictions:\n",
    "            company_name = company_dict.get(symbol, 'Unknown')\n",
    "            print(f\"{symbol} ({company_name}): {pred[0].item() * 100:.2f}%\")\n",
    "    else:\n",
    "        company_name = company_dict.get(predictionTarget, 'Unknown')\n",
    "        print(f\"Prediction for {predictionTarget} ({company_name}):\")\n",
    "        print(f\"{prediction[0][0][0].item() * 100:.2f}%\")\n",
    "    print(\"Free money?!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
