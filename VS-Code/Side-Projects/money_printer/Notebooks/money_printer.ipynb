{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stock Prediction Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Library Installation (if needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install libraries the first time\n",
    "#! pip install -U yfinance pandas pathlib numpy "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pathlib as Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['MATAS.CO', 'TRIFOR.CO', 'QQ.L', 'RNMBY', 'SAABF', 'BCKIY',\n",
       "       'BAESY', 'IVSO.ST', 'NSKFF', 'GMAB', 'GN.CO', 'NVDA', 'LLY',\n",
       "       'DANSKE.CO', 'CARL-B.CO', 'MAERSK-B.CO', 'RBREW.CO', 'ISS.CO',\n",
       "       'DSV.CO', 'SCHO.CO', 'NETC.CO', 'JYSK.CO', 'ABBN.SW', 'TER',\n",
       "       'PARKEN.CO', 'NFLX', 'TRMD-A.CO', 'STG.CO', 'NOVO-B.CO', 'EQNR',\n",
       "       'NKT.CO', 'NSIS-B.CO', 'KCC.OL'], dtype=object)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "symbols = pd.read_csv('../data/simple_screener_results.csv')['Ticker'].tolist()\n",
    "symbols = pd.Series(symbols).unique()\n",
    "symbols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download and Process Historical Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#symbols = ['AAPL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "Date",
         "rawType": "datetime64[ns, America/New_York]",
         "type": "unknown"
        },
        {
         "name": "Open",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "High",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Low",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Close",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Volume",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Dividends",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Stock Splits",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "8aab1319-6551-4f06-b35e-6598b8cf5630",
       "rows": [
        [
         "2015-04-06 00:00:00-04:00",
         "27.797597513549732",
         "28.47651388747033",
         "27.766331773633098",
         "28.440780639648438",
         "148776000",
         "0.0",
         "0.0"
        ],
        [
         "2015-04-07 00:00:00-04:00",
         "28.505541095085928",
         "28.61273741287234",
         "28.134818082280347",
         "28.141517639160156",
         "140049200",
         "0.0",
         "0.0"
        ],
        [
         "2015-04-08 00:00:00-04:00",
         "28.105791108542423",
         "28.22862202685145",
         "27.909263343103138",
         "28.049959182739258",
         "149316800",
         "0.0",
         "0.0"
        ],
        [
         "2015-04-09 00:00:00-04:00",
         "28.105784648045375",
         "28.26881458361224",
         "27.84002590093208",
         "28.264347076416016",
         "129936000",
         "0.0",
         "0.0"
        ],
        [
         "2015-04-10 00:00:00-04:00",
         "28.128119659072386",
         "28.409513003394114",
         "27.974024723727684",
         "28.384946823120117",
         "160752000",
         "0.0",
         "0.0"
        ],
        [
         "2015-04-13 00:00:00-04:00",
         "28.668572226421336",
         "28.713240487086196",
         "28.275516749623428",
         "28.32911491394043",
         "145460400",
         "0.0",
         "0.0"
        ],
        [
         "2015-04-14 00:00:00-04:00",
         "28.36261224663373",
         "28.427377471187047",
         "28.11918592000505",
         "28.206283569335938",
         "102098400",
         "0.0",
         "0.0"
        ],
        [
         "2015-04-15 00:00:00-04:00",
         "28.230855939059044",
         "28.391650455034295",
         "28.14152451650268",
         "28.313486099243164",
         "115881600",
         "0.0",
         "0.0"
        ],
        [
         "2015-04-16 00:00:00-04:00",
         "28.201817995892192",
         "28.384946617793247",
         "28.163852700781522",
         "28.1772518157959",
         "113476000",
         "0.0",
         "0.0"
        ],
        [
         "2015-04-17 00:00:00-04:00",
         "28.038795069481793",
         "28.17055760239898",
         "27.79536697612658",
         "27.860132217407227",
         "207828000",
         "0.0",
         "0.0"
        ],
        [
         "2015-04-20 00:00:00-04:00",
         "28.043255739378658",
         "28.612740281366165",
         "27.9539243296978",
         "28.496610641479492",
         "188217200",
         "0.0",
         "0.0"
        ],
        [
         "2015-04-21 00:00:00-04:00",
         "28.608278348151323",
         "28.630609072444127",
         "28.288917980602285",
         "28.342517852783203",
         "129740400",
         "0.0",
         "0.0"
        ],
        [
         "2015-04-22 00:00:00-04:00",
         "28.360381781433826",
         "28.78023720904142",
         "28.21075264383407",
         "28.72440528869629",
         "150618000",
         "0.0",
         "0.0"
        ],
        [
         "2015-04-23 00:00:00-04:00",
         "28.65293883984712",
         "29.126392378340356",
         "28.617205597169626",
         "28.95889663696289",
         "183083600",
         "0.0",
         "0.0"
        ],
        [
         "2015-04-24 00:00:00-04:00",
         "29.142027413436796",
         "29.173293149349522",
         "28.860632382512645",
         "29.09512710571289",
         "178103600",
         "0.0",
         "0.0"
        ],
        [
         "2015-04-27 00:00:00-04:00",
         "29.54849146288168",
         "29.731621837503962",
         "29.289430477776826",
         "29.624422073364258",
         "387816800",
         "0.0",
         "0.0"
        ],
        [
         "2015-04-28 00:00:00-04:00",
         "30.028641437076313",
         "30.046504652742044",
         "28.93656921509768",
         "29.15766143798828",
         "475696000",
         "0.0",
         "0.0"
        ],
        [
         "2015-04-29 00:00:00-04:00",
         "29.068329393289822",
         "29.38768631523267",
         "28.652939804382193",
         "28.728870391845703",
         "253544400",
         "0.0",
         "0.0"
        ],
        [
         "2015-04-30 00:00:00-04:00",
         "28.728870229886088",
         "28.728870229886088",
         "27.82216046971891",
         "27.9494571685791",
         "332781600",
         "0.0",
         "0.0"
        ],
        [
         "2015-05-01 00:00:00-04:00",
         "28.161621657599095",
         "29.061633688785268",
         "27.982960526076326",
         "28.798105239868164",
         "234050400",
         "0.0",
         "0.0"
        ],
        [
         "2015-05-04 00:00:00-04:00",
         "28.920934246947915",
         "29.15989649766207",
         "28.644006699969907",
         "28.742271423339844",
         "203953200",
         "0.0",
         "0.0"
        ],
        [
         "2015-05-05 00:00:00-04:00",
         "28.619443525852404",
         "28.68644251770801",
         "28.090157964842682",
         "28.09462547302246",
         "197085600",
         "0.0",
         "0.0"
        ],
        [
         "2015-05-06 00:00:00-04:00",
         "28.26435069961003",
         "28.306783504139773",
         "27.549702803532718",
         "27.918193817138672",
         "288564000",
         "0.0",
         "0.0"
        ],
        [
         "2015-05-07 00:00:00-04:00",
         "27.980988629364855",
         "28.274771119191875",
         "27.81279321350652",
         "28.090877532958984",
         "175763600",
         "0.13",
         "0.0"
        ],
        [
         "2015-05-08 00:00:00-04:00",
         "28.40931940534275",
         "28.62012481689453",
         "28.281490992395174",
         "28.62012481689453",
         "222201600",
         "0.0",
         "0.0"
        ],
        [
         "2015-05-11 00:00:00-04:00",
         "28.56854717086461",
         "28.606671048184207",
         "28.17384818122156",
         "28.328588485717773",
         "168143200",
         "0.0",
         "0.0"
        ],
        [
         "2015-05-12 00:00:00-04:00",
         "28.167121277698104",
         "28.454174472680343",
         "27.992198343941556",
         "28.227672576904297",
         "192640000",
         "0.0",
         "0.0"
        ],
        [
         "2015-05-13 00:00:00-04:00",
         "28.290463870135095",
         "28.523695007429993",
         "28.227671200665686",
         "28.25906753540039",
         "138776800",
         "0.0",
         "0.0"
        ],
        [
         "2015-05-14 00:00:00-04:00",
         "28.57303438501216",
         "28.918394088745117",
         "28.51696925361795",
         "28.918394088745117",
         "180814000",
         "0.0",
         "0.0"
        ],
        [
         "2015-05-15 00:00:00-04:00",
         "28.94530579997525",
         "29.039494803970687",
         "28.752441623619692",
         "28.878026962280273",
         "152832000",
         "0.0",
         "0.0"
        ],
        [
         "2015-05-18 00:00:00-04:00",
         "28.79056501210627",
         "29.31533377773786",
         "28.786078843817585",
         "29.196475982666016",
         "203531600",
         "0.0",
         "0.0"
        ],
        [
         "2015-05-19 00:00:00-04:00",
         "29.30860753376486",
         "29.351217579534275",
         "29.073133306369215",
         "29.169567108154297",
         "178532800",
         "0.0",
         "0.0"
        ],
        [
         "2015-05-20 00:00:00-04:00",
         "29.15386707536664",
         "29.37364142286742",
         "29.005854313563628",
         "29.167322158813477",
         "145819600",
         "0.0",
         "0.0"
        ],
        [
         "2015-05-21 00:00:00-04:00",
         "29.169567416955264",
         "29.519413279900895",
         "29.115743660369798",
         "29.46558952331543",
         "158921600",
         "0.0",
         "0.0"
        ],
        [
         "2015-05-22 00:00:00-04:00",
         "29.512683945539724",
         "29.81991974360408",
         "29.46782910680423",
         "29.723485946655273",
         "182384000",
         "0.0",
         "0.0"
        ],
        [
         "2015-05-26 00:00:00-04:00",
         "29.736952048819326",
         "29.806472275899534",
         "28.956522825090502",
         "29.068653106689453",
         "282790400",
         "0.0",
         "0.0"
        ],
        [
         "2015-05-27 00:00:00-04:00",
         "29.23012019454992",
         "29.660700053598696",
         "29.165086138638944",
         "29.61136245727539",
         "183332800",
         "0.0",
         "0.0"
        ],
        [
         "2015-05-28 00:00:00-04:00",
         "29.571003698111085",
         "29.591186331421124",
         "29.400566869139727",
         "29.553062438964844",
         "122933200",
         "0.0",
         "0.0"
        ],
        [
         "2015-05-29 00:00:00-04:00",
         "29.42970827147302",
         "29.47904586162355",
         "29.131441357179046",
         "29.21666145324707",
         "203538000",
         "0.0",
         "0.0"
        ],
        [
         "2015-06-01 00:00:00-04:00",
         "29.216662593019702",
         "29.465591926874144",
         "29.16508362746622",
         "29.27496910095215",
         "128451200",
         "0.0",
         "0.0"
        ],
        [
         "2015-06-02 00:00:00-04:00",
         "29.122475758744436",
         "29.30188488682264",
         "29.001376564909734",
         "29.1449031829834",
         "134670400",
         "0.0",
         "0.0"
        ],
        [
         "2015-06-03 00:00:00-04:00",
         "29.301873442504082",
         "29.364666099515095",
         "29.131433297522904",
         "29.180770874023438",
         "123934000",
         "0.0",
         "0.0"
        ],
        [
         "2015-06-04 00:00:00-04:00",
         "29.059683054716306",
         "29.28394360915436",
         "28.90942889387615",
         "29.010345458984375",
         "153800400",
         "0.0",
         "0.0"
        ],
        [
         "2015-06-05 00:00:00-04:00",
         "29.041744761959322",
         "29.084354819159856",
         "28.786087840700947",
         "28.85112190246582",
         "142507200",
         "0.0",
         "0.0"
        ],
        [
         "2015-06-08 00:00:00-04:00",
         "28.907180282206603",
         "28.976703919337854",
         "28.442962775213026",
         "28.66049575805664",
         "210699200",
         "0.0",
         "0.0"
        ],
        [
         "2015-06-09 00:00:00-04:00",
         "28.41381895030291",
         "28.723299690912448",
         "28.171618788878405",
         "28.575286865234375",
         "224301600",
         "0.0",
         "0.0"
        ],
        [
         "2015-06-10 00:00:00-04:00",
         "28.68740690004075",
         "29.005856445482788",
         "28.671708731208312",
         "28.902698516845703",
         "156349200",
         "0.0",
         "0.0"
        ],
        [
         "2015-06-11 00:00:00-04:00",
         "28.96997272313957",
         "29.194233246174658",
         "28.812991041403812",
         "28.83765983581543",
         "141563600",
         "0.0",
         "0.0"
        ],
        [
         "2015-06-12 00:00:00-04:00",
         "28.74796135646813",
         "28.779357697578135",
         "28.505759544214744",
         "28.519214630126953",
         "147544800",
         "0.0",
         "0.0"
        ],
        [
         "2015-06-15 00:00:00-04:00",
         "28.27925453759451",
         "28.534911423420617",
         "28.19179306144201",
         "28.46314811706543",
         "175955600",
         "0.0",
         "0.0"
        ]
       ],
       "shape": {
        "columns": 7,
        "rows": 2517
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Dividends</th>\n",
       "      <th>Stock Splits</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2015-04-06 00:00:00-04:00</th>\n",
       "      <td>27.797598</td>\n",
       "      <td>28.476514</td>\n",
       "      <td>27.766332</td>\n",
       "      <td>28.440781</td>\n",
       "      <td>148776000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-04-07 00:00:00-04:00</th>\n",
       "      <td>28.505541</td>\n",
       "      <td>28.612737</td>\n",
       "      <td>28.134818</td>\n",
       "      <td>28.141518</td>\n",
       "      <td>140049200</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-04-08 00:00:00-04:00</th>\n",
       "      <td>28.105791</td>\n",
       "      <td>28.228622</td>\n",
       "      <td>27.909263</td>\n",
       "      <td>28.049959</td>\n",
       "      <td>149316800</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-04-09 00:00:00-04:00</th>\n",
       "      <td>28.105785</td>\n",
       "      <td>28.268815</td>\n",
       "      <td>27.840026</td>\n",
       "      <td>28.264347</td>\n",
       "      <td>129936000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-04-10 00:00:00-04:00</th>\n",
       "      <td>28.128120</td>\n",
       "      <td>28.409513</td>\n",
       "      <td>27.974025</td>\n",
       "      <td>28.384947</td>\n",
       "      <td>160752000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-03-31 00:00:00-04:00</th>\n",
       "      <td>217.009995</td>\n",
       "      <td>225.619995</td>\n",
       "      <td>216.229996</td>\n",
       "      <td>222.130005</td>\n",
       "      <td>65299300</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-04-01 00:00:00-04:00</th>\n",
       "      <td>219.809998</td>\n",
       "      <td>223.679993</td>\n",
       "      <td>218.899994</td>\n",
       "      <td>223.190002</td>\n",
       "      <td>36412700</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-04-02 00:00:00-04:00</th>\n",
       "      <td>221.320007</td>\n",
       "      <td>225.190002</td>\n",
       "      <td>221.020004</td>\n",
       "      <td>223.889999</td>\n",
       "      <td>35905900</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-04-03 00:00:00-04:00</th>\n",
       "      <td>205.539993</td>\n",
       "      <td>207.490005</td>\n",
       "      <td>201.250000</td>\n",
       "      <td>203.190002</td>\n",
       "      <td>103204700</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-04-04 00:00:00-04:00</th>\n",
       "      <td>193.925003</td>\n",
       "      <td>199.880005</td>\n",
       "      <td>187.345001</td>\n",
       "      <td>189.250000</td>\n",
       "      <td>87169277</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2517 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Open        High         Low       Close  \\\n",
       "Date                                                                        \n",
       "2015-04-06 00:00:00-04:00   27.797598   28.476514   27.766332   28.440781   \n",
       "2015-04-07 00:00:00-04:00   28.505541   28.612737   28.134818   28.141518   \n",
       "2015-04-08 00:00:00-04:00   28.105791   28.228622   27.909263   28.049959   \n",
       "2015-04-09 00:00:00-04:00   28.105785   28.268815   27.840026   28.264347   \n",
       "2015-04-10 00:00:00-04:00   28.128120   28.409513   27.974025   28.384947   \n",
       "...                               ...         ...         ...         ...   \n",
       "2025-03-31 00:00:00-04:00  217.009995  225.619995  216.229996  222.130005   \n",
       "2025-04-01 00:00:00-04:00  219.809998  223.679993  218.899994  223.190002   \n",
       "2025-04-02 00:00:00-04:00  221.320007  225.190002  221.020004  223.889999   \n",
       "2025-04-03 00:00:00-04:00  205.539993  207.490005  201.250000  203.190002   \n",
       "2025-04-04 00:00:00-04:00  193.925003  199.880005  187.345001  189.250000   \n",
       "\n",
       "                              Volume  Dividends  Stock Splits  \n",
       "Date                                                           \n",
       "2015-04-06 00:00:00-04:00  148776000        0.0           0.0  \n",
       "2015-04-07 00:00:00-04:00  140049200        0.0           0.0  \n",
       "2015-04-08 00:00:00-04:00  149316800        0.0           0.0  \n",
       "2015-04-09 00:00:00-04:00  129936000        0.0           0.0  \n",
       "2015-04-10 00:00:00-04:00  160752000        0.0           0.0  \n",
       "...                              ...        ...           ...  \n",
       "2025-03-31 00:00:00-04:00   65299300        0.0           0.0  \n",
       "2025-04-01 00:00:00-04:00   36412700        0.0           0.0  \n",
       "2025-04-02 00:00:00-04:00   35905900        0.0           0.0  \n",
       "2025-04-03 00:00:00-04:00  103204700        0.0           0.0  \n",
       "2025-04-04 00:00:00-04:00   87169277        0.0           0.0  \n",
       "\n",
       "[2517 rows x 7 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yf.Ticker(\"AAPL\").history(period='10y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "0",
         "rawType": "object",
         "type": "string"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "d1abaa44-1288-4cdc-ba7b-de97652f91dc",
       "rows": [
        [
         "0",
         "Free Cash Flow"
        ],
        [
         "1",
         "Repurchase Of Capital Stock"
        ],
        [
         "2",
         "Repayment Of Debt"
        ],
        [
         "3",
         "Issuance Of Debt"
        ],
        [
         "4",
         "Issuance Of Capital Stock"
        ],
        [
         "5",
         "Capital Expenditure"
        ],
        [
         "6",
         "Interest Paid Supplemental Data"
        ],
        [
         "7",
         "Income Tax Paid Supplemental Data"
        ],
        [
         "8",
         "End Cash Position"
        ],
        [
         "9",
         "Beginning Cash Position"
        ],
        [
         "10",
         "Changes In Cash"
        ],
        [
         "11",
         "Financing Cash Flow"
        ],
        [
         "12",
         "Cash Flow From Continuing Financing Activities"
        ],
        [
         "13",
         "Net Other Financing Charges"
        ],
        [
         "14",
         "Cash Dividends Paid"
        ],
        [
         "15",
         "Common Stock Dividend Paid"
        ],
        [
         "16",
         "Net Common Stock Issuance"
        ],
        [
         "17",
         "Common Stock Payments"
        ],
        [
         "18",
         "Common Stock Issuance"
        ],
        [
         "19",
         "Net Issuance Payments Of Debt"
        ],
        [
         "20",
         "Net Short Term Debt Issuance"
        ],
        [
         "21",
         "Net Long Term Debt Issuance"
        ],
        [
         "22",
         "Long Term Debt Payments"
        ],
        [
         "23",
         "Long Term Debt Issuance"
        ],
        [
         "24",
         "Investing Cash Flow"
        ],
        [
         "25",
         "Cash Flow From Continuing Investing Activities"
        ],
        [
         "26",
         "Net Other Investing Changes"
        ],
        [
         "27",
         "Net Investment Purchase And Sale"
        ],
        [
         "28",
         "Sale Of Investment"
        ],
        [
         "29",
         "Purchase Of Investment"
        ],
        [
         "30",
         "Net Business Purchase And Sale"
        ],
        [
         "31",
         "Purchase Of Business"
        ],
        [
         "32",
         "Net PPE Purchase And Sale"
        ],
        [
         "33",
         "Purchase Of PPE"
        ],
        [
         "34",
         "Operating Cash Flow"
        ],
        [
         "35",
         "Cash Flow From Continuing Operating Activities"
        ],
        [
         "36",
         "Change In Working Capital"
        ],
        [
         "37",
         "Change In Other Working Capital"
        ],
        [
         "38",
         "Change In Other Current Liabilities"
        ],
        [
         "39",
         "Change In Other Current Assets"
        ],
        [
         "40",
         "Change In Payables And Accrued Expense"
        ],
        [
         "41",
         "Change In Payable"
        ],
        [
         "42",
         "Change In Account Payable"
        ],
        [
         "43",
         "Change In Inventory"
        ],
        [
         "44",
         "Change In Receivables"
        ],
        [
         "45",
         "Changes In Account Receivables"
        ],
        [
         "46",
         "Other Non Cash Items"
        ],
        [
         "47",
         "Stock Based Compensation"
        ],
        [
         "48",
         "Deferred Tax"
        ],
        [
         "49",
         "Deferred Income Tax"
        ]
       ],
       "shape": {
        "columns": 1,
        "rows": 53
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Free Cash Flow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Repurchase Of Capital Stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Repayment Of Debt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Issuance Of Debt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Issuance Of Capital Stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Capital Expenditure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Interest Paid Supplemental Data</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Income Tax Paid Supplemental Data</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>End Cash Position</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Beginning Cash Position</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Changes In Cash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Financing Cash Flow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Cash Flow From Continuing Financing Activities</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Net Other Financing Charges</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Cash Dividends Paid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Common Stock Dividend Paid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Net Common Stock Issuance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Common Stock Payments</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Common Stock Issuance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Net Issuance Payments Of Debt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Net Short Term Debt Issuance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Net Long Term Debt Issuance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Long Term Debt Payments</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Long Term Debt Issuance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Investing Cash Flow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Cash Flow From Continuing Investing Activities</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Net Other Investing Changes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Net Investment Purchase And Sale</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Sale Of Investment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Purchase Of Investment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Net Business Purchase And Sale</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Purchase Of Business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Net PPE Purchase And Sale</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Purchase Of PPE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Operating Cash Flow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Cash Flow From Continuing Operating Activities</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Change In Working Capital</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Change In Other Working Capital</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Change In Other Current Liabilities</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>Change In Other Current Assets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>Change In Payables And Accrued Expense</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>Change In Payable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>Change In Account Payable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>Change In Inventory</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>Change In Receivables</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>Changes In Account Receivables</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>Other Non Cash Items</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>Stock Based Compensation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>Deferred Tax</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>Deferred Income Tax</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>Depreciation Amortization Depletion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>Depreciation And Amortization</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>Net Income From Continuing Operations</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 0\n",
       "0                                   Free Cash Flow\n",
       "1                      Repurchase Of Capital Stock\n",
       "2                                Repayment Of Debt\n",
       "3                                 Issuance Of Debt\n",
       "4                        Issuance Of Capital Stock\n",
       "5                              Capital Expenditure\n",
       "6                  Interest Paid Supplemental Data\n",
       "7                Income Tax Paid Supplemental Data\n",
       "8                                End Cash Position\n",
       "9                          Beginning Cash Position\n",
       "10                                 Changes In Cash\n",
       "11                             Financing Cash Flow\n",
       "12  Cash Flow From Continuing Financing Activities\n",
       "13                     Net Other Financing Charges\n",
       "14                             Cash Dividends Paid\n",
       "15                      Common Stock Dividend Paid\n",
       "16                       Net Common Stock Issuance\n",
       "17                           Common Stock Payments\n",
       "18                           Common Stock Issuance\n",
       "19                   Net Issuance Payments Of Debt\n",
       "20                    Net Short Term Debt Issuance\n",
       "21                     Net Long Term Debt Issuance\n",
       "22                         Long Term Debt Payments\n",
       "23                         Long Term Debt Issuance\n",
       "24                             Investing Cash Flow\n",
       "25  Cash Flow From Continuing Investing Activities\n",
       "26                     Net Other Investing Changes\n",
       "27                Net Investment Purchase And Sale\n",
       "28                              Sale Of Investment\n",
       "29                          Purchase Of Investment\n",
       "30                  Net Business Purchase And Sale\n",
       "31                            Purchase Of Business\n",
       "32                       Net PPE Purchase And Sale\n",
       "33                                 Purchase Of PPE\n",
       "34                             Operating Cash Flow\n",
       "35  Cash Flow From Continuing Operating Activities\n",
       "36                       Change In Working Capital\n",
       "37                 Change In Other Working Capital\n",
       "38             Change In Other Current Liabilities\n",
       "39                  Change In Other Current Assets\n",
       "40          Change In Payables And Accrued Expense\n",
       "41                               Change In Payable\n",
       "42                       Change In Account Payable\n",
       "43                             Change In Inventory\n",
       "44                           Change In Receivables\n",
       "45                  Changes In Account Receivables\n",
       "46                            Other Non Cash Items\n",
       "47                        Stock Based Compensation\n",
       "48                                    Deferred Tax\n",
       "49                             Deferred Income Tax\n",
       "50             Depreciation Amortization Depletion\n",
       "51                   Depreciation And Amortization\n",
       "52           Net Income From Continuing Operations"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ticker = yf.Ticker('AAPL')\n",
    "earning_dates = ticker.cashflow.columns.tolist()\n",
    "cashflow = ticker.balance_sheet\n",
    "cashflow_columns = cashflow[earning_dates[0]].keys().tolist()\n",
    "pd.DataFrame(cashflow_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MATAS.CO...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "$MATAS.CO: possibly delisted; no price data found  (1d 2024-03-30 -> 2024-04-01)\n",
      "C:\\Users\\Gamer\\AppData\\Local\\Temp\\ipykernel_3208\\3458907841.py:44: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, pd.DataFrame([row_data])], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRIFOR.CO...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "$TRIFOR.CO: possibly delisted; no price data found  (1d 2023-12-30 -> 2024-01-01)\n",
      "C:\\Users\\Gamer\\AppData\\Local\\Temp\\ipykernel_3208\\3458907841.py:44: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, pd.DataFrame([row_data])], ignore_index=True)\n",
      "$TRIFOR.CO: possibly delisted; no price data found  (1d 2020-12-30 -> 2021-01-01) (Yahoo error = \"Data doesn't exist for startDate = 1609282800, endDate = 1609455600\")\n",
      "C:\\Users\\Gamer\\AppData\\Local\\Temp\\ipykernel_3208\\3458907841.py:44: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, pd.DataFrame([row_data])], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QQ.L...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "$QQ.L: possibly delisted; no price data found  (1d 2024-03-30 -> 2024-04-01)\n",
      "C:\\Users\\Gamer\\AppData\\Local\\Temp\\ipykernel_3208\\3458907841.py:44: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, pd.DataFrame([row_data])], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNMBY...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "$RNMBY: possibly delisted; no price data found  (1d 2023-12-30 -> 2024-01-01)\n",
      "C:\\Users\\Gamer\\AppData\\Local\\Temp\\ipykernel_3208\\3458907841.py:44: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, pd.DataFrame([row_data])], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAABF...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "$SAABF: possibly delisted; no price data found  (1d 2023-12-30 -> 2024-01-01)\n",
      "C:\\Users\\Gamer\\AppData\\Local\\Temp\\ipykernel_3208\\3458907841.py:44: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, pd.DataFrame([row_data])], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BCKIY...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "$BCKIY: possibly delisted; no price data found  (1d 2024-03-30 -> 2024-04-01)\n",
      "C:\\Users\\Gamer\\AppData\\Local\\Temp\\ipykernel_3208\\3458907841.py:44: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, pd.DataFrame([row_data])], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BAESY...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "$BAESY: possibly delisted; no price data found  (1d 2023-12-30 -> 2024-01-01)\n",
      "C:\\Users\\Gamer\\AppData\\Local\\Temp\\ipykernel_3208\\3458907841.py:44: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, pd.DataFrame([row_data])], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IVSO.ST...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "$IVSO.ST: possibly delisted; no price data found  (1d 2023-12-30 -> 2024-01-01)\n",
      "C:\\Users\\Gamer\\AppData\\Local\\Temp\\ipykernel_3208\\3458907841.py:44: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, pd.DataFrame([row_data])], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NSKFF...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "$NSKFF: possibly delisted; no price data found  (1d 2023-12-30 -> 2024-01-01)\n",
      "C:\\Users\\Gamer\\AppData\\Local\\Temp\\ipykernel_3208\\3458907841.py:44: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, pd.DataFrame([row_data])], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error for NSKFF on 2020-12-31 00:00:00: Timestamp('2020-12-31 00:00:00')\n",
      "GMAB...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "$GMAB: possibly delisted; no price data found  (1d 2023-12-30 -> 2024-01-01)\n",
      "C:\\Users\\Gamer\\AppData\\Local\\Temp\\ipykernel_3208\\3458907841.py:44: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, pd.DataFrame([row_data])], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GN.CO...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "$GN.CO: possibly delisted; no price data found  (1d 2023-12-30 -> 2024-01-01)\n",
      "C:\\Users\\Gamer\\AppData\\Local\\Temp\\ipykernel_3208\\3458907841.py:44: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, pd.DataFrame([row_data])], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error for GN.CO on 2020-12-31 00:00:00: Timestamp('2020-12-31 00:00:00')\n",
      "NVDA...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "$NVDA: possibly delisted; no price data found  (1d 2021-01-30 -> 2021-02-01)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error for NVDA on 2021-01-31 00:00:00: Timestamp('2021-01-31 00:00:00')\n",
      "LLY...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gamer\\AppData\\Local\\Temp\\ipykernel_3208\\3458907841.py:44: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, pd.DataFrame([row_data])], ignore_index=True)\n",
      "$LLY: possibly delisted; no price data found  (1d 2023-12-30 -> 2024-01-01)\n",
      "C:\\Users\\Gamer\\AppData\\Local\\Temp\\ipykernel_3208\\3458907841.py:44: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, pd.DataFrame([row_data])], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DANSKE.CO...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "$DANSKE.CO: possibly delisted; no price data found  (1d 2023-12-30 -> 2024-01-01)\n",
      "C:\\Users\\Gamer\\AppData\\Local\\Temp\\ipykernel_3208\\3458907841.py:44: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, pd.DataFrame([row_data])], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CARL-B.CO...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "$CARL-B.CO: possibly delisted; no price data found  (1d 2023-12-30 -> 2024-01-01)\n",
      "C:\\Users\\Gamer\\AppData\\Local\\Temp\\ipykernel_3208\\3458907841.py:44: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, pd.DataFrame([row_data])], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAERSK-B.CO...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "$MAERSK-B.CO: possibly delisted; no price data found  (1d 2023-12-30 -> 2024-01-01)\n",
      "C:\\Users\\Gamer\\AppData\\Local\\Temp\\ipykernel_3208\\3458907841.py:44: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, pd.DataFrame([row_data])], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RBREW.CO...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "$RBREW.CO: possibly delisted; no price data found  (1d 2023-12-30 -> 2024-01-01)\n",
      "C:\\Users\\Gamer\\AppData\\Local\\Temp\\ipykernel_3208\\3458907841.py:44: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, pd.DataFrame([row_data])], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ISS.CO...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "$ISS.CO: possibly delisted; no price data found  (1d 2023-12-30 -> 2024-01-01)\n",
      "C:\\Users\\Gamer\\AppData\\Local\\Temp\\ipykernel_3208\\3458907841.py:44: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, pd.DataFrame([row_data])], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error for ISS.CO on 2020-12-31 00:00:00: Timestamp('2020-12-31 00:00:00')\n",
      "DSV.CO...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "$DSV.CO: possibly delisted; no price data found  (1d 2023-12-30 -> 2024-01-01)\n",
      "C:\\Users\\Gamer\\AppData\\Local\\Temp\\ipykernel_3208\\3458907841.py:44: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, pd.DataFrame([row_data])], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCHO.CO...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "$SCHO.CO: possibly delisted; no price data found  (1d 2023-12-30 -> 2024-01-01)\n",
      "C:\\Users\\Gamer\\AppData\\Local\\Temp\\ipykernel_3208\\3458907841.py:44: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, pd.DataFrame([row_data])], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NETC.CO...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "$NETC.CO: possibly delisted; no price data found  (1d 2023-12-30 -> 2024-01-01)\n",
      "C:\\Users\\Gamer\\AppData\\Local\\Temp\\ipykernel_3208\\3458907841.py:44: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, pd.DataFrame([row_data])], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error for NETC.CO on 2020-12-31 00:00:00: Timestamp('2020-12-31 00:00:00')\n",
      "JYSK.CO...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "$JYSK.CO: possibly delisted; no price data found  (1d 2023-12-30 -> 2024-01-01)\n",
      "C:\\Users\\Gamer\\AppData\\Local\\Temp\\ipykernel_3208\\3458907841.py:44: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, pd.DataFrame([row_data])], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ABBN.SW...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "$ABBN.SW: possibly delisted; no price data found  (1d 2023-12-30 -> 2024-01-01)\n",
      "C:\\Users\\Gamer\\AppData\\Local\\Temp\\ipykernel_3208\\3458907841.py:44: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, pd.DataFrame([row_data])], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TER...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "$TER: possibly delisted; no price data found  (1d 2023-12-30 -> 2024-01-01)\n",
      "C:\\Users\\Gamer\\AppData\\Local\\Temp\\ipykernel_3208\\3458907841.py:44: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, pd.DataFrame([row_data])], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PARKEN.CO...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "$PARKEN.CO: possibly delisted; no price data found  (1d 2023-12-30 -> 2024-01-01)\n",
      "C:\\Users\\Gamer\\AppData\\Local\\Temp\\ipykernel_3208\\3458907841.py:44: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, pd.DataFrame([row_data])], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NFLX...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "$NFLX: possibly delisted; no price data found  (1d 2023-12-30 -> 2024-01-01)\n",
      "C:\\Users\\Gamer\\AppData\\Local\\Temp\\ipykernel_3208\\3458907841.py:44: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, pd.DataFrame([row_data])], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error for NFLX on 2020-12-31 00:00:00: Timestamp('2020-12-31 00:00:00')\n",
      "TRMD-A.CO...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "$TRMD-A.CO: possibly delisted; no price data found  (1d 2023-12-30 -> 2024-01-01)\n",
      "C:\\Users\\Gamer\\AppData\\Local\\Temp\\ipykernel_3208\\3458907841.py:44: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, pd.DataFrame([row_data])], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STG.CO...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "$STG.CO: possibly delisted; no price data found  (1d 2023-12-30 -> 2024-01-01)\n",
      "C:\\Users\\Gamer\\AppData\\Local\\Temp\\ipykernel_3208\\3458907841.py:44: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, pd.DataFrame([row_data])], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOVO-B.CO...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "$NOVO-B.CO: possibly delisted; no price data found  (1d 2023-12-30 -> 2024-01-01)\n",
      "C:\\Users\\Gamer\\AppData\\Local\\Temp\\ipykernel_3208\\3458907841.py:44: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, pd.DataFrame([row_data])], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error for NOVO-B.CO on 2020-12-31 00:00:00: Timestamp('2020-12-31 00:00:00')\n",
      "EQNR...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "$EQNR: possibly delisted; no price data found  (1d 2023-12-30 -> 2024-01-01)\n",
      "C:\\Users\\Gamer\\AppData\\Local\\Temp\\ipykernel_3208\\3458907841.py:44: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, pd.DataFrame([row_data])], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NKT.CO...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "$NKT.CO: possibly delisted; no price data found  (1d 2023-12-30 -> 2024-01-01)\n",
      "C:\\Users\\Gamer\\AppData\\Local\\Temp\\ipykernel_3208\\3458907841.py:44: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, pd.DataFrame([row_data])], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error for NKT.CO on 2020-12-31 00:00:00: Timestamp('2020-12-31 00:00:00')\n",
      "NSIS-B.CO...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "$NSIS-B.CO: possibly delisted; no price data found  (1d 2023-12-30 -> 2024-01-01)\n",
      "C:\\Users\\Gamer\\AppData\\Local\\Temp\\ipykernel_3208\\3458907841.py:44: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, pd.DataFrame([row_data])], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KCC.OL...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "$KCC.OL: possibly delisted; no price data found  (1d 2023-12-30 -> 2024-01-01)\n",
      "C:\\Users\\Gamer\\AppData\\Local\\Temp\\ipykernel_3208\\3458907841.py:44: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, pd.DataFrame([row_data])], ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame()\n",
    "for symbol in symbols:\n",
    "    ticker = yf.Ticker(symbol)\n",
    "    earning_dates = ticker.cash_flow.columns.tolist()\n",
    "    cash_flow = ticker.cash_flow\n",
    "    cash_flow_columns = cash_flow[earning_dates[0]].keys().tolist()\n",
    "    balance_sheet = ticker.balance_sheet\n",
    "    balance_sheet_columns = balance_sheet[earning_dates[0]].keys().tolist()\n",
    "    income_statement = ticker.income_statement\n",
    "    income_statement_columns = income_statement[earning_dates[0]].keys().tolist()\n",
    "\n",
    "    for earning_date in earning_dates:\n",
    "        row_data = {'Ticker': symbol, 'Date': earning_date}\n",
    "\n",
    "        try:\n",
    "            for column in cash_flow_columns:\n",
    "                row_data[column] = cash_flow[earning_date][column]\n",
    "\n",
    "            for column in balance_sheet_columns:\n",
    "                row_data[column] = balance_sheet[earning_date][column]\n",
    "\n",
    "            for column in income_statement_columns:\n",
    "                row_data[column] = income_statement[earning_date][column]\n",
    "\n",
    "            price_data = ticker.history(timeframe='10y')\n",
    "        \n",
    "        row_data['Close Price'] = price_data['Close'].iloc[-1]\n",
    "\n",
    "        df = pd.concat([df, pd.DataFrame([row_data])], ignore_index=True)\n",
    "\n",
    "df.to_csv('../data/earnings_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'testFolder' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m tickers \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[43mtestFolder\u001b[49m \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfilteredTickers.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTicker\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      2\u001b[0m trainingTickers \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mchoice(tickers, size\u001b[38;5;241m=\u001b[39mtrainingSize, replace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m      3\u001b[0m trainingRowAmount \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(pd\u001b[38;5;241m.\u001b[39mread_csv(testFolder \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfilteredTickers.csv\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'testFolder' is not defined"
     ]
    }
   ],
   "source": [
    "tickers = pd.read_csv(testFolder / 'filteredTickers.csv')['Ticker']\n",
    "trainingTickers = np.random.choice(tickers, size=trainingSize, replace=False)\n",
    "trainingRowAmount = len(pd.read_csv(testFolder / 'filteredTickers.csv'))\n",
    "\n",
    "if getNewData:\n",
    "    histData = pd.DataFrame()\n",
    "    valid_tickers = []\n",
    "    \n",
    "    for ticker in trainingTickers:\n",
    "        print(f\"Processing {ticker}...\")\n",
    "        try:\n",
    "            data = calculateFutureYearChange(ticker, timeFrame)\n",
    "            if not data.empty:\n",
    "                data['Ticker'] = ticker\n",
    "                data['Industry'] = yf.Ticker(ticker).info.get('industry', 'Unknown')\n",
    "                data['Date'] = pd.to_datetime(data['Date']).dt.tz_localize(None)\n",
    "                \n",
    "                # Enrich individual ticker data first\n",
    "                ticker_data = enrichDataWithMetrics(data)\n",
    "                histData = pd.concat([histData, ticker_data])\n",
    "                \n",
    "                # Check if metrics were added\n",
    "                if 'ROIC' not in ticker_data.columns:\n",
    "                    print(f\"WARNING: Failed to add metrics for {ticker}\")\n",
    "                \n",
    "                valid_tickers.append(ticker)\n",
    "            else:\n",
    "                print(f\"Skipped {ticker} - insufficient data\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {ticker}: {e}\")\n",
    "\n",
    "    print(f\"\\nColumns in final dataset: {histData.columns.tolist()}\")\n",
    "    \n",
    "    if not histData.empty:\n",
    "        histData = enrichDataWithMetrics(histData)\n",
    "        histData.to_csv(dataFolder / trainingData, index=True)\n",
    "        # Verify no future targets leaked to past dates\n",
    "        latest_date = pd.to_datetime(histData['Date']).max()\n",
    "        if 'Future Year Change' in histData.columns:\n",
    "            target_dates = histData[histData['Future Year Change'].notnull()]['Date']\n",
    "            if any(pd.to_datetime(target_dates) > latest_date):\n",
    "                raise ValueError(\"CRITICAL: Analyst targets contain future dates!\")\n",
    "        trainingRowAmount = len(histData)\n",
    "        print(f\"Saved training data with {trainingRowAmount} rows\")\n",
    "    else:\n",
    "        print(\"Warning: No data collected - check your tickers list\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Short visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if getNewData:\n",
    "    histData.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if trainNewModel:\n",
    "    TRAINING_CUTOFF = pd.to_datetime('2023-01-01')\n",
    "\n",
    "    df = pd.read_csv(dataFolder / trainingData)\n",
    "    dfCleaned = df.dropna(subset=['EV/EBIT', 'ROIC']).copy()\n",
    "\n",
    "    # Convert 'Date' to datetime, parse UTC-aware dates, then make naive\n",
    "    dfCleaned['Date'] = pd.to_datetime(dfCleaned['Date'], errors='coerce', utc=True).dt.tz_convert(None)\n",
    "    dfCleaned = dfCleaned.dropna(subset=['Date'])\n",
    "\n",
    "    # Clean 'EV/EBIT' and reset index\n",
    "    dfCleaned['EV/EBIT'] = dfCleaned['EV/EBIT'].replace([np.inf, -np.inf], np.nan)\n",
    "    dfCleaned = dfCleaned.dropna(subset=['EV/EBIT', 'ROIC'])\n",
    "    dfCleaned = dfCleaned.reset_index(drop=True)\n",
    "    dfCleaned.to_csv(dataFolder / trainingData, index=False)\n",
    "\n",
    "    # Check for empty data\n",
    "    if dfCleaned.empty:\n",
    "        raise ValueError(\"The cleaned DataFrame is empty.\")\n",
    "\n",
    "    # Create splits with valid indices\n",
    "    train_mask = dfCleaned['Date'] < TRAINING_CUTOFF\n",
    "    valid_mask = ~train_mask\n",
    "    splits = (list(dfCleaned[train_mask].index), list(dfCleaned[valid_mask].index))\n",
    "\n",
    "    if not splits[0] or not splits[1]:\n",
    "        raise ValueError(\"Empty training or validation split.\")\n",
    "\n",
    "    # Proceed with TabularPandas\n",
    "    to = TabularPandas(\n",
    "        dfCleaned, \n",
    "        procs=[Categorify, FillMissing, Normalize],\n",
    "        y_names=yNames,\n",
    "        cat_names=catNames, \n",
    "        cont_names=contNames,\n",
    "        splits=splits\n",
    "    )\n",
    "\n",
    "    dls = to.dataloaders(bs=batchSize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if trainNewModel:\n",
    "    learn = tabular_learner(dls, metrics=[rmse, mae])\n",
    "\n",
    "    # Learning rate finder\n",
    "    lr_find_results = learn.lr_find(suggest_funcs=(minimum, steep))\n",
    "\n",
    "    # Debugging information\n",
    "    print(f\"Learning rate finder results: {lr_find_results}\")\n",
    "\n",
    "    # Extract learning rates\n",
    "    lr_min, lr_steep = lr_find_results\n",
    "\n",
    "    # Check if learning rates are valid\n",
    "    if lr_min is None or lr_steep is None or lr_min == 0 or lr_steep == 0:\n",
    "        raise ValueError(\"Learning rate finder did not return valid learning rates.\")\n",
    "\n",
    "    # Train\n",
    "    print(f\"Training for {epochs} epochs...\")\n",
    "    learn.fit_one_cycle(epochs, lr_max=lr_steep)\n",
    "    print(\"Model training complete\")\n",
    "\n",
    "    learn.export(modelFolder / f'{modelName}.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing and benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logEvaluation(model_name, mae, rmse, r2, model_folder, test_tickers):\n",
    "    \"\"\"Log evaluation metrics to CSV file\"\"\"\n",
    "    log_file = model_folder / \"modelEvaluations.csv\"\n",
    "    \n",
    "    new_entry_df = pd.DataFrame([{\n",
    "        \"Model Name\": model_name,\n",
    "        \"Timestamp\": datetime.now().strftime('%Y-%m-%d %H:%M'),\n",
    "        \"MAE\": f'{mae:.3f}',\n",
    "        \"RMSE\": f'{rmse:.3f}',\n",
    "        \"R2\": f'{r2:.3f}',\n",
    "        \"Epochs\": epochs,\n",
    "        \"Training Size\": trainingSize,\n",
    "        \"Training Rows\": trainingRowAmount,\n",
    "        \"Test Size\": len(test_tickers),\n",
    "        \"Cat Names\": catNames,\n",
    "        \"Cont Names\": contNames,\n",
    "    }])\n",
    "    \n",
    "    try:\n",
    "        log_df = pd.read_csv(log_file)\n",
    "        log_df = pd.concat([log_df, new_entry_df], ignore_index=True)\n",
    "    except FileNotFoundError:\n",
    "        log_df = new_entry_df\n",
    "        \n",
    "    log_df.to_csv(log_file, index=False)\n",
    "    print(f\"Logged evaluation results to {log_file}\")\n",
    "\n",
    "def plotResults(results_df, model_name, model_folder):\n",
    "    \"\"\"Create and save visualization plots using all data points.\"\"\"\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.scatter(results_df['Actual'], results_df['Predicted'], alpha=0.7, label='Predictions')\n",
    "    min_val = min(results_df['Actual'].min(), results_df['Predicted'].min())\n",
    "    max_val = max(results_df['Actual'].max(), results_df['Predicted'].max())\n",
    "    plt.plot([min_val, max_val], [min_val, max_val], color='red', linestyle='--', label='Perfect Prediction')\n",
    "    plt.title(f'Predicted vs. Actual Returns - {model_name}', fontsize=14)\n",
    "    plt.xlabel('Actual Returns')\n",
    "    plt.ylabel('Predicted Returns')\n",
    "    plt.legend()\n",
    "    plt.grid(alpha=0.5)\n",
    "    \n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.scatter(results_df['Predicted'], results_df['Residual'], alpha=0.7)\n",
    "    plt.axhline(y=0, color='r', linestyle='--')\n",
    "    plt.title('Residual Plot')\n",
    "    plt.xlabel('Predicted Returns')\n",
    "    plt.ylabel('Residual')\n",
    "    plt.grid(alpha=0.5)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "if trainNewModel:\n",
    "    nonTrainingTickers = list(set(tickers) - set(trainingTickers))\n",
    "    validTestData = []\n",
    "    attempted_tickers = set()\n",
    "    attempts = 0\n",
    "\n",
    "    if testSize * 4 <= len(tickers):  \n",
    "        max_attempts = testSize * 4 # Prevent infinite loops\n",
    "    else:\n",
    "        max_attempts = len(tickers)\n",
    "\n",
    "    # Keep trying until we reach testSize or exhaust attempts\n",
    "    while len(validTestData) < testSize and attempts < max_attempts:\n",
    "        # Get a new ticker we haven't tried yet\n",
    "        remaining_tickers = [t for t in nonTrainingTickers if t not in attempted_tickers]\n",
    "        if not remaining_tickers:  # If all tried, reset attempted list\n",
    "            attempted_tickers = set()\n",
    "            remaining_tickers = nonTrainingTickers\n",
    "            \n",
    "        ticker = np.random.choice(remaining_tickers)\n",
    "        attempted_tickers.add(ticker)\n",
    "        attempts += 1\n",
    "\n",
    "        # Fetch and validate data\n",
    "        data = getTickerDataFrom1YrAgo(ticker)\n",
    "        if not data.empty and not data[['EV/EBIT', 'ROIC']].isna().any().any():\n",
    "            validTestData.append(data)\n",
    "\n",
    "    if not validTestData:\n",
    "        raise ValueError(\"No valid test data collected after multiple attempts\")\n",
    "        \n",
    "    # Trim to exact testSize if we collected more\n",
    "    validTestData = validTestData[:testSize]  \n",
    "    combinedTestData = pd.concat(validTestData, ignore_index=True)\n",
    "\n",
    "    # Clean data\n",
    "    test_data_clean = combinedTestData.dropna(subset=['EV/EBIT', 'ROIC', 'Future Year Change'])\n",
    "    \n",
    "    if test_data_clean.empty:\n",
    "        raise ValueError(\"No valid test data after cleaning NaN values\")\n",
    "\n",
    "    # Create test dataloader\n",
    "    test_dl = learn.dls.test_dl(test_data_clean)\n",
    "    preds, targs = learn.get_preds(dl=test_dl)\n",
    "    \n",
    "    # Create results DataFrame\n",
    "    results_df = pd.DataFrame({\n",
    "        'Predicted': preds.numpy().flatten(),\n",
    "        'Actual': targs.numpy().flatten()\n",
    "    })\n",
    "    results_df['Residual'] = results_df['Actual'] - results_df['Predicted']\n",
    "\n",
    "    # Calculate metrics\n",
    "    mae = np.mean(np.abs(results_df['Residual']))\n",
    "    rmse = np.sqrt(np.mean(results_df['Residual']**2))\n",
    "    r2 = 1 - (np.sum(results_df['Residual']**2) / np.sum((results_df['Actual'] - results_df['Actual'].mean())**2))\n",
    "\n",
    "    # Log and plot\n",
    "    logEvaluation(modelName, mae, rmse, r2, modelFolder, test_data_clean['Ticker'].unique())\n",
    "    plotResults(results_df, modelName, modelFolder)\n",
    "\n",
    "    # Show collection stats\n",
    "    print(f\"Collected {len(validTestData)} valid test tickers (target: {testSize})\")\n",
    "    if attempts >= max_attempts:\n",
    "        print(f\"Warning: Reached max attempts ({max_attempts}). Some invalid tickers may remain.\")  \n",
    "    print(f\"\\nEvaluation Results:\")\n",
    "    print(f\"MAE: {mae:.3f}\")\n",
    "    print(f\"RMSE: {rmse:.3f}\")\n",
    "    print(f\"RÂ²: {r2:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model files in modelFolder:\n",
      "stockScreenerV1.0.pkl\n",
      "stockScreenerV1.1.pkl\n",
      "stockScreenerV1.10.pkl\n",
      "stockScreenerV1.2.pkl\n",
      "stockScreenerV1.3.pkl\n",
      "stockScreenerV1.4.pkl\n",
      "stockScreenerV1.5.pkl\n",
      "stockScreenerV1.6.pkl\n",
      "stockScreenerV1.7.pkl\n",
      "stockScreenerV1.8.pkl\n",
      "stockScreenerV1.9.pkl\n",
      "stockScreenerV2.0.pkl\n",
      "stockScreenerV2.1.pkl\n",
      "stockScreenerV2.2.pkl\n"
     ]
    }
   ],
   "source": [
    "print('Model files in modelFolder:')\n",
    "for file in modelFolder.glob('*.pkl'):\n",
    "    print(file.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model Name    stockScreenerV1.7\n",
       "Timestamp      2025-01-27 08:45\n",
       "MAE                       0.328\n",
       "RMSE                      0.739\n",
       "R2                        0.077\n",
       "Name: 7, dtype: object"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluations = pd.read_csv(modelFolder / 'modelEvaluations.csv')\n",
    "bestModel = evaluations.sort_values('MAE', ascending=True).iloc[0]\n",
    "bestModel.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.name == 'nt':\n",
    "    temp = pathlib.PosixPath\n",
    "    pathlib.PosixPath = pathlib.WindowsPath\n",
    "else:\n",
    "    pathlib.WindowsPath = pathlib.PosixPath\n",
    "\n",
    "importedModel = Path(f\"{bestModel['Model Name']}.pkl\") # Change this if you want to try other models\n",
    "learn = load_learner(modelFolder / importedModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predictionTarget = '95%'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'getTickerData' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[64], line 12\u001b[0m\n\u001b[0;32m      9\u001b[0m     predictionTickers \u001b[38;5;241m=\u001b[39m [predictionTarget]\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Fetch data for prediction tickers\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m dfPrediction \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([\u001b[43mgetTickerData\u001b[49m(ticker) \u001b[38;5;28;01mfor\u001b[39;00m ticker \u001b[38;5;129;01min\u001b[39;00m predictionTickers], ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Ensure dfPrediction is a DataFrame\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dfPrediction, \u001b[38;5;28mdict\u001b[39m):\n",
      "\u001b[1;31mNameError\u001b[0m: name 'getTickerData' is not defined"
     ]
    }
   ],
   "source": [
    "if predictionTarget != 'None':\n",
    "    if predictionTarget == 'ALL':\n",
    "        predictionTickers = tickers\n",
    "    elif predictionTarget.endswith('%'):\n",
    "        percentage = float(predictionTarget.strip('%')) / 100\n",
    "        num_tickers = int(len(tickers) * percentage)\n",
    "        predictionTickers = np.random.choice(tickers, size=num_tickers, replace=False).tolist()\n",
    "    else:\n",
    "        predictionTickers = [predictionTarget]\n",
    "\n",
    "    # Fetch data for prediction tickers\n",
    "    dfPrediction = pd.concat([getTickerData(ticker) for ticker in predictionTickers], ignore_index=True)\n",
    "\n",
    "    # Ensure dfPrediction is a DataFrame\n",
    "    if isinstance(dfPrediction, dict):\n",
    "        dfPrediction = pd.DataFrame([dfPrediction])\n",
    "\n",
    "    # Drop rows with NaN values in 'EV/EBIT' or 'ROIC'\n",
    "    dfPrediction = dfPrediction.dropna(subset=['EV/EBIT', 'ROIC'])\n",
    "\n",
    "    # Create test dataloader\n",
    "    dl = learn.dls.test_dl(dfPrediction)\n",
    "    dfPrediction.head()\n",
    "\n",
    "    # Make predictions\n",
    "    prediction = learn.get_preds(dl=dl)\n",
    "    adr_df = pd.read_csv(testFolder / 'filteredTickers.csv')\n",
    "    company_dict = dict(zip(adr_df['Ticker'], adr_df['Company']))\n",
    "\n",
    "    if predictionTarget == 'ALL' or predictionTarget.endswith('%'):\n",
    "        sorted_predictions = sorted(zip(predictionTickers, prediction[0]), key=lambda x: x[1], reverse=True)\n",
    "        print(f\"Got predictions for {len(sorted_predictions)} tickers, expected: {len(predictionTickers)}\")\n",
    "        print(f\"Prediction for best performing tickers:\")\n",
    "        for symbol, pred in sorted_predictions:\n",
    "            company_name = company_dict.get(symbol, 'Unknown')\n",
    "            print(f\"{symbol} ({company_name}): {pred[0].item() * 100:.2f}%\")\n",
    "    else:\n",
    "        company_name = company_dict.get(predictionTarget, 'Unknown')\n",
    "        print(f\"Prediction for {predictionTarget} ({company_name}):\")\n",
    "        print(f\"{prediction[0][0][0].item() * 100:.2f}%\")\n",
    "    print(\"Free money?!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
