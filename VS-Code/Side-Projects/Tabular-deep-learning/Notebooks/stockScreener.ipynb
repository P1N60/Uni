{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install libraries the first time\n",
    "# ! pip install -q ipynb yfinance pandas pathlib numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.tabular.all import *\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "from datetime import *\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelName = 'stockScreenerV2.0'\n",
    "trainingData = 'stockData.csv'\n",
    "getNewData = True \n",
    "trainNewModel = True\n",
    "predictionTarget = None # 'ALL' for all tickers decending, 'None' for no prediction\n",
    "\n",
    "# Training parameters\n",
    "trainingSize = 10 # Number of stocks to get from the test tickers\n",
    "timeFrame = '2y'  # Options: 1d, 5d, 1mo, 3mo, 6mo, 1y, 2y, 5y, 10y, ytd, max\n",
    "yNames = ['Future Year Change']\n",
    "catNames = ['Industry']\n",
    "contNames = [\n",
    "    'Open',\n",
    "    'Close', \n",
    "    'Volume', \n",
    "    'Dividends', \n",
    "    'Stock Splits', \n",
    "    'EV/EBIT', \n",
    "    'ROIC'\n",
    "]\n",
    "epochs = 3\n",
    "\n",
    "# Test parameters\n",
    "testSize = 200 # Number of stocks to test. 'ALL' for all non-training stocks\n",
    "\n",
    "# Folder- and file paths\n",
    "dataFolder = Path.cwd().parent / 'TrainingData'\n",
    "dataName = 'stockData.csv'\n",
    "testFolder = Path.cwd().parent / 'TestData'\n",
    "trainingFolder = Path.cwd().parent / 'TrainingData'\n",
    "modelFolder = Path.cwd().parent.parent / 'TrainedModels' / 'stockScreener'\n",
    "testFolder = Path.cwd().parent / 'TestData'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting historic stock data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ticker cleaning (remove duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data from the CSV file\n",
    "df = pd.read_csv(testFolder / 'tickers.csv')\n",
    "    \n",
    "# Remove duplicate rows\n",
    "df = df.drop_duplicates().reset_index(drop=True)\n",
    "    \n",
    "# Save the cleaned data to a new file\n",
    "df.to_csv(testFolder / 'tickers.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for Data Collection and Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_industry(ticker_symbol):\n",
    "    try:\n",
    "        return yf.Ticker(ticker_symbol).info.get('industry', 'Unknown')\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching industry for {ticker_symbol}: {e}\")\n",
    "        return 'Error'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_future_year_change(ticker_symbol, timeframe, buffer=1):\n",
    "    valid_periods = ['1d', '5d', '1mo', '3mo', '6mo', '1y', '2y', '5y', '10y', 'ytd', 'max']\n",
    "    try:\n",
    "        if timeframe == 'max':\n",
    "            future_change = yf.Ticker(ticker_symbol).history(period='max')\n",
    "            future_change['Future Year Change'] = (future_change['Close'].shift(-252) / future_change['Close'] - 1)\n",
    "            future_change = future_change.dropna(subset=['Future Year Change'])\n",
    "        else:\n",
    "            extended_timeframe = valid_periods[valid_periods.index(timeframe) + buffer]\n",
    "            future_change = yf.Ticker(ticker_symbol).history(period=extended_timeframe)\n",
    "            future_change['Future Year Change'] = (future_change['Close'].shift(-252) / future_change['Close'] - 1)\n",
    "\n",
    "            end_date = future_change.index[-1] - pd.DateOffset(years=1)\n",
    "            start_date = end_date - pd.DateOffset(years=int(timeframe[:-1]))\n",
    "            future_change = future_change.loc[start_date:end_date].dropna(subset=['Future Year Change'])\n",
    "        return future_change\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {ticker_symbol}: {e}\")\n",
    "        return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_static_ev_data(ticker_symbol):\n",
    "    try:\n",
    "        info = yf.Ticker(ticker_symbol).info\n",
    "        return info.get('totalDebt', 0), info.get('totalCash', 0), info.get('sharesOutstanding', None)\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching EV components for {ticker_symbol}: {e}\")\n",
    "        return None, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_approx_ebit(ticker_symbol):\n",
    "    try:\n",
    "        info = yf.Ticker(ticker_symbol).info\n",
    "        revenue = info.get('totalRevenue', None)\n",
    "        operating_income = info.get('operatingIncome', None)\n",
    "        return operating_income if operating_income else (revenue * 0.15 if revenue else None)\n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating EBIT for {ticker_symbol}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_daily_ev_ebit(hist_data):\n",
    "    hist_data['EV/EBIT'] = None\n",
    "    for symbol in hist_data['Ticker'].unique():\n",
    "        ticker_data = hist_data[hist_data['Ticker'] == symbol].copy()\n",
    "        total_debt, cash, shares_outstanding = get_static_ev_data(symbol)\n",
    "        ebit = calculate_approx_ebit(symbol)\n",
    "\n",
    "        if shares_outstanding and ebit and ebit != 0:\n",
    "            ticker_data['EV'] = (ticker_data['Close'] * shares_outstanding) + total_debt - cash\n",
    "            ticker_data['EV/EBIT'] = ticker_data['EV'] / ebit\n",
    "            hist_data.loc[ticker_data.index, 'EV/EBIT'] = ticker_data['EV/EBIT']\n",
    "    return hist_data.drop(columns=['EV'], errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_daily_roic(hist_data):\n",
    "    for symbol in hist_data['Ticker'].unique():\n",
    "        ticker_data = hist_data[hist_data['Ticker'] == symbol].copy()\n",
    "        total_debt, cash, shares_outstanding = get_static_ev_data(symbol)\n",
    "        ebit = calculate_approx_ebit(symbol)\n",
    "\n",
    "        if shares_outstanding and ebit:\n",
    "            tax_rate = 0.21\n",
    "            nopat = ebit * (1 - tax_rate)\n",
    "            invested_capital = total_debt + (ticker_data['Close'] * shares_outstanding) - cash\n",
    "            ticker_data['ROIC'] = np.where(invested_capital != 0, nopat / invested_capital, None)\n",
    "            hist_data.loc[ticker_data.index, 'ROIC'] = ticker_data['ROIC']\n",
    "    return hist_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTickerData(ticker_symbol):\n",
    "    try:\n",
    "        ticker = yf.Ticker(ticker_symbol)\n",
    "        hist = ticker.history(period='1d')\n",
    "        total_debt, cash, shares_outstanding = get_static_ev_data(ticker_symbol)\n",
    "        ebit = calculate_approx_ebit(ticker_symbol)\n",
    "        ev = (hist['Close'].iloc[-1] * shares_outstanding) + total_debt - cash\n",
    "        ev_ebit = ev / ebit if ebit else None\n",
    "        market_cap = hist['Close'].iloc[-1] * shares_outstanding\n",
    "        tax_rate = 0.21\n",
    "        nopat = ebit * (1 - tax_rate) if ebit else None\n",
    "        invested_capital = total_debt + market_cap - cash\n",
    "        roic = nopat / invested_capital if nopat and invested_capital else None\n",
    "        industry = get_industry(ticker_symbol)\n",
    "\n",
    "        return pd.DataFrame([{\n",
    "            'Open': hist['Open'].iloc[-1],\n",
    "            'High': hist['High'].iloc[-1],\n",
    "            'Low': hist['Low'].iloc[-1],\n",
    "            'Close': hist['Close'].iloc[-1],\n",
    "            'Volume': hist['Volume'].iloc[-1],\n",
    "            'Dividends': hist.get('Dividends', pd.Series([0.0])).iloc[-1],\n",
    "            'Stock Splits': hist.get('Stock Splits', pd.Series([0.0])).iloc[-1],\n",
    "            'EV/EBIT': ev_ebit,\n",
    "            'Market Cap': market_cap,\n",
    "            'ROIC': roic,\n",
    "            'Industry': industry\n",
    "        }])\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching data for {ticker_symbol}: {e}\")\n",
    "        return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTickerDataFrom1YrAgo(ticker_symbol):\n",
    "    try:\n",
    "        # Fetch ticker data\n",
    "        ticker = yf.Ticker(ticker_symbol)\n",
    "\n",
    "        # Define the date range: one year ago to today\n",
    "        today = datetime.today()\n",
    "        one_year_ago = today - timedelta(days=365)\n",
    "\n",
    "        # Fetch historical data for one year ago\n",
    "        hist = ticker.history(start=(one_year_ago - timedelta(days=30)).strftime('%Y-%m-%d'), \n",
    "                              end=(one_year_ago + timedelta(days=0)).strftime('%Y-%m-%d'))\n",
    "        if hist.empty:\n",
    "            raise ValueError(f\"No historical data available for {ticker_symbol} around {one_year_ago.strftime('%Y-%m-%d')}.\")\n",
    "\n",
    "        # Extract the closest data point to one year ago\n",
    "        row = hist.iloc[0]  # Get the first available entry within the date range\n",
    "\n",
    "        # Price today\n",
    "        price_today = ticker.history(period='1d')['Close'].iloc[-1]\n",
    "\n",
    "        # Calculate future price change (from one year ago to today)\n",
    "        price_change_future = ((price_today - row['Close']) / row['Close']) if row['Close'] else None\n",
    "\n",
    "        # Collect additional data\n",
    "        total_debt, cash, shares_outstanding = get_static_ev_data(ticker_symbol)\n",
    "        ebit = calculate_approx_ebit(ticker_symbol)\n",
    "        ev = (row['Close'] * shares_outstanding) + total_debt - cash if shares_outstanding else None\n",
    "        ev_ebit = ev / ebit if ebit else None\n",
    "        market_cap = row['Close'] * shares_outstanding if shares_outstanding else None\n",
    "        tax_rate = 0.21\n",
    "        nopat = ebit * (1 - tax_rate) if ebit else None\n",
    "        invested_capital = total_debt + market_cap - cash if market_cap and total_debt and cash else None\n",
    "        roic = nopat / invested_capital if nopat and invested_capital else None\n",
    "        industry = get_industry(ticker_symbol)\n",
    "\n",
    "        # Return as a DataFrame\n",
    "        return pd.DataFrame([{\n",
    "            'Date': row.name,\n",
    "            'Open': row['Open'],\n",
    "            'High': row['High'],\n",
    "            'Low': row['Low'],\n",
    "            'Close': row['Close'],\n",
    "            'Volume': row['Volume'],\n",
    "            'Dividends': row.get('Dividends', 0.0),\n",
    "            'Stock Splits': row.get('Stock Splits', 0.0),\n",
    "            'Future Year Change': price_change_future,\n",
    "            'Industry': industry,\n",
    "            'EV/EBIT': ev_ebit,\n",
    "            'ROIC': roic\n",
    "        }])\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching data for {ticker_symbol}: {e}\")\n",
    "        return pd.DataFrame()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "$FCH-C: possibly delisted; no price data found  (period=5y) (Yahoo error = \"No data found, symbol may be delisted\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing FCH-C: index -1 is out of bounds for axis 0 with size 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "404 Client Error: Not Found for url: https://query2.finance.yahoo.com/v10/finance/quoteSummary/FCH-C?modules=financialData%2CquoteType%2CdefaultKeyStatistics%2CassetProfile%2CsummaryDetail&corsDomain=finance.yahoo.com&formatted=false&symbol=FCH-C&crumb=vSmwn1lwQ8s\n",
      "C:\\Users\\Gamer\\AppData\\Local\\Temp\\ipykernel_14704\\3605072956.py:10: FutureWarning: The behavior of array concatenation with empty entries is deprecated. In a future version, this will no longer exclude empty items when determining the result dtype. To retain the old behavior, exclude the empty entries before the concat operation.\n",
      "  hist_data = pd.concat([hist_data, future_change])\n",
      "$SWJ: possibly delisted; no price data found  (period=5y) (Yahoo error = \"No data found, symbol may be delisted\")\n",
      "C:\\Users\\Gamer\\AppData\\Local\\Temp\\ipykernel_14704\\3605072956.py:10: FutureWarning: The behavior of array concatenation with empty entries is deprecated. In a future version, this will no longer exclude empty items when determining the result dtype. To retain the old behavior, exclude the empty entries before the concat operation.\n",
      "  hist_data = pd.concat([hist_data, future_change])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing SWJ: index -1 is out of bounds for axis 0 with size 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "$ARI-A: possibly delisted; no price data found  (period=5y) (Yahoo error = \"No data found, symbol may be delisted\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing ARI-A: index -1 is out of bounds for axis 0 with size 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "404 Client Error: Not Found for url: https://query2.finance.yahoo.com/v10/finance/quoteSummary/ARI-A?modules=financialData%2CquoteType%2CdefaultKeyStatistics%2CassetProfile%2CsummaryDetail&corsDomain=finance.yahoo.com&formatted=false&symbol=ARI-A&crumb=vSmwn1lwQ8s\n",
      "C:\\Users\\Gamer\\AppData\\Local\\Temp\\ipykernel_14704\\3605072956.py:10: FutureWarning: The behavior of array concatenation with empty entries is deprecated. In a future version, this will no longer exclude empty items when determining the result dtype. To retain the old behavior, exclude the empty entries before the concat operation.\n",
      "  hist_data = pd.concat([hist_data, future_change])\n",
      "$SQNM: possibly delisted; no price data found  (period=5y)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing SQNM: index -1 is out of bounds for axis 0 with size 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gamer\\AppData\\Local\\Temp\\ipykernel_14704\\3605072956.py:10: FutureWarning: The behavior of array concatenation with empty entries is deprecated. In a future version, this will no longer exclude empty items when determining the result dtype. To retain the old behavior, exclude the empty entries before the concat operation.\n",
      "  hist_data = pd.concat([hist_data, future_change])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000\n"
     ]
    }
   ],
   "source": [
    "if getNewData == True:\n",
    "    hist_data = pd.DataFrame()\n",
    "    symbols = pd.read_csv(testFolder / 'tickers.csv')['Ticker']\n",
    "    trainingTickers = np.random.choice(symbols, size=trainingSize, replace=False).tolist()\n",
    "    for trainingTicker in trainingTickers:\n",
    "        try:\n",
    "            future_change = calculate_future_year_change(trainingTicker, timeFrame)\n",
    "            future_change['Ticker'] = trainingTicker\n",
    "            future_change['Industry'] = get_industry(trainingTicker)\n",
    "            hist_data = pd.concat([hist_data, future_change])\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {trainingTicker}: {e}\")\n",
    "\n",
    "    hist_data.reset_index(inplace=True)\n",
    "    hist_data = calculate_daily_ev_ebit(hist_data)\n",
    "    hist_data = calculate_daily_roic(hist_data)\n",
    "    hist_data.to_csv(dataFolder / trainingData, index=False)\n",
    "\n",
    "trainingRows = pd.read_csv(dataFolder / trainingData)\n",
    "print(len(trainingRows))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can have a look at how the data is structured:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Ticker</th>\n",
       "      <th>Industry</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Dividends</th>\n",
       "      <th>Stock Splits</th>\n",
       "      <th>Capital Gains</th>\n",
       "      <th>Future Year Change</th>\n",
       "      <th>EV/EBIT</th>\n",
       "      <th>ROIC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-01-21 00:00:00-05:00</td>\n",
       "      <td>EUFN</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>17.846360</td>\n",
       "      <td>17.880830</td>\n",
       "      <td>17.682634</td>\n",
       "      <td>17.760189</td>\n",
       "      <td>1681400.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.011868</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-01-24 00:00:00-05:00</td>\n",
       "      <td>EUFN</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>17.260387</td>\n",
       "      <td>17.458584</td>\n",
       "      <td>16.950165</td>\n",
       "      <td>17.441349</td>\n",
       "      <td>2376200.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.013927</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-01-25 00:00:00-05:00</td>\n",
       "      <td>EUFN</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>17.363796</td>\n",
       "      <td>17.682635</td>\n",
       "      <td>17.225920</td>\n",
       "      <td>17.570610</td>\n",
       "      <td>1410000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.017213</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-01-26 00:00:00-05:00</td>\n",
       "      <td>EUFN</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>17.889447</td>\n",
       "      <td>17.958385</td>\n",
       "      <td>17.570607</td>\n",
       "      <td>17.674015</td>\n",
       "      <td>3323000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.007193</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-01-27 00:00:00-05:00</td>\n",
       "      <td>EUFN</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>17.915301</td>\n",
       "      <td>17.992855</td>\n",
       "      <td>17.579226</td>\n",
       "      <td>17.691252</td>\n",
       "      <td>1972400.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.000395</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       index Ticker Industry       Open       High        Low  \\\n",
       "0  2022-01-21 00:00:00-05:00   EUFN  Unknown  17.846360  17.880830  17.682634   \n",
       "1  2022-01-24 00:00:00-05:00   EUFN  Unknown  17.260387  17.458584  16.950165   \n",
       "2  2022-01-25 00:00:00-05:00   EUFN  Unknown  17.363796  17.682635  17.225920   \n",
       "3  2022-01-26 00:00:00-05:00   EUFN  Unknown  17.889447  17.958385  17.570607   \n",
       "4  2022-01-27 00:00:00-05:00   EUFN  Unknown  17.915301  17.992855  17.579226   \n",
       "\n",
       "       Close     Volume  Dividends  Stock Splits  Capital Gains  \\\n",
       "0  17.760189  1681400.0        0.0           0.0            0.0   \n",
       "1  17.441349  2376200.0        0.0           0.0            0.0   \n",
       "2  17.570610  1410000.0        0.0           0.0            0.0   \n",
       "3  17.674015  3323000.0        0.0           0.0            0.0   \n",
       "4  17.691252  1972400.0        0.0           0.0            0.0   \n",
       "\n",
       "   Future Year Change  EV/EBIT  ROIC  \n",
       "0           -0.011868      NaN   NaN  \n",
       "1            0.013927      NaN   NaN  \n",
       "2            0.017213      NaN   NaN  \n",
       "3            0.007193      NaN   NaN  \n",
       "4           -0.000395      NaN   NaN  "
      ]
     },
     "execution_count": 490,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataPath = Path()\n",
    "df = pd.read_csv(trainingFolder/trainingData)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the columns are continuous (like age) and we will treat them as float numbers we can feed our model directly. Others are categorical (like workclass or education) and we will convert them to a unique index that we will feed to embedding layers. We can specify our categorical and continuous column names, as well as the name of the dependent variable in TabularDataLoaders factory methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Gamer\\miniconda3\\Lib\\site-packages\\fastai\\tabular\\core.py:314: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  to[n].fillna(self.na_dict[n], inplace=True)\n",
      "c:\\Users\\Gamer\\miniconda3\\Lib\\site-packages\\fastai\\tabular\\core.py:314: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  to[n].fillna(self.na_dict[n], inplace=True)\n"
     ]
    }
   ],
   "source": [
    "dls = TabularDataLoaders.from_csv(trainingFolder / trainingData, path=dataPath, \n",
    "    y_names=yNames,\n",
    "    cat_names=catNames,\n",
    "    cont_names=contNames,\n",
    "    procs = [Categorify, FillMissing, Normalize])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last part is the list of pre-processors we apply to our data:\n",
    "\n",
    "* Categorify is going to take every categorical variable and make a map from integer to unique categories, then replace the values by the corresponding index.\n",
    "* FillMissing will fill the missing values in the continuous variables by the median of existing values (you can choose a specific value if you prefer)\n",
    "* Normalize will normalize the continuous variables (subtract the mean and divide by the std)\n",
    "\n",
    "To further expose what’s going on below the surface, let’s rewrite this utilizing fastai’s TabularPandas class. We will need to make one adjustment, which is defining how we want to split our data. By default the factory method above used a random 80/20 split, so we will do the same:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = EndSplitter (valid_pct=0.2, valid_last=True)(range_of(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Gamer\\miniconda3\\Lib\\site-packages\\fastai\\tabular\\core.py:314: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  to[n].fillna(self.na_dict[n], inplace=True)\n",
      "c:\\Users\\Gamer\\miniconda3\\Lib\\site-packages\\fastai\\tabular\\core.py:314: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  to[n].fillna(self.na_dict[n], inplace=True)\n"
     ]
    }
   ],
   "source": [
    "to = TabularPandas(df, procs=[Categorify, FillMissing, Normalize],\n",
    "    y_names=yNames,\n",
    "    cat_names = catNames,\n",
    "    cont_names = contNames,\n",
    "    splits=splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we build our TabularPandas object, our data is completely preprocessed as seen below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>EV/EBIT_na</th>\n",
       "      <th>ROIC_na</th>\n",
       "      <th>Open</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Dividends</th>\n",
       "      <th>Stock Splits</th>\n",
       "      <th>EV/EBIT</th>\n",
       "      <th>ROIC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.508579</td>\n",
       "      <td>-0.51</td>\n",
       "      <td>1.204792</td>\n",
       "      <td>-0.035072</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.499398</td>\n",
       "      <td>0.368375</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  EV/EBIT_na  ROIC_na      Open  Close    Volume  Dividends  \\\n",
       "0      1           2        2 -0.508579  -0.51  1.204792  -0.035072   \n",
       "\n",
       "   Stock Splits   EV/EBIT      ROIC  \n",
       "0           0.0 -0.499398  0.368375  "
      ]
     },
     "execution_count": 499,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to.xs.iloc[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can build our DataLoaders again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = to.dataloaders(bs=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The show_batch method works like for every other application:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>EV/EBIT_na</th>\n",
       "      <th>ROIC_na</th>\n",
       "      <th>Open</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Dividends</th>\n",
       "      <th>Stock Splits</th>\n",
       "      <th>EV/EBIT</th>\n",
       "      <th>ROIC</th>\n",
       "      <th>Future Year Change</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-04-13 00:00:00-04:00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>4.450000</td>\n",
       "      <td>4.450002</td>\n",
       "      <td>2199.981391</td>\n",
       "      <td>1.762562e-11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.768000</td>\n",
       "      <td>0.165688</td>\n",
       "      <td>-0.377528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-02-03 00:00:00-05:00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>145.039994</td>\n",
       "      <td>142.510002</td>\n",
       "      <td>469400.001410</td>\n",
       "      <td>1.762562e-11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>58.325641</td>\n",
       "      <td>0.013545</td>\n",
       "      <td>0.572802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-08-22 00:00:00-04:00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>32.840000</td>\n",
       "      <td>32.080001</td>\n",
       "      <td>147124.997336</td>\n",
       "      <td>1.762562e-11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.136837</td>\n",
       "      <td>0.153791</td>\n",
       "      <td>0.029676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-12-19 00:00:00-05:00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>33.680000</td>\n",
       "      <td>34.220001</td>\n",
       "      <td>322000.006916</td>\n",
       "      <td>1.762562e-11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.652992</td>\n",
       "      <td>0.091298</td>\n",
       "      <td>-0.254237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-03-08 00:00:00-05:00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>17.270002</td>\n",
       "      <td>17.490000</td>\n",
       "      <td>74699.993663</td>\n",
       "      <td>1.762562e-11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.908867</td>\n",
       "      <td>0.160933</td>\n",
       "      <td>-0.189251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2023-03-28 00:00:00-04:00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>4.120000</td>\n",
       "      <td>4.119998</td>\n",
       "      <td>2600.008322</td>\n",
       "      <td>1.762562e-11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.487694</td>\n",
       "      <td>0.176037</td>\n",
       "      <td>-0.313107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2023-08-24 00:00:00-04:00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>3.709999</td>\n",
       "      <td>3.760000</td>\n",
       "      <td>1900.003778</td>\n",
       "      <td>1.762562e-11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.181904</td>\n",
       "      <td>0.188909</td>\n",
       "      <td>-0.260638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2022-02-28 00:00:00-05:00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>34.520000</td>\n",
       "      <td>35.231999</td>\n",
       "      <td>260124.996688</td>\n",
       "      <td>1.762562e-11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.477122</td>\n",
       "      <td>0.144236</td>\n",
       "      <td>-0.133969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2022-05-13 00:00:00-04:00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>33.576000</td>\n",
       "      <td>33.895999</td>\n",
       "      <td>194750.006328</td>\n",
       "      <td>1.762562e-11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.332889</td>\n",
       "      <td>0.148137</td>\n",
       "      <td>-0.175596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2022-10-26 00:00:00-04:00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>30.568002</td>\n",
       "      <td>30.616000</td>\n",
       "      <td>134875.002997</td>\n",
       "      <td>1.762562e-11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.978785</td>\n",
       "      <td>0.158673</td>\n",
       "      <td>0.006794</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dls.show_batch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can define a model using the tabular_learner method. When we define our model, fastai will try to infer the loss function based on our y_names earlier.\n",
    "\n",
    "Note: Sometimes with tabular data, your y’s may be encoded (such as 0 and 1). In such a case you should explicitly pass y_block = CategoryBlock in your constructor so fastai won’t presume you are doing regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can train that model with the fit_one_cycle method (the fine_tune method won’t be useful here since we don’t have a pretrained model)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then have a look at some training predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training stockScreenerV2.0 for 3 epochs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>_rmse</th>\n",
       "      <th>mae</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.164666</td>\n",
       "      <td>0.331723</td>\n",
       "      <td>0.575954</td>\n",
       "      <td>0.546322</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.110369</td>\n",
       "      <td>0.268406</td>\n",
       "      <td>0.518080</td>\n",
       "      <td>0.388339</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.081473</td>\n",
       "      <td>0.293869</td>\n",
       "      <td>0.542096</td>\n",
       "      <td>0.379748</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>EV/EBIT_na</th>\n",
       "      <th>ROIC_na</th>\n",
       "      <th>Open</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Dividends</th>\n",
       "      <th>Stock Splits</th>\n",
       "      <th>EV/EBIT</th>\n",
       "      <th>ROIC</th>\n",
       "      <th>Future Year Change</th>\n",
       "      <th>Future Year Change_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>248.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.735732</td>\n",
       "      <td>-0.734971</td>\n",
       "      <td>-0.469610</td>\n",
       "      <td>-0.035072</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.322850</td>\n",
       "      <td>-0.825311</td>\n",
       "      <td>-0.409302</td>\n",
       "      <td>-0.353596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>463.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.749292</td>\n",
       "      <td>-0.749815</td>\n",
       "      <td>-0.439063</td>\n",
       "      <td>-0.035072</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.551052</td>\n",
       "      <td>1.361729</td>\n",
       "      <td>-0.312500</td>\n",
       "      <td>0.393161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>363.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.741069</td>\n",
       "      <td>-0.741024</td>\n",
       "      <td>-0.523670</td>\n",
       "      <td>-0.035072</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.415903</td>\n",
       "      <td>-0.399069</td>\n",
       "      <td>-0.439306</td>\n",
       "      <td>-0.214906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>171.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.742512</td>\n",
       "      <td>-0.743618</td>\n",
       "      <td>-0.395605</td>\n",
       "      <td>-0.035072</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.455783</td>\n",
       "      <td>-0.105512</td>\n",
       "      <td>-0.283871</td>\n",
       "      <td>-0.133416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>39.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.724336</td>\n",
       "      <td>-0.722000</td>\n",
       "      <td>-0.187971</td>\n",
       "      <td>-0.035072</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.123451</td>\n",
       "      <td>-1.254191</td>\n",
       "      <td>-0.550820</td>\n",
       "      <td>-0.387987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>376.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.741069</td>\n",
       "      <td>-0.741312</td>\n",
       "      <td>-0.453654</td>\n",
       "      <td>-0.035072</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.420334</td>\n",
       "      <td>-0.370996</td>\n",
       "      <td>-0.397661</td>\n",
       "      <td>-0.201124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>71.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.734289</td>\n",
       "      <td>-0.734250</td>\n",
       "      <td>-0.408726</td>\n",
       "      <td>-0.035072</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.311772</td>\n",
       "      <td>-0.861498</td>\n",
       "      <td>-0.377273</td>\n",
       "      <td>-0.397314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>43.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.720152</td>\n",
       "      <td>-0.721424</td>\n",
       "      <td>-0.338080</td>\n",
       "      <td>-0.035072</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.114588</td>\n",
       "      <td>-1.266166</td>\n",
       "      <td>-0.550162</td>\n",
       "      <td>-0.405838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>421.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.720873</td>\n",
       "      <td>-0.720991</td>\n",
       "      <td>-0.557681</td>\n",
       "      <td>-0.035072</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.550270</td>\n",
       "      <td>1.340570</td>\n",
       "      <td>-0.169872</td>\n",
       "      <td>0.669314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>292.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.745830</td>\n",
       "      <td>-0.745203</td>\n",
       "      <td>-0.238252</td>\n",
       "      <td>-0.035072</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.480154</td>\n",
       "      <td>0.133105</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.075873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>463.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.729240</td>\n",
       "      <td>-0.728486</td>\n",
       "      <td>-0.558941</td>\n",
       "      <td>-0.035072</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.565387</td>\n",
       "      <td>1.803703</td>\n",
       "      <td>-0.080769</td>\n",
       "      <td>1.151058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>45.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.720296</td>\n",
       "      <td>-0.721424</td>\n",
       "      <td>-0.299871</td>\n",
       "      <td>-0.035072</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.114588</td>\n",
       "      <td>-1.266166</td>\n",
       "      <td>-0.540453</td>\n",
       "      <td>-0.377692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>450.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.747994</td>\n",
       "      <td>-0.747221</td>\n",
       "      <td>-0.448825</td>\n",
       "      <td>-0.035072</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.511172</td>\n",
       "      <td>0.539611</td>\n",
       "      <td>-0.469231</td>\n",
       "      <td>-0.300579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>105.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.739050</td>\n",
       "      <td>-0.738862</td>\n",
       "      <td>-0.392561</td>\n",
       "      <td>-0.035072</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.382670</td>\n",
       "      <td>-0.582472</td>\n",
       "      <td>-0.122340</td>\n",
       "      <td>-0.183401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>441.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.729240</td>\n",
       "      <td>-0.729783</td>\n",
       "      <td>-0.558206</td>\n",
       "      <td>-0.035072</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.568003</td>\n",
       "      <td>1.897172</td>\n",
       "      <td>-0.051793</td>\n",
       "      <td>1.305953</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if trainNewModel == True:\n",
    "    learn = tabular_learner(dls, metrics=[rmse, mae])\n",
    "\n",
    "    print(f\"Training {modelName} for {epochs} epochs\")\n",
    "    learn.fit_one_cycle(epochs)\n",
    "\n",
    "    learn.show_results(max_n=15)\n",
    "\n",
    "    learn.export(modelFolder / f'{modelName}.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(learn, testTickers, model_name, model_folder, cont_names, cat_names):\n",
    "    \"\"\"\n",
    "    Evaluate a fastai model on a list of test tickers and log the results.\n",
    "    \n",
    "    Args:\n",
    "        learn: fastai Learner object\n",
    "        testTickers (list): List of ticker symbols to test on\n",
    "        model_name (str): Name of the model for logging\n",
    "        model_folder (Path): Path to save evaluation results\n",
    "        cont_names (list): List of continuous feature names\n",
    "        cat_names (list): List of categorical feature names\n",
    "    \"\"\"\n",
    "    all_predictions = []\n",
    "    all_actuals = []\n",
    "    test_data_list = []\n",
    "    \n",
    "    # Collect test data for all tickers\n",
    "    for ticker in testTickers:\n",
    "        try:\n",
    "            # Get test data\n",
    "            test_data = getTickerDataFrom1YrAgo(ticker)\n",
    "            if test_data.empty:\n",
    "                print(f\"Skipping {ticker} due to missing data\")\n",
    "                continue\n",
    "            \n",
    "            test_data_list.append(test_data)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching data for {ticker}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    if not test_data_list:\n",
    "        print(\"No valid test data collected\")\n",
    "        return None, None, None\n",
    "    \n",
    "    # Combine all test data\n",
    "    combined_test_data = pd.concat(test_data_list, ignore_index=True)\n",
    "    \n",
    "    # Create fastai test dataloader\n",
    "    test_dl = learn.dls.test_dl(combined_test_data)\n",
    "    \n",
    "    # Get predictions\n",
    "    preds, targs = learn.get_preds(dl=test_dl)\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    predictions = preds.numpy()\n",
    "    actuals = targs.numpy()\n",
    "    \n",
    "    # Create DataFrame for analysis\n",
    "    results_df = pd.DataFrame({\n",
    "        'Predicted': predictions.flatten(),\n",
    "        'Actual': actuals.flatten()\n",
    "    })\n",
    "\n",
    "    # Calculate residuals\n",
    "    results_df['Residual'] = results_df['Actual'] - results_df['Predicted']\n",
    "\n",
    "    # Define outlier threshold (2 standard deviations)\n",
    "    outlier_threshold = 2 * results_df['Residual'].std()\n",
    "\n",
    "    # Filter outliers\n",
    "    filtered_df = results_df[abs(results_df['Residual']) <= outlier_threshold]\n",
    "\n",
    "    # Calculate metrics\n",
    "    mae = np.mean(np.abs(filtered_df['Residual']))\n",
    "    rmse = np.sqrt(np.mean(filtered_df['Residual']**2))\n",
    "    r2 = 1 - (np.sum(filtered_df['Residual']**2) / \n",
    "              np.sum((filtered_df['Actual'] - filtered_df['Actual'].mean())**2))\n",
    "\n",
    "    # Log results\n",
    "    log_evaluation(model_name, mae, rmse, r2, model_folder, testTickers)\n",
    "    \n",
    "    # Create visualizations\n",
    "    plot_results(filtered_df, model_name, model_folder)\n",
    "    \n",
    "    return mae, rmse, r2\n",
    "\n",
    "def log_evaluation(model_name, mae, rmse, r2, model_folder, testTickers):\n",
    "    \"\"\"Log evaluation metrics to CSV file\"\"\"\n",
    "    log_file = model_folder / \"modelEvaluations.csv\"\n",
    "    \n",
    "    new_entry_df = pd.DataFrame([{\n",
    "        \"Model Name\": modelName,\n",
    "        \"Timestamp\": datetime.now().strftime('%Y-%m-%d %H:%M'),\n",
    "        \"MAE\": f'{mae:.3f}',\n",
    "        \"RMSE\": f'{rmse:.3f}',\n",
    "        \"R2\": f'{r2:.3f}',\n",
    "        \"Epochs\": epochs,\n",
    "        \"Training Size\": trainingSize,\n",
    "        \"Test Size\": len(testTickers),\n",
    "        \"Cat Names\": catNames,\n",
    "        \"Cont Names\": contNames,\n",
    "    }])\n",
    "    \n",
    "    try:\n",
    "        log_df = pd.read_csv(log_file)\n",
    "        log_df = pd.concat([log_df, new_entry_df], ignore_index=True)\n",
    "    except FileNotFoundError:\n",
    "        log_df = new_entry_df\n",
    "        \n",
    "    log_df.to_csv(log_file, index=False)\n",
    "    print(f\"Logged evaluation results to {log_file}\")\n",
    "\n",
    "def plot_results(filtered_df, model_name, model_folder):\n",
    "    \"\"\"Create and save visualization plots\"\"\"\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Scatter plot\n",
    "    plt.subplot(2, 1, 1)\n",
    "    actuals = filtered_df['Actual']\n",
    "    predictions = filtered_df['Predicted']\n",
    "    plt.scatter(actuals, predictions, alpha=0.7, label='Predictions')\n",
    "    \n",
    "    # Perfect prediction line\n",
    "    min_val = min(actuals.min(), predictions.min())\n",
    "    max_val = max(actuals.max(), predictions.max())\n",
    "    plt.plot([min_val, max_val], [min_val, max_val], \n",
    "             color='red', linestyle='--', label='Perfect Prediction')\n",
    "    \n",
    "    plt.title(f'Predicted vs. Actual Returns - {model_name}', fontsize=14)\n",
    "    plt.xlabel('Actual Returns', fontsize=12)\n",
    "    plt.ylabel('Predicted Returns', fontsize=12)\n",
    "    plt.legend()\n",
    "    plt.grid(alpha=0.5)\n",
    "    \n",
    "    # Residual plot\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.scatter(predictions, filtered_df['Residual'], alpha=0.7)\n",
    "    plt.axhline(y=0, color='r', linestyle='--')\n",
    "    plt.title('Residual Plot', fontsize=14)\n",
    "    plt.xlabel('Predicted Returns', fontsize=12)\n",
    "    plt.ylabel('Residual', fontsize=12)\n",
    "    plt.grid(alpha=0.5)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Show plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount of test tickers: 200, Expected: 200\n",
      "['TVE', 'AIZ', 'GRX', 'BVN', 'APC', 'MBLX', 'NDSN', 'AFT', 'SENEB', 'SHBI', 'CCC', 'OKE', 'NWSA', 'EDT', 'AUO', 'OPHC', 'CEM', 'CPA', 'ADRD', 'OLN', 'FULL', 'MRH', 'RYL', 'EEFT', 'KELYB', 'JPZ', 'GB', 'CAE', 'WERN', 'HPY', 'ROLL', 'AGD', 'STNG', 'RFI', 'CLV', 'TSU', 'PCG', 'STP', 'CBNJ', 'AFH', 'RIT', 'TS', 'SPNS', 'SGMO', 'PHK', 'DDD', 'NBHC', 'SBY', 'TXT', 'AXS', 'AAXJ', 'ARCC', 'C', 'ASBCW', 'PBI-B', 'WIRE', 'GLRE', 'DTT', 'ARIA', 'SBR', 'SSD', 'PEO', 'DATE', 'STAG', 'APU', 'XUE', 'CMLP', 'JNS', 'PAYX', 'TRIP', 'EXEL', 'CNSI', 'FOE', 'IRF', 'AVGO', 'HTS-A', 'KMM', 'DRE', 'LMNX', 'HSH', 'TOWN', 'SGU', 'MFNC', 'NMT-C', 'SMSI', 'TDI', 'NTWK', 'ABAX', 'HSIC', 'TXRH', 'TYL', 'C-P', 'C-Q', 'PBNY', 'PMX', 'GKNT', 'DSL', 'RNDY', 'OVAS', 'PFBI', 'ESD', 'BSMX', 'NCT', 'MSBF', 'NTAP', 'JZL', 'FWLT', 'PAAS', 'HDNG', 'PWRD', 'MYD', 'GGG', 'KBR', 'ARII', 'WBS', 'MTOR', 'CFR-A', 'WSCI', 'GOODN', 'KONG', 'BYFC', 'SUPN', 'LKFN', 'HPF', 'BIOD', 'MHI', 'MICT', 'TOL', 'AIF', 'GCAP', 'BNJ', 'WLDN', 'NEO', 'XXIA', 'CADX', 'ALTR', 'TCBK', 'STSI', 'RALY', 'DB', 'TMK', 'NLSN', 'WWD', 'SCOK', 'ORIT', 'ESV', 'JPM-I', 'LBTYK', 'EPR-F', 'BYD', 'DLR', 'SHLD', 'ARR', 'BIDU', 'CST', 'CVO', 'ROIC', 'VAC', 'LTRE', 'MRGE', 'DNP', 'SSL', 'AGO-F', 'CPAH', 'COR', 'ECOL', 'NQC', 'PROV', 'KERX', 'SFD', 'ZOLT', 'SDR', 'NAN', 'BPI', 'OIBR', 'SBGI', 'APRI', 'HCLP', 'IBCP', 'ARTNA', 'RNET', 'INSY', 'KIM-J', 'SMIT', 'MOG.B', 'ONFC', 'CMCSA', 'ROL', 'NWBO', 'LTBR', 'CDXS', 'BMS', 'CRDS', 'CGNX', 'AXDX', 'KEF', 'BGT', 'DPG', 'OTEX', 'IART']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "$APC: possibly delisted; no timezone found\n",
      "$MBLX: possibly delisted; no price data found  (1d 2023-12-23 -> 2024-01-22)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching data for APC: No historical data available for APC around 2024-01-22.\n",
      "Skipping APC due to missing data\n",
      "Error fetching data for MBLX: No historical data available for MBLX around 2024-01-22.\n",
      "Skipping MBLX due to missing data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "$AFT: possibly delisted; no timezone found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching data for AFT: No historical data available for AFT around 2024-01-22.\n",
      "Skipping AFT due to missing data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "$SENEB: possibly delisted; no price data found  (period=1d)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching data for SENEB: single positional indexer is out-of-bounds\n",
      "Skipping SENEB due to missing data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "$CCC: possibly delisted; no timezone found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching data for CCC: No historical data available for CCC around 2024-01-22.\n",
      "Skipping CCC due to missing data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "$EDT: possibly delisted; no price data found  (1d 2023-12-23 -> 2024-01-22)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching data for EDT: No historical data available for EDT around 2024-01-22.\n",
      "Skipping EDT due to missing data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "$AUO: possibly delisted; no timezone found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching data for AUO: No historical data available for AUO around 2024-01-22.\n",
      "Skipping AUO due to missing data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "$CEM: possibly delisted; no timezone found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching data for CEM: No historical data available for CEM around 2024-01-22.\n",
      "Skipping CEM due to missing data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "$ADRD: possibly delisted; no price data found  (1d 2023-12-23 -> 2024-01-22)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching data for ADRD: No historical data available for ADRD around 2024-01-22.\n",
      "Skipping ADRD due to missing data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "$FULL: possibly delisted; no price data found  (1d 2023-12-23 -> 2024-01-22)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching data for FULL: No historical data available for FULL around 2024-01-22.\n",
      "Skipping FULL due to missing data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "$MRH: possibly delisted; no price data found  (1d 2023-12-23 -> 2024-01-22)\n",
      "$RYL: possibly delisted; no price data found  (1d 2023-12-23 -> 2024-01-22)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching data for MRH: No historical data available for MRH around 2024-01-22.\n",
      "Skipping MRH due to missing data\n",
      "Error fetching data for RYL: No historical data available for RYL around 2024-01-22.\n",
      "Skipping RYL due to missing data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "$KELYB: possibly delisted; no price data found  (period=1d)\n",
      "$JPZ: possibly delisted; no price data found  (1d 2023-12-23 -> 2024-01-22)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching data for KELYB: single positional indexer is out-of-bounds\n",
      "Skipping KELYB due to missing data\n",
      "Error fetching data for JPZ: No historical data available for JPZ around 2024-01-22.\n",
      "Skipping JPZ due to missing data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "$HPY: possibly delisted; no price data found  (1d 2023-12-23 -> 2024-01-22)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching data for HPY: No historical data available for HPY around 2024-01-22.\n",
      "Skipping HPY due to missing data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "$ROLL: possibly delisted; no timezone found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching data for ROLL: No historical data available for ROLL around 2024-01-22.\n",
      "Skipping ROLL due to missing data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "$CLV: possibly delisted; no price data found  (1d 2023-12-23 -> 2024-01-22)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching data for CLV: No historical data available for CLV around 2024-01-22.\n",
      "Skipping CLV due to missing data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "$TSU: possibly delisted; no timezone found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching data for TSU: No historical data available for TSU around 2024-01-22.\n",
      "Skipping TSU due to missing data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "$STP: possibly delisted; no price data found  (1d 2023-12-23 -> 2024-01-22)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching data for STP: No historical data available for STP around 2024-01-22.\n",
      "Skipping STP due to missing data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "$CBNJ: possibly delisted; no price data found  (1d 2023-12-23 -> 2024-01-22)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching data for CBNJ: No historical data available for CBNJ around 2024-01-22.\n",
      "Skipping CBNJ due to missing data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "$AFH: possibly delisted; no timezone found\n",
      "$RIT: possibly delisted; no price data found  (1d 2023-12-23 -> 2024-01-22)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching data for AFH: No historical data available for AFH around 2024-01-22.\n",
      "Skipping AFH due to missing data\n",
      "Error fetching data for RIT: No historical data available for RIT around 2024-01-22.\n",
      "Skipping RIT due to missing data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "$SBY: possibly delisted; no price data found  (1d 2023-12-23 -> 2024-01-22)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching data for SBY: No historical data available for SBY around 2024-01-22.\n",
      "Skipping SBY due to missing data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "$ASBCW: possibly delisted; no price data found  (1d 2023-12-23 -> 2024-01-22)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching data for ASBCW: No historical data available for ASBCW around 2024-01-22.\n",
      "Skipping ASBCW due to missing data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "$PBI-B: possibly delisted; no timezone found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching data for PBI-B: No historical data available for PBI-B around 2024-01-22.\n",
      "Skipping PBI-B due to missing data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "$WIRE: possibly delisted; no timezone found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching data for WIRE: No historical data available for WIRE around 2024-01-22.\n",
      "Skipping WIRE due to missing data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "$DTT: possibly delisted; no price data found  (1d 2023-12-23 -> 2024-01-22)\n",
      "$ARIA: possibly delisted; no price data found  (1d 2023-12-23 -> 2024-01-22)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching data for DTT: No historical data available for DTT around 2024-01-22.\n",
      "Skipping DTT due to missing data\n",
      "Error fetching data for ARIA: No historical data available for ARIA around 2024-01-22.\n",
      "Skipping ARIA due to missing data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "$DATE: possibly delisted; no price data found  (1d 2023-12-23 -> 2024-01-22)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching data for DATE: No historical data available for DATE around 2024-01-22.\n",
      "Skipping DATE due to missing data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "$APU: possibly delisted; no timezone found\n",
      "$XUE: possibly delisted; no price data found  (1d 2023-12-23 -> 2024-01-22)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching data for APU: No historical data available for APU around 2024-01-22.\n",
      "Skipping APU due to missing data\n",
      "Error fetching data for XUE: No historical data available for XUE around 2024-01-22.\n",
      "Skipping XUE due to missing data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "$CMLP: possibly delisted; no price data found  (1d 2023-12-23 -> 2024-01-22)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching data for CMLP: No historical data available for CMLP around 2024-01-22.\n",
      "Skipping CMLP due to missing data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "$JNS: possibly delisted; no price data found  (1d 2023-12-23 -> 2024-01-22)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching data for JNS: No historical data available for JNS around 2024-01-22.\n",
      "Skipping JNS due to missing data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "$CNSI: possibly delisted; no price data found  (1d 2023-12-23 -> 2024-01-22)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching data for CNSI: No historical data available for CNSI around 2024-01-22.\n",
      "Skipping CNSI due to missing data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "$FOE: possibly delisted; no timezone found\n",
      "$IRF: possibly delisted; no price data found  (1d 2023-12-23 -> 2024-01-22)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching data for FOE: No historical data available for FOE around 2024-01-22.\n",
      "Skipping FOE due to missing data\n",
      "Error fetching data for IRF: No historical data available for IRF around 2024-01-22.\n",
      "Skipping IRF due to missing data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "$HTS-A: possibly delisted; no timezone found\n",
      "$KMM: possibly delisted; no price data found  (1d 2023-12-23 -> 2024-01-22)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching data for HTS-A: No historical data available for HTS-A around 2024-01-22.\n",
      "Skipping HTS-A due to missing data\n",
      "Error fetching data for KMM: No historical data available for KMM around 2024-01-22.\n",
      "Skipping KMM due to missing data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "$DRE: possibly delisted; no timezone found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching data for DRE: No historical data available for DRE around 2024-01-22.\n",
      "Skipping DRE due to missing data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "$LMNX: possibly delisted; no timezone found\n",
      "$HSH: possibly delisted; no price data found  (1d 2023-12-23 -> 2024-01-22)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching data for LMNX: No historical data available for LMNX around 2024-01-22.\n",
      "Skipping LMNX due to missing data\n",
      "Error fetching data for HSH: No historical data available for HSH around 2024-01-22.\n",
      "Skipping HSH due to missing data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "$MFNC: possibly delisted; no timezone found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching data for MFNC: No historical data available for MFNC around 2024-01-22.\n",
      "Skipping MFNC due to missing data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "$NMT-C: possibly delisted; no timezone found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching data for NMT-C: No historical data available for NMT-C around 2024-01-22.\n",
      "Skipping NMT-C due to missing data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "$TDI: possibly delisted; no price data found  (period=1d)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching data for TDI: single positional indexer is out-of-bounds\n",
      "Skipping TDI due to missing data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "$ABAX: possibly delisted; no price data found  (1d 2023-12-23 -> 2024-01-22)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching data for ABAX: No historical data available for ABAX around 2024-01-22.\n",
      "Skipping ABAX due to missing data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "$C-P: possibly delisted; no timezone found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching data for C-P: No historical data available for C-P around 2024-01-22.\n",
      "Skipping C-P due to missing data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "$C-Q: possibly delisted; no timezone found\n",
      "$PBNY: possibly delisted; no price data found  (1d 2023-12-23 -> 2024-01-22)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching data for C-Q: No historical data available for C-Q around 2024-01-22.\n",
      "Skipping C-Q due to missing data\n",
      "Error fetching data for PBNY: No historical data available for PBNY around 2024-01-22.\n",
      "Skipping PBNY due to missing data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "$GKNT: possibly delisted; no price data found  (1d 2023-12-23 -> 2024-01-22)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching data for GKNT: No historical data available for GKNT around 2024-01-22.\n",
      "Skipping GKNT due to missing data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "$RNDY: possibly delisted; no price data found  (1d 2023-12-23 -> 2024-01-22)\n",
      "$OVAS: possibly delisted; no price data found  (1d 2023-12-23 -> 2024-01-22)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching data for RNDY: No historical data available for RNDY around 2024-01-22.\n",
      "Skipping RNDY due to missing data\n",
      "Error fetching data for OVAS: No historical data available for OVAS around 2024-01-22.\n",
      "Skipping OVAS due to missing data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "$PFBI: possibly delisted; no timezone found\n",
      "$ESD: possibly delisted; no price data found  (1d 2023-12-23 -> 2024-01-22)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching data for PFBI: No historical data available for PFBI around 2024-01-22.\n",
      "Skipping PFBI due to missing data\n",
      "Error fetching data for ESD: No historical data available for ESD around 2024-01-22.\n",
      "Skipping ESD due to missing data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "$BSMX: possibly delisted; no timezone found\n",
      "$NCT: possibly delisted; no price data found  (1d 2023-12-23 -> 2024-01-22)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching data for BSMX: No historical data available for BSMX around 2024-01-22.\n",
      "Skipping BSMX due to missing data\n",
      "Error fetching data for NCT: No historical data available for NCT around 2024-01-22.\n",
      "Skipping NCT due to missing data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "$MSBF: possibly delisted; no timezone found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching data for MSBF: No historical data available for MSBF around 2024-01-22.\n",
      "Skipping MSBF due to missing data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "$JZL: possibly delisted; no price data found  (1d 2023-12-23 -> 2024-01-22)\n",
      "$FWLT: possibly delisted; no price data found  (1d 2023-12-23 -> 2024-01-22)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching data for JZL: No historical data available for JZL around 2024-01-22.\n",
      "Skipping JZL due to missing data\n",
      "Error fetching data for FWLT: No historical data available for FWLT around 2024-01-22.\n",
      "Skipping FWLT due to missing data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "$HDNG: possibly delisted; no price data found  (1d 2023-12-23 -> 2024-01-22)\n",
      "$PWRD: possibly delisted; no price data found  (1d 2023-12-23 -> 2024-01-22)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching data for HDNG: No historical data available for HDNG around 2024-01-22.\n",
      "Skipping HDNG due to missing data\n",
      "Error fetching data for PWRD: No historical data available for PWRD around 2024-01-22.\n",
      "Skipping PWRD due to missing data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "$ARII: possibly delisted; no price data found  (1d 2023-12-23 -> 2024-01-22)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching data for ARII: No historical data available for ARII around 2024-01-22.\n",
      "Skipping ARII due to missing data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "$MTOR: possibly delisted; no timezone found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching data for MTOR: No historical data available for MTOR around 2024-01-22.\n",
      "Skipping MTOR due to missing data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "$CFR-A: possibly delisted; no timezone found\n",
      "$WSCI: possibly delisted; no price data found  (1d 2023-12-23 -> 2024-01-22)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching data for CFR-A: No historical data available for CFR-A around 2024-01-22.\n",
      "Skipping CFR-A due to missing data\n",
      "Error fetching data for WSCI: No historical data available for WSCI around 2024-01-22.\n",
      "Skipping WSCI due to missing data\n",
      "Error fetching data for GOODN: unsupported operand type(s) for /: 'NoneType' and 'float'\n",
      "Skipping GOODN due to missing data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "$KONG: possibly delisted; no price data found  (period=1d)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching data for KONG: single positional indexer is out-of-bounds\n",
      "Skipping KONG due to missing data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "$BIOD: possibly delisted; no price data found  (1d 2023-12-23 -> 2024-01-22)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching data for BIOD: No historical data available for BIOD around 2024-01-22.\n",
      "Skipping BIOD due to missing data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "$MICT: possibly delisted; no timezone found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching data for MICT: No historical data available for MICT around 2024-01-22.\n",
      "Skipping MICT due to missing data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "$AIF: possibly delisted; no timezone found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching data for AIF: No historical data available for AIF around 2024-01-22.\n",
      "Skipping AIF due to missing data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "$GCAP: possibly delisted; no timezone found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching data for GCAP: No historical data available for GCAP around 2024-01-22.\n",
      "Skipping GCAP due to missing data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "$XXIA: possibly delisted; no price data found  (1d 2023-12-23 -> 2024-01-22)\n",
      "$CADX: possibly delisted; no price data found  (1d 2023-12-23 -> 2024-01-22)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching data for XXIA: No historical data available for XXIA around 2024-01-22.\n",
      "Skipping XXIA due to missing data\n",
      "Error fetching data for CADX: No historical data available for CADX around 2024-01-22.\n",
      "Skipping CADX due to missing data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "$STSI: possibly delisted; no price data found  (1d 2023-12-23 -> 2024-01-22)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching data for STSI: No historical data available for STSI around 2024-01-22.\n",
      "Skipping STSI due to missing data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "$RALY: possibly delisted; no price data found  (1d 2023-12-23 -> 2024-01-22)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching data for RALY: No historical data available for RALY around 2024-01-22.\n",
      "Skipping RALY due to missing data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "$TMK: possibly delisted; no timezone found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching data for TMK: No historical data available for TMK around 2024-01-22.\n",
      "Skipping TMK due to missing data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "$NLSN: possibly delisted; no timezone found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching data for NLSN: No historical data available for NLSN around 2024-01-22.\n",
      "Skipping NLSN due to missing data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "$SCOK: possibly delisted; no price data found  (1d 2023-12-23 -> 2024-01-22)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching data for SCOK: No historical data available for SCOK around 2024-01-22.\n",
      "Skipping SCOK due to missing data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "$ORIT: possibly delisted; no timezone found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching data for ORIT: No historical data available for ORIT around 2024-01-22.\n",
      "Skipping ORIT due to missing data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "$ESV: possibly delisted; no timezone found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching data for ESV: No historical data available for ESV around 2024-01-22.\n",
      "Skipping ESV due to missing data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "$JPM-I: possibly delisted; no timezone found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching data for JPM-I: No historical data available for JPM-I around 2024-01-22.\n",
      "Skipping JPM-I due to missing data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "$EPR-F: possibly delisted; no timezone found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching data for EPR-F: No historical data available for EPR-F around 2024-01-22.\n",
      "Skipping EPR-F due to missing data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "$SHLD: possibly delisted; no price data found  (1d 2023-12-23 -> 2024-01-22) (Yahoo error = \"Data doesn't exist for startDate = 1703307600, endDate = 1705899600\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching data for SHLD: No historical data available for SHLD around 2024-01-22.\n",
      "Skipping SHLD due to missing data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "$CST: possibly delisted; no price data found  (1d 2023-12-23 -> 2024-01-22)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching data for CST: No historical data available for CST around 2024-01-22.\n",
      "Skipping CST due to missing data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "$CVO: possibly delisted; no price data found  (1d 2023-12-23 -> 2024-01-22)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching data for CVO: No historical data available for CVO around 2024-01-22.\n",
      "Skipping CVO due to missing data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "$MRGE: possibly delisted; no price data found  (period=1d)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching data for MRGE: single positional indexer is out-of-bounds\n",
      "Skipping MRGE due to missing data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "$AGO-F: possibly delisted; no timezone found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching data for AGO-F: No historical data available for AGO-F around 2024-01-22.\n",
      "Skipping AGO-F due to missing data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "$CPAH: possibly delisted; no timezone found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching data for CPAH: No historical data available for CPAH around 2024-01-22.\n",
      "Skipping CPAH due to missing data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "$ECOL: possibly delisted; no timezone found\n",
      "$NQC: possibly delisted; no price data found  (1d 2023-12-23 -> 2024-01-22)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching data for ECOL: No historical data available for ECOL around 2024-01-22.\n",
      "Skipping ECOL due to missing data\n",
      "Error fetching data for NQC: No historical data available for NQC around 2024-01-22.\n",
      "Skipping NQC due to missing data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "$PROV: possibly delisted; no price data found  (period=1d)\n",
      "$KERX: possibly delisted; no price data found  (1d 2023-12-23 -> 2024-01-22)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching data for PROV: single positional indexer is out-of-bounds\n",
      "Skipping PROV due to missing data\n",
      "Error fetching data for KERX: No historical data available for KERX around 2024-01-22.\n",
      "Skipping KERX due to missing data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "$SFD: possibly delisted; no price data found  (1d 2023-12-23 -> 2024-01-22)\n",
      "$ZOLT: possibly delisted; no price data found  (1d 2023-12-23 -> 2024-01-22)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching data for SFD: No historical data available for SFD around 2024-01-22.\n",
      "Skipping SFD due to missing data\n",
      "Error fetching data for ZOLT: No historical data available for ZOLT around 2024-01-22.\n",
      "Skipping ZOLT due to missing data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "$SDR: possibly delisted; no timezone found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching data for SDR: No historical data available for SDR around 2024-01-22.\n",
      "Skipping SDR due to missing data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "$BPI: possibly delisted; no timezone found\n",
      "$OIBR: possibly delisted; no price data found  (1d 2023-12-23 -> 2024-01-22)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching data for BPI: No historical data available for BPI around 2024-01-22.\n",
      "Skipping BPI due to missing data\n",
      "Error fetching data for OIBR: No historical data available for OIBR around 2024-01-22.\n",
      "Skipping OIBR due to missing data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "$APRI: possibly delisted; no price data found  (1d 2023-12-23 -> 2024-01-22)\n",
      "$HCLP: possibly delisted; no price data found  (1d 2023-12-23 -> 2024-01-22)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching data for APRI: No historical data available for APRI around 2024-01-22.\n",
      "Skipping APRI due to missing data\n",
      "Error fetching data for HCLP: No historical data available for HCLP around 2024-01-22.\n",
      "Skipping HCLP due to missing data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "$RNET: possibly delisted; no timezone found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching data for RNET: No historical data available for RNET around 2024-01-22.\n",
      "Skipping RNET due to missing data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "$INSY: possibly delisted; no timezone found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching data for INSY: No historical data available for INSY around 2024-01-22.\n",
      "Skipping INSY due to missing data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "$KIM-J: possibly delisted; no timezone found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching data for KIM-J: No historical data available for KIM-J around 2024-01-22.\n",
      "Skipping KIM-J due to missing data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "$MOG.B: possibly delisted; no timezone found\n",
      "$ONFC: possibly delisted; no price data found  (1d 2023-12-23 -> 2024-01-22)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching data for MOG.B: No historical data available for MOG.B around 2024-01-22.\n",
      "Skipping MOG.B due to missing data\n",
      "Error fetching data for ONFC: No historical data available for ONFC around 2024-01-22.\n",
      "Skipping ONFC due to missing data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "$BMS: possibly delisted; no price data found  (1d 2023-12-23 -> 2024-01-22)\n",
      "$CRDS: possibly delisted; no price data found  (1d 2023-12-23 -> 2024-01-22)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching data for BMS: No historical data available for BMS around 2024-01-22.\n",
      "Skipping BMS due to missing data\n",
      "Error fetching data for CRDS: No historical data available for CRDS around 2024-01-22.\n",
      "Skipping CRDS due to missing data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "$KEF: possibly delisted; no price data found  (1d 2023-12-23 -> 2024-01-22)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching data for KEF: No historical data available for KEF around 2024-01-22.\n",
      "Skipping KEF due to missing data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gamer\\AppData\\Local\\Temp\\ipykernel_14704\\3433454675.py:37: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  combined_test_data = pd.concat(test_data_list, ignore_index=True)\n",
      "c:\\Users\\Gamer\\miniconda3\\Lib\\site-packages\\fastai\\tabular\\core.py:314: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  to[n].fillna(self.na_dict[n], inplace=True)\n",
      "c:\\Users\\Gamer\\miniconda3\\Lib\\site-packages\\fastai\\tabular\\core.py:314: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  to[n].fillna(self.na_dict[n], inplace=True)\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"['index'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[504], line 16\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAmount of test tickers: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(testTickers)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Expected: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtestSize\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(testTickers)\n\u001b[1;32m---> 16\u001b[0m mae, rmse, r2 \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlearn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Your fastai learner\u001b[39;49;00m\n\u001b[0;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtestTickers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtestTickers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodelName\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodelFolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcont_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontNames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcat_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcatNames\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mae \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m rmse \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m r2 \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEvaluation Results:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[503], line 40\u001b[0m, in \u001b[0;36mevaluate_model\u001b[1;34m(learn, testTickers, model_name, model_folder, cont_names, cat_names)\u001b[0m\n\u001b[0;32m     37\u001b[0m combined_test_data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat(test_data_list, ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# Create fastai test dataloader\u001b[39;00m\n\u001b[1;32m---> 40\u001b[0m test_dl \u001b[38;5;241m=\u001b[39m \u001b[43mlearn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdls\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest_dl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcombined_test_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# Get predictions\u001b[39;00m\n\u001b[0;32m     43\u001b[0m preds, targs \u001b[38;5;241m=\u001b[39m learn\u001b[38;5;241m.\u001b[39mget_preds(dl\u001b[38;5;241m=\u001b[39mtest_dl)\n",
      "File \u001b[1;32mc:\\Users\\Gamer\\miniconda3\\Lib\\site-packages\\fastai\\tabular\\data.py:56\u001b[0m, in \u001b[0;36mTabularDataLoaders.test_dl\u001b[1;34m(self, test_items, rm_type_tfms, process, inplace, **kwargs)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreate test `TabDataLoader` from `test_items` using validation `procs`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     55\u001b[0m to \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_ds\u001b[38;5;241m.\u001b[39mnew(test_items, inplace\u001b[38;5;241m=\u001b[39minplace)\n\u001b[1;32m---> 56\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m process: \u001b[43mto\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalid\u001b[38;5;241m.\u001b[39mnew(to, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Gamer\\miniconda3\\Lib\\site-packages\\fastai\\tabular\\core.py:182\u001b[0m, in \u001b[0;36mTabular.process\u001b[1;34m(self)\u001b[0m\n\u001b[1;32m--> 182\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess\u001b[39m(\u001b[38;5;28mself\u001b[39m): \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocs\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Gamer\\miniconda3\\Lib\\site-packages\\fastcore\\transform.py:210\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[1;34m(self, o)\u001b[0m\n\u001b[1;32m--> 210\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, o): \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcompose_tfms\u001b[49m\u001b[43m(\u001b[49m\u001b[43mo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtfms\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit_idx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Gamer\\miniconda3\\Lib\\site-packages\\fastcore\\transform.py:160\u001b[0m, in \u001b[0;36mcompose_tfms\u001b[1;34m(x, tfms, is_enc, reverse, **kwargs)\u001b[0m\n\u001b[0;32m    158\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m tfms:\n\u001b[0;32m    159\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_enc: f \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mdecode\n\u001b[1;32m--> 160\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32mc:\\Users\\Gamer\\miniconda3\\Lib\\site-packages\\fastcore\\transform.py:83\u001b[0m, in \u001b[0;36mTransform.__call__\u001b[1;34m(self, x, **kwargs)\u001b[0m\n\u001b[1;32m---> 83\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs): \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mencodes\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Gamer\\miniconda3\\Lib\\site-packages\\fastcore\\transform.py:109\u001b[0m, in \u001b[0;36mInplaceTransform._call\u001b[1;34m(self, fn, x, split_idx, **kwargs)\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_call\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn, x, split_idx\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 109\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32mc:\\Users\\Gamer\\miniconda3\\Lib\\site-packages\\fastcore\\transform.py:93\u001b[0m, in \u001b[0;36mTransform._call\u001b[1;34m(self, fn, x, split_idx, **kwargs)\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_call\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn, x, split_idx\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     92\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m split_idx\u001b[38;5;241m!=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msplit_idx \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msplit_idx \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m: \u001b[38;5;28;01mreturn\u001b[39;00m x\n\u001b[1;32m---> 93\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Gamer\\miniconda3\\Lib\\site-packages\\fastcore\\transform.py:99\u001b[0m, in \u001b[0;36mTransform._do_call\u001b[1;34m(self, f, x, **kwargs)\u001b[0m\n\u001b[0;32m     97\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m f \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m: \u001b[38;5;28;01mreturn\u001b[39;00m x\n\u001b[0;32m     98\u001b[0m     ret \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mreturns(x) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(f,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreturns\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m---> 99\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m retain_type(\u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m, x, ret)\n\u001b[0;32m    100\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_call(f, x_, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;28;01mfor\u001b[39;00m x_ \u001b[38;5;129;01min\u001b[39;00m x)\n\u001b[0;32m    101\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m retain_type(res, x)\n",
      "File \u001b[1;32mc:\\Users\\Gamer\\miniconda3\\Lib\\site-packages\\fastcore\\dispatch.py:122\u001b[0m, in \u001b[0;36mTypeDispatch.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minst \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m: f \u001b[38;5;241m=\u001b[39m MethodType(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minst)\n\u001b[0;32m    121\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mowner \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m: f \u001b[38;5;241m=\u001b[39m MethodType(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mowner)\n\u001b[1;32m--> 122\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Gamer\\miniconda3\\Lib\\site-packages\\fastai\\tabular\\core.py:247\u001b[0m, in \u001b[0;36mCategorify.encodes\u001b[1;34m(self, to)\u001b[0m\n\u001b[1;32m--> 247\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mencodes\u001b[39m(\u001b[38;5;28mself\u001b[39m, to): \u001b[43mto\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mto\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_apply_cats\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclasses\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Gamer\\miniconda3\\Lib\\site-packages\\fastai\\tabular\\core.py:205\u001b[0m, in \u001b[0;36mTabularPandas.transform\u001b[1;34m(self, cols, f, all_col)\u001b[0m\n\u001b[0;32m    203\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtransform\u001b[39m(\u001b[38;5;28mself\u001b[39m, cols, f, all_col\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m    204\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m all_col: cols \u001b[38;5;241m=\u001b[39m [c \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m cols \u001b[38;5;28;01mif\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems\u001b[38;5;241m.\u001b[39mcolumns]\n\u001b[1;32m--> 205\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(cols) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m: \u001b[38;5;28mself\u001b[39m[cols] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mcols\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mtransform(f)\n",
      "File \u001b[1;32mc:\\Users\\Gamer\\miniconda3\\Lib\\site-packages\\fastcore\\foundation.py:90\u001b[0m, in \u001b[0;36mCollBase.__getitem__\u001b[1;34m(self, k)\u001b[0m\n\u001b[1;32m---> 90\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, k): \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43mCollBase\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Gamer\\miniconda3\\Lib\\site-packages\\pandas\\core\\frame.py:4108\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4106\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[0;32m   4107\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[1;32m-> 4108\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolumns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m   4110\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[0;32m   4111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\Gamer\\miniconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6200\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[1;34m(self, key, axis_name)\u001b[0m\n\u001b[0;32m   6197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   6198\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[1;32m-> 6200\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   6202\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[0;32m   6203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[0;32m   6204\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Gamer\\miniconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6252\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[1;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[0;32m   6249\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   6251\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[1;32m-> 6252\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['index'] not in index\""
     ]
    }
   ],
   "source": [
    "if trainNewModel == True:\n",
    "    # Evaluate model\n",
    "    nonTrainingTickers = list(set(symbols) - set(trainingTickers))\n",
    "    \n",
    "    if not nonTrainingTickers:\n",
    "        raise ValueError(\"No tickers available for testing. Ensure nonTrainingTickers is populated correctly.\")\n",
    "    \n",
    "    if testSize == 'ALL':\n",
    "        testTickers = nonTrainingTickers\n",
    "    else:\n",
    "        testTickers = np.random.choice(nonTrainingTickers, size=min(testSize, len(nonTrainingTickers)), replace=False).tolist()\n",
    "    \n",
    "    print(f\"Amount of test tickers: {len(testTickers)}, Expected: {testSize}\")\n",
    "    print(testTickers)\n",
    "    \n",
    "    mae, rmse, r2 = evaluate_model(\n",
    "        learn=learn,  # Your fastai learner\n",
    "        testTickers=testTickers,\n",
    "        model_name=modelName,\n",
    "        model_folder=modelFolder,\n",
    "        cont_names=contNames,\n",
    "        cat_names=catNames\n",
    "    )\n",
    "    \n",
    "    if mae is not None and rmse is not None and r2 is not None:\n",
    "        print(f\"Evaluation Results:\")\n",
    "        print(f\"MAE: {mae:.3f}\")\n",
    "        print(f\"RMSE: {rmse:.3f}\")\n",
    "        print(f\"R2: {r2:.3f}\")\n",
    "    else:\n",
    "        print(\"Evaluation failed. Metrics are None.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model files in modelFolder:\n",
      "stockScreenerV0.0.pkl\n",
      "stockScreenerV1.0.pkl\n"
     ]
    }
   ],
   "source": [
    "print('Model files in modelFolder:')\n",
    "for file in modelFolder.glob('*.pkl'):\n",
    "    print(file.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model Name    stockScreenerV0.0\n",
       "Timestamp      2025-01-21 17:41\n",
       "MAE                       0.203\n",
       "RMSE                      0.247\n",
       "R2                       -2.154\n",
       "Name: 1, dtype: object"
      ]
     },
     "execution_count": 453,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluations = pd.read_csv(modelFolder / 'modelEvaluations.csv')\n",
    "bestModel = evaluations.sort_values('MAE', ascending=True).iloc[0]\n",
    "bestModel.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.name == 'nt':\n",
    "    temp = pathlib.PosixPath\n",
    "    pathlib.PosixPath = pathlib.WindowsPath\n",
    "\n",
    "importedModel = Path(f\"{bestModel['Model Name']}.pkl\") # Change this if you want to try other models\n",
    "learn = load_learner(modelFolder / importedModel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if predictionTarget != None:\n",
    "    if predictionTarget == 'ALL':\n",
    "        adr_df = pd.read_csv(testFolder / 'tickers.csv')\n",
    "        symbols = adr_df['Ticker'].tolist()\n",
    "        test_df = [getTickerData(symbol) for symbol in symbols]\n",
    "        test_df = pd.concat(test_df, ignore_index=True)  # Concatenate list of DataFrames into a single DataFrame\n",
    "    else:\n",
    "        test_df = getTickerData(predictionTarget)\n",
    "\n",
    "    # Ensure test_df is a DataFrame\n",
    "    if isinstance(test_df, dict):\n",
    "        test_df = pd.DataFrame([test_df])\n",
    "\n",
    "    dl = learn.dls.test_dl(test_df)\n",
    "    test_df.head()\n",
    "\n",
    "    if predictionTarget == 'ALL':\n",
    "        prediction = learn.get_preds(dl=dl)\n",
    "        adr_df = pd.read_csv(testFolder / 'tickers.csv')\n",
    "        company_dict = dict(zip(adr_df['Ticker'], adr_df['Company']))\n",
    "        sorted_predictions = sorted(zip(symbols, prediction[0]), key=lambda x: x[1], reverse=True)\n",
    "        print(f\"Got predictions for {len(sorted_predictions)} tickers, expected: {len(symbols)}\")\n",
    "        print(f\"Prediction for best performing tickers:\")\n",
    "        for symbol, pred in sorted_predictions:\n",
    "            company_name = company_dict.get(symbol, 'Unknown')\n",
    "            print(f\"{symbol} ({company_name}): {pred[0].item() * 100:.2f}%\")\n",
    "    else:\n",
    "        prediction = learn.get_preds(dl=dl)\n",
    "        company_name = company_dict.get(predictionTarget, 'Unknown')\n",
    "        print(f\"Prediction for {predictionTarget} ({company_name}):\")\n",
    "        print(f\"{prediction[0][0][0].item() * 100:.2f}%\")\n",
    "    print(\"Free money?!\")\n",
    "\n",
    "predictionTarget = 'ALL' # 'ALL' for all tickers decending, 'None' for no prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
